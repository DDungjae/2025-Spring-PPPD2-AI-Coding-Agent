{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJ6i2qcdZ0Ow"
      },
      "source": [
        "# Code generation with RAG and self-correction\n",
        "## 작성자 : AISchool ( http://aischool.ai/%ec%98%a8%eb%9d%bc%ec%9d%b8-%ea%b0%95%ec%9d%98-%ec%b9%b4%ed%85%8c%ea%b3%a0%eb%a6%ac/ )\n",
        "## Reference : https://langchain-ai.github.io/langgraph/tutorials/code_assistant/langgraph_code_assistant/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-FT4Hj18g_D"
      },
      "source": [
        "AlphaCodium은 제어 흐름(control flow)을 활용한 코드 생성 방식을 제안했습니다.\n",
        "\n",
        "핵심 아이디어: 코딩 문제에 대한 정답을 반복적으로 구성하는 것입니다.(https://x.com/karpathy/status/1748043513156272416?s=20)\n",
        "\n",
        "AlphaCodium( https://github.com/Codium-ai/AlphaCodium )은 특정 문제에 대해 공개 테스트와 AI가 생성한 테스트를 사용해 정답을 반복적으로 테스트하고 개선합니다.\n",
        "\n",
        "다음은 LangGraph를 사용하여 이러한 아이디어 중 일부를 처음부터 구현하는 과정입니다:\n",
        "\n",
        "1. 사용자에 의해 지정된 문서 집합으로 시작합니다.\n",
        "2. 이 문서를 장문 컨텍스트 LLM(long context LLM)을 통해 입력받고, 이를 기반으로 RAG(Retrieval-Augmented Generation)를 수행하여 질문에 답변합니다.\n",
        "3. 도구(tool)를 호출하여 구조화된 출력(structured output)을 생성합니다.\n",
        "4. 사용자에게 솔루션을 반환하기 전에 두 가지 단위 테스트(임포트 확인 및 코드 실행 테스트)를 수행합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHu16F9PEfk9"
      },
      "source": [
        "![code_assistant.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbuKuMt363n2"
      },
      "source": [
        "# 라이브러리 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8tLq3Mf6xh3",
        "outputId": "cb73cf15-189a-4510-b865-b8c8a15b6c69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.31)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting langchain_experimental\n",
            "  Downloading langchain_experimental-0.3.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.3.31-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: langchain_core in /usr/local/lib/python3.11/dist-packages (0.3.52)\n",
            "Collecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-8.0.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.21-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.3.54-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.75.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.16)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.24-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-prebuilt<0.2,>=0.1.8 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.1.8-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.61-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting xxhash<4.0.0,>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain_core) (4.13.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Collecting primp>=0.15.0 (from duckduckgo-search)\n",
            "  Downloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.32.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.23.0)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from wikipedia) (4.13.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain_core) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting ormsgpack<2.0.0,>=1.8.0 (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph)\n",
            "  Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.53b1)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.30.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain_openai-0.3.14-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_experimental-0.3.4-py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.2/209.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.3.31-py3-none-any.whl (145 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.2/145.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.54-py3-none-any.whl (433 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading duckduckgo_search-8.0.1-py3-none-any.whl (18 kB)\n",
            "Downloading langchain_community-0.3.21-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.24-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.1.8-py3-none-any.whl (25 kB)\n",
            "Downloading langgraph_sdk-0.1.61-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading primp-0.15.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading ormsgpack-1.9.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (223 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.6/223.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: wikipedia, pypika\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11678 sha256=963caba0637e2a7d9bf109bb90109c1d00f05f938abcee8c3d07d3caba55152b\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/cb/45ccc40522d3a1c41e1d2ad53b8f33a62f394011ec38cd71c6\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53801 sha256=f8fa6528de94f6ad762a0101bdbb832b6ae04f0702ed39288c3c567556f5da35\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built wikipedia pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, xxhash, uvloop, uvicorn, python-dotenv, pyproject_hooks, primp, overrides, ormsgpack, opentelemetry-util-http, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, wikipedia, watchfiles, typing-inspect, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, duckduckgo-search, coloredlogs, build, bs4, pydantic-settings, onnxruntime, langgraph-sdk, kubernetes, fastapi, dataclasses-json, opentelemetry-instrumentation, langchain_core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain_openai, opentelemetry-instrumentation-fastapi, langgraph-prebuilt, langgraph, langchain-community, chromadb, langchain_experimental\n",
            "  Attempting uninstall: langchain_core\n",
            "    Found existing installation: langchain-core 0.3.52\n",
            "    Uninstalling langchain-core-0.3.52:\n",
            "      Successfully uninstalled langchain-core-0.3.52\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.3.0 bs4-0.0.2 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 dataclasses-json-0.6.7 duckduckgo-search-8.0.1 durationpy-0.9 fastapi-0.115.9 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 kubernetes-32.0.1 langchain-community-0.3.21 langchain_core-0.3.54 langchain_experimental-0.3.4 langchain_openai-0.3.14 langgraph-0.3.31 langgraph-checkpoint-2.0.24 langgraph-prebuilt-0.1.8 langgraph-sdk-0.1.61 marshmallow-3.26.1 mmh3-5.1.0 monotonic-1.6 mypy-extensions-1.0.0 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 ormsgpack-1.9.1 overrides-7.7.0 posthog-3.25.0 primp-0.15.0 pydantic-settings-2.9.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 starlette-0.45.3 tiktoken-0.9.0 typing-inspect-0.9.0 uvicorn-0.34.2 uvloop-0.21.0 watchfiles-1.0.5 wikipedia-1.4.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_openai langsmith pandas langchain_experimental matplotlib langgraph langchain_core duckduckgo-search langchain-community chromadb wikipedia bs4 langchain_community"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrNDtWnp67YC"
      },
      "source": [
        "# API Key 설정"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEJ6uyWB67YC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"Code generation with RAG and self-correction - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"여러분의_LANGCHAIN_API_KEY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qvl0Nyms67YD",
        "outputId": "52905fa1-ceee-40d2-d98c-3c2a0bee8bf7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'14519ab4'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JXZokq667ID"
      },
      "outputs": [],
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = \"여러분의_OPENAI_API_KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZnv90Fztrww"
      },
      "source": [
        "# Docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mEMSADvuGWe"
      },
      "source": [
        "예시로 LangChain Expression Language (LCEL) 문서( https://python.langchain.com/docs/concepts/#langchain-expression-language-lcel )를 불러옵니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73M0OoOgttC3",
        "outputId": "96177ad6-6f25-440d-bf5b-3159db852cd9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# LCEL 관련 문서 URL 20개 (공식 문서에서 수집한 주요 개념 페이지들)\n",
        "urls = [\n",
        "    \"https://python.langchain.com/docs/concepts/lcel/\",\n",
        "    \"https://python.langchain.com/docs/concepts/runnables/\",\n",
        "    \"https://python.langchain.com/docs/concepts/structured_outputs/\",\n",
        "    \"https://python.langchain.com/docs/concepts/tool_calling/\",\n",
        "    \"https://python.langchain.com/docs/concepts/tools/\",\n",
        "    \"https://python.langchain.com/docs/concepts/callbacks/\",\n",
        "    \"https://python.langchain.com/docs/concepts/messages/\",\n",
        "    \"https://python.langchain.com/docs/concepts/chat_models/\",\n",
        "    \"https://python.langchain.com/docs/concepts/chat_history/\",\n",
        "    \"https://python.langchain.com/docs/concepts/text_splitters/\",\n",
        "    \"https://python.langchain.com/docs/concepts/output_parsers/\",\n",
        "    \"https://python.langchain.com/docs/concepts/prompt_templates/\",\n",
        "    \"https://python.langchain.com/docs/concepts/agents/\",\n",
        "    \"https://python.langchain.com/docs/concepts/evaluation/\",\n",
        "    \"https://python.langchain.com/docs/concepts/testing/\",\n",
        "    \"https://python.langchain.com/docs/concepts/tracing/\",\n",
        "    \"https://python.langchain.com/docs/concepts/embedding_models/\",\n",
        "    \"https://python.langchain.com/docs/concepts/vectorstores/\",\n",
        "    \"https://python.langchain.com/docs/concepts/retrievers/\",\n",
        "    \"https://python.langchain.com/docs/concepts/retrieval/\"\n",
        "]\n",
        "\n",
        "# WebBaseLoader를 사용해 문서 로드\n",
        "loader = WebBaseLoader(urls)\n",
        "docs = loader.load()\n",
        "\n",
        "# Sort the list based on the URLs and get the text\n",
        "d_sorted = sorted(docs, key=lambda x: x.metadata[\"source\"])\n",
        "d_reversed = list(reversed(d_sorted))\n",
        "concatenated_content = \"\\n\\n\\n --- \\n\\n\\n\".join(\n",
        "    [doc.page_content for doc in d_reversed]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsUTFpQfvI7Q",
        "outputId": "0341d3d1-53d9-479a-aa1b-cfeb3a3d4a30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://python.langchain.com/docs/concepts/agents/', 'title': 'Agents | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAgents | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideAgentsOn this pageAgents\\nBy themselves, language models can\\'t take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\\nLangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.\\nPlease see the following resources for more information:\\n\\nLangGraph docs on common agent architectures\\nPre-built agents in LangGraph\\n\\nLegacy agent concept: AgentExecutor‚Äã\\nLangChain previously introduced the AgentExecutor as a runtime for agents.\\nWhile it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.\\nAs a result, we\\'re gradually phasing out AgentExecutor in favor of more flexible solutions in LangGraph.\\nTransitioning from AgentExecutor to langgraph‚Äã\\nIf you\\'re currently using AgentExecutor, don\\'t worry! We\\'ve prepared resources to help you:\\n\\n\\nFor those who still need to use AgentExecutor, we offer a comprehensive guide on how to use AgentExecutor.\\n\\n\\nHowever, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we\\'ve created a detailed migration guide to help you move from AgentExecutor to LangGraph seamlessly.\\n\\nEdit this pageWas this page helpful?PreviousConceptual guideNextArchitectureLegacy agent concept: AgentExecutorTransitioning from AgentExecutor to langgraphCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/callbacks/', 'title': 'Callbacks | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Runnable interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nCallbacks | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideCallbacksOn this pageCallbacks\\nPrerequisites\\nRunnable interface\\n\\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\\nYou can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\\nCallback events‚Äã\\nEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retry\\nCallback handlers‚Äã\\nCallback handlers can either be sync or async:\\n\\nSync callback handlers implement the BaseCallbackHandler interface.\\nAsync callback handlers implement the AsyncCallbackHandler interface.\\n\\nDuring run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\\nPassing callbacks‚Äã\\nThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\\n\\nRequest time callbacks: Passed at the time of the request in addition to the input data.\\nAvailable on all standard Runnable objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).\\nConstructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are not inherited by any children of the object.\\n\\nwarningConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\\nof the object.\\nIf you\\'re creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.\\nAsync in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\\nand is running async in python<=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.\\nFor specifics on how to use callbacks, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousAsync programming with langchainNextChat historyCallback eventsCallback handlersPassing callbacksCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/chat_history/', 'title': 'Chat history | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Messages', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nChat history | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat historyOn this pageChat history\\nPrerequisites\\nMessages\\nChat models\\nTool calling\\n\\nChat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of messages, each of which is associated with a specific role, such as \"user\", \"assistant\", \"system\", or \"tool\".\\nConversation patterns‚Äã\\n\\nMost conversations start with a system message that sets the context for the conversation. This is followed by a user message containing the user\\'s input, and then an assistant message containing the model\\'s response.\\nThe assistant may respond directly to the user or if configured with tools request that a tool be invoked to perform a specific task.\\nA full conversation often involves a combination of two patterns of alternating messages:\\n\\nThe user and the assistant representing a back-and-forth conversation.\\nThe assistant and tool messages representing an \"agentic\" workflow where the assistant is invoking tools to perform specific tasks.\\n\\nManaging chat history‚Äã\\nSince chat models have a maximum limit on input size, it\\'s important to manage chat history and trim it as needed to avoid exceeding the context window.\\nWhile processing chat history, it\\'s essential to preserve a correct conversation structure.\\nKey guidelines for managing chat history:\\n\\nThe conversation should follow one of these structures:\\n\\nThe first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\\nThe last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\\n\\n\\nWhen using tool calling, a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\\n\\ntipUnderstanding correct conversation structure is essential for being able to properly implement\\nmemory in chat models.\\nRelated resources‚Äã\\n\\nHow to trim messages\\nMemory guide for information on implementing short-term and long-term memory in chat models using LangGraph.\\nEdit this pageWas this page helpful?PreviousCallbacksNextChat modelsConversation patternsManaging chat historyRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/chat_models/', 'title': 'Chat models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Overview', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nChat models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat modelsOn this pageChat models\\nOverview‚Äã\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\nModern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output.\\nThe newest generation of chat models offer additional capabilities:\\n\\nTool calling: Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMultimodality: The ability to work with data other than text; for example, images, audio, and video.\\n\\nFeatures‚Äã\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\nIntegrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see chat model integrations for an up-to-date list of supported models.\\nUse either LangChain\\'s messages format or OpenAI format.\\nStandard tool calling API: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\nStandard API for structuring outputs via the with_structured_output method.\\nProvides support for async programming, efficient batching, a rich streaming API.\\nIntegration with LangSmith for monitoring and debugging production-grade applications based on LLMs.\\nAdditional features like standardized token usage, rate limiting, caching and more.\\n\\nIntegrations‚Äã\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\nThese integrations are one of two types:\\n\\nOfficial models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages.\\nCommunity models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\\nPlease review the chat model integrations for a list of supported models.\\nnoteModels that do not include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\nInterface‚Äã\\nLangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface, chat models support a standard streaming interface, async programming, optimized batching, and more. Please see the Runnable Interface for more details.\\nMany of the key methods of chat models operate on messages as input and return messages as output.\\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\\nnoteIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., Ollama, Anthropic, OpenAI, etc.).\\nThese models implement the BaseLLM interface and may be named with the \"LLM\" suffix (e.g., OllamaLLM, AnthropicLLM, OpenAILLM, etc.). Generally, users should not use these models.\\nKey methods‚Äã\\nThe key methods of a chat model are:\\n\\ninvoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\\nstream: A method that allows you to stream the output of a chat model as it is generated.\\nbatch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\\nbind_tools: A method that allows you to bind a tool to a chat model for use in the model\\'s execution context.\\nwith_structured_output: A wrapper around the invoke method for models that natively support structured output.\\n\\nOther important methods can be found in the BaseChatModel API Reference.\\nInputs and outputs‚Äã\\nModern LLMs are typically accessed through a chat model interface that takes messages as input and returns messages as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\\nLangChain supports two message formats to interact with chat models:\\n\\nLangChain Message Format: LangChain\\'s own message format, which is used by default and is used internally by LangChain.\\nOpenAI\\'s Message Format: OpenAI\\'s message format.\\n\\nStandard parameters‚Äã\\nMany chat models have standardized parameters that can be used to configure the model:\\nParameterDescriptionmodelThe name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\").temperatureControls the randomness of the model\\'s output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.timeoutThe maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn‚Äôt hang indefinitely.max_tokensLimits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.stopSpecifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.max_retriesThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.api_keyThe API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.base_urlThe URL of the API endpoint where requests are sent. This is typically provided by the model\\'s provider and is necessary for directing your requests.rate_limiterAn optional BaseRateLimiter to space out requests to avoid exceeding rate limits.  See rate-limiting below for more details.\\nSome important things to note:\\n\\nStandard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\\'t be supported on these.\\nStandard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they\\'re not enforced on models in langchain-community.\\n\\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective API reference for that model.\\nTool calling‚Äã\\nChat models can call tools to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\\nsee the tool calling guide for more information.\\nStructured outputs‚Äã\\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\\nuseful for information extraction tasks. Please read more about\\nthe technique in the structured outputs guide.\\nMultimodality‚Äã\\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as multimodality.\\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\\nContext window‚Äã\\nA chat model\\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the memory.\\nThe size of the input is measured in tokens which are the unit of processing that the model uses.\\nAdvanced topics‚Äã\\nRate-limiting‚Äã\\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\\nYou have a few options to deal with rate limits:\\n\\nTry to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the how to handle rate limits for more information on how to use this feature.\\nTry to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a max_retries parameter that can be used to control the number of retries. See the standard parameters section for more information.\\nFallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\\n\\nCaching‚Äã\\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the exact inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an embedding model to convert text to a vector representation), and it\\'s not guaranteed to capture the meaning of the input accurately.\\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\\nPlease see the how to cache chat model responses guide for more details.\\nRelated resources‚Äã\\n\\nHow-to guides on using chat models: how-to guides.\\nList of supported chat models: chat model integrations.\\n\\nConceptual guides‚Äã\\n\\nMessages\\nTool calling\\nMultimodality\\nStructured outputs\\nTokens\\nEdit this pageWas this page helpful?PreviousChat historyNextDocument loadersOverviewFeaturesIntegrationsInterfaceKey methodsInputs and outputsStandard parametersTool callingStructured outputsMultimodalityContext windowAdvanced topicsRate-limitingCachingRelated resourcesConceptual guidesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/embedding_models/', 'title': 'Embedding models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Documents', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEmbedding models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEmbedding modelsOn this pageEmbedding models\\n\\nPrerequisites\\nDocuments\\n\\nNoteThis conceptual overview focuses on text-based embedding models.Embedding models can also be multimodal though such models are not currently supported by LangChain.\\nImagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.\\nThis is the power of embedding models, which lie at the heart of many retrieval systems.\\nEmbedding models transform human language into a format that machines can understand and compare with speed and accuracy.\\nThese models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text\\'s semantic meaning.\\nEmbeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\\nKey concepts‚Äã\\n\\n(1) Embed text as a vector: Embeddings transform text into a numerical vector representation.\\n(2) Measure similarity: Embedding vectors can be compared using simple mathematical operations.\\nEmbedding‚Äã\\nHistorical context‚Äã\\nThe landscape of embedding models has evolved significantly over the years.\\nA pivotal moment came in 2018 when Google introduced BERT (Bidirectional Encoder Representations from Transformers).\\nBERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.\\nHowever, BERT wasn\\'t optimized for generating sentence embeddings efficiently.\\nThis limitation spurred the creation of SBERT (Sentence-BERT), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.\\nToday, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.\\nTo navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) here for objective comparisons.\\nFurther reading\\nSee the seminal BERT paper.\\nSee Cameron Wolfe\\'s excellent review of embedding models.\\nSee the Massive Text Embedding Benchmark (MTEB) leaderboard for a comprehensive overview of embedding models.\\n\\nInterface‚Äã\\nLangChain provides a universal interface for working with them, providing standard methods for common operations.\\nThis common interface simplifies interaction with various embedding providers through two central methods:\\n\\nembed_documents: For embedding multiple texts (documents)\\nembed_query: For embedding a single text (query)\\n\\nThis distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\\nTo illustrate, here\\'s a practical example using LangChain\\'s .embed_documents method to embed a list of strings:\\nfrom langchain_openai import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings()embeddings = embeddings_model.embed_documents(    [        \"Hi there!\",        \"Oh, hello!\",        \"What\\'s your name?\",        \"My friends call me World\",        \"Hello World!\"    ])len(embeddings), len(embeddings[0])(5, 1536)API Reference:OpenAIEmbeddings\\nFor convenience, you can also use the embed_query method to embed a single text:\\nquery_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")\\nFurther reading\\nSee the full list of LangChain embedding model integrations.\\nSee these how-to guides for working with embedding models.\\n\\nIntegrations‚Äã\\nLangChain offers many embedding model integrations which you can find on the embedding models integrations page.\\nMeasure similarity‚Äã\\nEach embedding is essentially a set of coordinates, often in a high-dimensional space.\\nIn this space, the position of each point (embedding) reflects the meaning of its corresponding text.\\nJust as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\\nThis allows for intuitive comparisons between different pieces of text.\\nBy reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\\nSome common similarity metrics include:\\n\\nCosine Similarity: Measures the cosine of the angle between two vectors.\\nEuclidean Distance: Measures the straight-line distance between two points.\\nDot Product: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric should be chosen based on the model.\\nAs an example, OpenAI suggests cosine similarity for their embeddings, which can be easily implemented:\\nimport numpy as npdef cosine_similarity(vec1, vec2):    dot_product = np.dot(vec1, vec2)    norm_vec1 = np.linalg.norm(vec1)    norm_vec2 = np.linalg.norm(vec2)    return dot_product / (norm_vec1 * norm_vec2)similarity = cosine_similarity(query_result, document_result)print(\"Cosine Similarity:\", similarity)\\nFurther reading\\nSee Simon Willison‚Äôs nice blog post and video on embeddings and similarity metrics.\\nSee this documentation from Google on similarity metrics to consider with embeddings.\\nSee Pinecone\\'s blog post on similarity metrics.\\nSee OpenAI\\'s FAQ on what similarity metric to use with OpenAI embeddings.\\nEdit this pageWas this page helpful?PreviousDocument loadersNextEvaluationKey conceptsEmbeddingHistorical contextInterfaceIntegrationsMeasure similarityCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/evaluation/', 'title': 'Evaluation | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEvaluationEvaluation\\n\\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications.\\n\\nLangSmith helps with this process in a few ways:\\n\\nIt makes it easier to create and curate datasets via its tracing and annotation features\\nIt provides an evaluation framework that helps you define metrics and run your app against your dataset\\nIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\\n\\nTo learn more, check out this LangSmith guide.Edit this pageWas this page helpful?PreviousEmbedding modelsNextExample selectorsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/lcel/', 'title': 'LangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Runnable Interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/messages/', 'title': 'Messages | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Chat Models', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nMessages | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideMessagesOn this pageMessages\\nPrerequisites\\nChat Models\\n\\nOverview‚Äã\\nMessages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\\nEach message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\\nLangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\nWhat is inside a message?‚Äã\\nA message typically consists of the following pieces of information:\\n\\nRole: The role of the message (e.g., \"user\", \"assistant\").\\nContent: The content of the message (e.g., text, multimodal data).\\nAdditional metadata: id, name, token usage and other model-specific metadata.\\n\\nRole‚Äã\\nRoles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\\nRoleDescriptionsystemUsed to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.userRepresents input from a user interacting with the model, usually in the form of text or other interactive input.assistantRepresents a response from the model, which can include text or a request to invoke tools.toolA message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.function (legacy)This is a legacy role, corresponding to OpenAI\\'s legacy function-calling API. tool role should be used instead.\\nContent‚Äã\\nThe content of a message text or a list of dictionaries representing multimodal data (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\\nCurrently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\\nFor more information see:\\n\\nSystemMessage -- for content which should be passed to direct the conversation\\nHumanMessage -- for content in the input from the user.\\nAIMessage -- for content in the response from the model.\\nMultimodality -- for more information on multimodal content.\\n\\nOther Message Data‚Äã\\nDepending on the chat model provider, messages can include other data such as:\\n\\nID: An optional unique identifier for the message.\\nName: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this!\\nMetadata: Additional information about the message, such as timestamps, token usage, etc.\\nTool Calls: A request made by the model to call one or more tools> See tool calling for more information.\\n\\nConversation Structure‚Äã\\nThe sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\\nFor example, a typical conversation structure might look like this:\\n\\nUser Message: \"Hello, how are you?\"\\nAssistant Message: \"I\\'m doing well, thank you for asking.\"\\nUser Message: \"Can you tell me a joke?\"\\nAssistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\\n\\nPlease read the chat history guide for more information on managing chat history and ensuring that the conversation structure is correct.\\nLangChain Messages‚Äã\\nLangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\nLangChain messages are Python objects that subclass from a BaseMessage.\\nThe five main message types are:\\n\\nSystemMessage: corresponds to system role\\nHumanMessage: corresponds to user role\\nAIMessage: corresponds to assistant role\\nAIMessageChunk: corresponds to assistant role, used for streaming responses\\nToolMessage: corresponds to tool role\\n\\nOther important messages include:\\n\\nRemoveMessage -- does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history.\\nLegacy FunctionMessage: corresponds to the function role in OpenAI\\'s legacy function-calling API.\\n\\nYou can find more information about messages in the API Reference.\\nSystemMessage‚Äã\\nA SystemMessage is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\\nDifferent chat providers may support system message in one of the following ways:\\n\\nThrough a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\\nThrough a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\\nNo support for system messages: Some models do not support system messages at all.\\n\\nMost major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider‚Äôs capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\\nIf no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message\\'s content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the langchain-community package) it is recommended to check the specific documentation for that model.\\nHumanMessage‚Äã\\nThe HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model.\\nText Content‚Äã\\nMost chat models expect the user input to be in the form of text.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hello, how are you?\")])API Reference:HumanMessage\\ntipWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing.model.invoke(\"Hello, how are you?\")\\nMulti-modal Content‚Äã\\nSome chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\\nPlease see the multimodality guide for more information.\\nAIMessage‚Äã\\nAIMessage is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.\\nfrom langchain_core.messages import HumanMessageai_message = model.invoke([HumanMessage(\"Tell me a joke\")])ai_message # <-- AIMessageAPI Reference:HumanMessage\\nAn AIMessage has the following attributes. The attributes which are standardized are the ones that LangChain attempts to standardize across different chat model providers. raw fields are specific to the model provider and may vary.\\nAttributeStandardized/RawDescriptioncontentRawUsually a string, but can be a list of content blocks. See content for details.tool_callsStandardizedTool calls associated with the message. See tool calling for details.invalid_tool_callsStandardizedTool calls with parsing errors associated with the message. See tool calling for details.usage_metadataStandardizedUsage metadata for a message, such as token counts. See Usage Metadata API Reference.idStandardizedAn optional unique identifier for the message, ideally provided by the provider/model that created the message.response_metadataRawResponse metadata, e.g., response headers, logprobs, token counts.\\ncontent‚Äã\\nThe content property of an AIMessage represents the response generated by the chat model.\\nThe content is either:\\n\\ntext -- the norm for virtually all chat models.\\nA list of dictionaries -- Each dictionary represents a content block and is associated with a type.\\n\\nUsed by Anthropic for surfacing agent thought process when doing tool calling.\\nUsed by OpenAI for audio outputs. Please see multi-modal content for more information.\\n\\n\\n\\nimportantThe content property is not standardized across different chat model providers, mostly because there are\\nstill few examples to generalize from.\\nAIMessageChunk‚Äã\\nIt is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\\nIt is returned from the stream, astream and astream_events methods of the chat model.\\nFor example,\\nfor chunk in model.stream([HumanMessage(\"what color is the sky?\")]):    print(chunk)\\nAIMessageChunk follows nearly the same structure as AIMessage, but uses a different ToolCallChunk\\nto be able to stream tool calling in a standardized manner.\\nAggregating‚Äã\\nAIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user.\\nai_message = chunk1 + chunk2 + chunk3 + ...\\nToolMessage‚Äã\\nThis represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has:\\n\\na tool_call_id field which conveys the id of the call to the tool that was called to produce this result.\\nan artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\\n\\nPlease see tool calling for more information.\\nRemoveMessage‚Äã\\nThis is a special message type that does not correspond to any roles. It is used\\nfor managing chat history in LangGraph.\\nPlease see the following for more information on how to use the RemoveMessage:\\n\\nMemory conceptual guide\\nHow to delete messages\\n\\n(Legacy) FunctionMessage‚Äã\\nThis is a legacy message type, corresponding to OpenAI\\'s legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.\\nOpenAI Format‚Äã\\nInputs‚Äã\\nChat models also accept OpenAI\\'s format as inputs to chat models:\\nchat_model.invoke([    {        \"role\": \"user\",        \"content\": \"Hello, how are you?\",    },    {        \"role\": \"assistant\",        \"content\": \"I\\'m doing well, thank you for asking.\",    },    {        \"role\": \"user\",        \"content\": \"Can you tell me a joke?\",    }])\\nOutputs‚Äã\\nAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you\\nneed OpenAI format for the output as well.\\nThe convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.Edit this pageWas this page helpful?PreviousLangChain Expression Language (LCEL)NextMultimodalityOverviewWhat is inside a message?RoleContentOther Message DataConversation StructureLangChain MessagesSystemMessageHumanMessageAIMessageAIMessageChunkToolMessageRemoveMessage(Legacy) FunctionMessageOpenAI FormatInputsOutputsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/output_parsers/', 'title': 'Output parsers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nOutput parsers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideOutput parsersOutput parsers\\n\\nnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that here.\\nOutput parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\\n\\nName: The name of the output parser\\nSupports Streaming: Whether the output parser supports streaming.\\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\\nOutput Type: The output type of the object returned by the parser.\\nDescription: Our commentary on this output parser and when to use it.\\n\\nNameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionStr‚úÖstr | MessageStringParses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).JSON‚úÖ‚úÖstr | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML‚úÖ‚úÖstr | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic\\'s).CSV‚úÖ‚úÖstr | MessageList[str]Returns a list of comma separated values.OutputFixing‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame‚úÖstr | MessagedictUseful for doing operations with pandas DataFrames.Enum‚úÖstr | MessageEnumParses response into one of the provided enum values.Datetime‚úÖstr | Messagedatetime.datetimeParses response into a datetime string.Structured‚úÖstr | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.\\nFor specifics on how to use output parsers, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousMultimodalityNextPrompt TemplatesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/prompt_templates/', 'title': 'Prompt Templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Prompt templates help to translate user input and parameters into instructions for a language model.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nPrompt Templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guidePrompt TemplatesOn this pagePrompt Templates\\nPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model\\'s response, helping it understand the context and generate relevant and coherent language-based output.\\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\\nThere are a few different types of prompt templates:\\nString PromptTemplates‚Äã\\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:\\nfrom langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplate\\nChatPromptTemplates‚Äã\\nThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\\nfrom langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplate\\nIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the topic variable the user passes in.\\nMessagesPlaceholder‚Äã\\n\\nThis prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\\nThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\nThis is useful for letting a list of messages be slotted into a particular spot.\\nAn alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\\nprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])\\nFor specifics on how to use prompt templates, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousOutput parsersNextRetrieval augmented generation (RAG)String PromptTemplatesChatPromptTemplatesMessagesPlaceholderCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/retrieval/', 'title': 'Retrieval | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Retrievers', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRetrieval | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrievalOn this pageRetrieval\\nPrerequisites\\nRetrievers\\nVector stores\\nEmbeddings\\nText splitters\\n\\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\\nThere are inherent risks in doing this.\\nMake sure that your database connection permissions are scoped as narrowly as possible for your application\\'s needs.\\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\\nFor more on general security best practices, see our security guide.\\nOverview‚Äã\\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\\nThese systems accommodate various data formats:\\n\\nUnstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\\nStructured data is typically housed in relational or graph databases with defined schemas.\\n\\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\\nThis translation enables more intuitive and flexible interactions with complex data structures.\\nKey concepts‚Äã\\n\\n(1) Query analysis: A process where models transform or construct search queries to optimize retrieval.\\n(2) Information retrieval: Search queries are used to fetch information from various retrieval systems.\\nQuery analysis‚Äã\\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\\n\\nQuery Re-writing: Queries can be re-written or expanded to improve semantic or lexical searches.\\nQuery Construction: Search indexes may require structured queries (e.g., SQL for databases).\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\\nQuery re-writing‚Äã\\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\\n\\nQuery Clarification: Models can rephrase ambiguous or poorly worded queries for clarity.\\nSemantic Understanding: They can capture the intent behind a query, going beyond literal keyword matching.\\nQuery Expansion: Models can generate related terms or concepts to broaden the search scope.\\nComplex Query Handling: They can break down multi-part questions into simpler sub-queries.\\n\\nVarious techniques have been developed to leverage models for query re-writing, including:\\nNameWhen to useDescriptionMulti-queryWhen you want to ensure high recall in retrieval by providing multiple phrasings of a question.Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. Paper.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. Paper.\\nAs an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.\\nThese can then be run sequentially or in parallel on a downstream retrieval system.\\nfrom typing import Listfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import SystemMessage, HumanMessage# Define a pydantic model to enforce the output structureclass Questions(BaseModel):    questions: List[str] = Field(        description=\"A list of sub-questions related to the input query.\"    )# Create an instance of the model and enforce the output structuremodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) structured_model = model.with_structured_output(Questions)# Define the system promptsystem = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\\\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \\\\n\"\"\"# Pass the question to the modelquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"questions = structured_model.invoke([SystemMessage(content=system)]+[HumanMessage(content=question)])API Reference:ChatOpenAI | SystemMessage | HumanMessage\\ntipSee our RAG from Scratch videos for a few different specific approaches:\\nMulti-query\\nDecomposition\\nStep-back\\nHyDE\\n\\nQuery construction‚Äã\\nQuery analysis also can focus on translating natural language queries into specialized query languages or filters.\\nThis translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.\\n\\n\\nStructured Data examples: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.\\n\\nText-to-SQL: Converts natural language to SQL for relational databases.\\nText-to-Cypher: Converts natural language to Cypher for graph databases.\\n\\n\\n\\nSemi-structured Data examples: For vectorstores, queries can combine semantic search with metadata filtering.\\n\\nNatural Language to Metadata Filters: Converts user queries into appropriate metadata filters.\\n\\n\\n\\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\\nNameWhen to UseDescriptionSelf QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).Text to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.\\nAs an example, here is how to use the SelfQueryRetriever to convert natural language queries into metadata filters.\\nmetadata_field_info = schema_for_metadata document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)\\nFurther reading\\nSee our tutorials on text-to-SQL, text-to-Cypher, and query analysis for metadata filters.\\nSee our blog post overview.\\nSee our RAG from Scratch video on query construction.\\n\\nInformation retrieval‚Äã\\nCommon retrieval systems‚Äã\\nLexical search indexes‚Äã\\nMany search engines are based upon matching words in a query to the words in each document.\\nThis approach is called lexical retrieval, using search algorithms that are typically based upon word frequencies.\\nThe intution is simple: a word appears frequently both in the user‚Äôs query and a particular document, then this document might be a good match.\\nThe particular data structure used to implement this is often an inverted index.\\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\\nBM25 and TF-IDF are two popular lexical search algorithms.\\nFurther reading\\nSee the BM25 retriever integration.\\nSee the Elasticsearch retriever integration.\\n\\nVector indexes‚Äã\\nVector indexes are an alternative way to index and store unstructured data.\\nSee our conceptual guide on vectorstores for a detailed overview.\\nIn short, rather than using word frequencies, vectorstores use an embedding model to compress documents into high-dimensional vector representation.\\nThis allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\\nFurther reading\\nSee our how-to guide for more details on working with vectorstores.\\nSee our list of vectorstore integrations.\\nSee Cameron Wolfe\\'s blog post on the basics of vector search.\\n\\nRelational databases‚Äã\\nRelational databases are a fundamental type of structured data storage used in many applications.\\nThey organize data into tables with predefined schemas, where each table represents an entity or relationship.\\nData is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).\\nRelational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.\\nFurther reading\\nSee our tutorial for working with SQL databases.\\nSee our SQL database toolkit.\\n\\nGraph databases‚Äã\\nGraph databases are a specialized type of database designed to store and manage highly interconnected data.\\nUnlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.\\nThis structure allows for efficient representation and querying of complex, interconnected data.\\nGraph databases store data in a graph structure, with nodes, edges, and properties.\\nThey are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services\\nFurther reading\\nSee our tutorial for working with graph databases.\\nSee our list of graph database integrations.\\nSee Neo4j\\'s starter kit for LangChain.\\n\\nRetriever‚Äã\\nLangChain provides a unified interface for interacting with various retrieval systems through the retriever concept. The interface is straightforward:\\n\\nInput: A query (string)\\nOutput: A list of documents (standardized LangChain Document objects)\\n\\nYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\\nRegardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple invoke method:\\ndocs = retriever.invoke(query)\\nFurther reading\\nSee our conceptual guide on retrievers.\\nSee our how-to guide on working with retrievers.\\nEdit this pageWas this page helpful?PreviousRetrieval augmented generation (RAG)NextRetrieversOverviewKey conceptsQuery analysisQuery re-writingQuery constructionInformation retrievalCommon retrieval systemsRetrieverCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/retrievers/', 'title': 'Retrievers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Vector stores', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRetrievers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrieversOn this pageRetrievers\\n\\nPrerequisites\\nVector stores\\nEmbeddings\\nText splitters\\n\\nOverview‚Äã\\nMany different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.\\nWith the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., RAG).\\nBecause of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.\\nThe LangChain retriever interface is straightforward:\\n\\nInput: A query (string)\\nOutput: A list of documents (standardized LangChain Document objects)\\n\\nKey concept‚Äã\\n\\nAll retrievers implement a simple interface for retrieving documents using natural language queries.\\nInterface‚Äã\\nThe only requirement for a retriever is the ability to accepts a query and return documents.\\nIn particular, LangChain\\'s retriever class only requires that the _get_relevant_documents method is implemented, which takes a query: str and returns a list of Document objects that are most relevant to the query.\\nThe underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.\\nA LangChain retriever is a runnable, which is a standard interface is for LangChain components.\\nThis means that it has a few common methods, including invoke, that are used to interact with it. A retriever can be invoked with a query:\\ndocs = retriever.invoke(query)\\nRetrievers return a list of Document objects, which have two attributes:\\n\\npage_content: The content of this document. Currently is a string.\\nmetadata: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).\\n\\nFurther reading\\nSee our how-to guide on building your own custom retriever.\\n\\nCommon types‚Äã\\nDespite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\\nSearch apis‚Äã\\nIt\\'s important to note that retrievers don\\'t need to actually store documents.\\nFor example, we can be built retrievers on top of search APIs that simply return search results!\\nSee our retriever integrations with Amazon Kendra or Wikipedia Search.\\nRelational or graph database‚Äã\\nRetrievers can be built on top of relational or graph databases.\\nIn these cases, query analysis techniques to construct a structured query from natural language is critical.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\\nFurther reading\\nSee our tutorial for context on how to build a retreiver using a SQL database and text-to-SQL.\\nSee our tutorial for context on how to build a retreiver using a graph database and text-to-Cypher.\\n\\nLexical search‚Äã\\nAs discussed in our conceptual review of retrieval, many search engines are based upon matching words in a query to the words in each document.\\nBM25 and TF-IDF are two popular lexical search algorithms.\\nLangChain has retrievers for many popular lexical search algorithms / engines.\\nFurther reading\\nSee the BM25 retriever integration.\\nSee the TF-IDF retriever integration.\\nSee the Elasticsearch retriever integration.\\n\\nVector store‚Äã\\nVector stores are a powerful and efficient way to index and retrieve unstructured data.\\nA vectorstore can be used as a retriever by calling the as_retriever() method.\\nvectorstore = MyVectorStore()retriever = vectorstore.as_retriever()\\nAdvanced retrieval patterns‚Äã\\nEnsemble‚Äã\\nBecause the retriever interface is so simple, returning a list of Document objects given a search query, it is possible to combine multiple retrievers using ensembling.\\nThis is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\\nIt is easy to create an ensemble retriever that combines multiple retrievers with linear weighted scores:\\n# Initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5])\\nWhen ensembling, how do we combine search results from many retrievers?\\nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as Reciprocal Rank Fusion (RRF).\\nSource document retention‚Äã\\nMany retrievers utilize some kind of index to make documents easily searchable.\\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting).\\nWhatever transformation is used, can be very useful to retain a link between the transformed document and the original, giving the retriever the ability to return the original document.\\n\\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\\nFor example, you may use small chunk size for indexing documents in a vectorstore.\\nIf you return only the chunks as the retrieval result, then the model will have lost the original document context for the chunks.\\nLangChain has two different retrievers that can be used to address this challenge.\\nThe Multi-Vector retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.\\nThe ParentDocument retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.\\nNameIndex TypeUses an LLMWhen to UseDescriptionParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.\\nFurther reading\\nSee our how-to guide on using the ParentDocument retriever.\\nSee our how-to guide on using the MultiVector retriever.\\nSee our RAG from Scratch video on the multi vector retriever.\\nEdit this pageWas this page helpful?PreviousRetrievalNextRunnable interfaceOverviewKey conceptInterfaceCommon typesSearch apisRelational or graph databaseLexical searchVector storeAdvanced retrieval patternsEnsembleSource document retentionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/runnables/', 'title': 'Runnable interface | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as language models, output parsers, retrievers, [compiled LangGraph graphs](\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRunnable interface | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRunnable interfaceOn this pageRunnable interface\\nThe Runnable interface is the foundation for working with LangChain components, and it\\'s implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\\nRelated Resources\\nThe \"Runnable\" Interface API Reference provides a detailed overview of the Runnable interface and its methods.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using the LangChain Expression Language (LCEL).\\n\\nOverview of runnable interface‚Äã\\nThe Runnable way defines a standard interface that allows a Runnable component to be:\\n\\nInvoked: A single input is transformed into an output.\\nBatched: Multiple inputs are efficiently transformed into outputs.\\nStreamed: Outputs are streamed as they are produced.\\nInspected: Schematic information about Runnable\\'s input, output, and configuration can be accessed.\\nComposed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.\\n\\nPlease review the LCEL Cheatsheet for some common patterns that involve the Runnable interface and LCEL expressions.\\n\\nOptimized parallel execution (batch)‚Äã\\n\\nLangChain Runnables offer a built-in batch (and batch_as_completed) API that allow you to process multiple inputs in parallel.\\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\\nprocessing can be done in parallel instead of sequentially.\\nThe two batching options are:\\n\\nbatch: Process multiple inputs in parallel, returning results in the same order as the inputs.\\nbatch_as_completed: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\\n\\nThe default implementation of batch and batch_as_completed use a thread pool executor to run the invoke method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\\nSome Runnables may provide their own implementations of batch and batch_as_completed that are optimized for their specific use case (e.g.,\\nrely on a batch API provided by a model provider).\\nnoteThe async versions of abatch and abatch_as_completed relies on asyncio\\'s gather and as_completed functions to run the ainvoke method in parallel.\\ntipWhen processing a large number of inputs using batch or batch_as_completed, users may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary. See the RunnableConfig for more information.Chat Models also have a built-in rate limiter that can be used to control the rate at which requests are made.\\nAsynchronous support‚Äã\\n\\nRunnables expose an asynchronous API, allowing them to be called using the await syntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g., ainvoke, abatch, astream, abatch_as_completed).\\nPlease refer to the Async Programming with LangChain guide for more details.\\nStreaming APIs‚Äã\\n\\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\\nRunnables expose the following three streaming APIs:\\n\\nsync stream and async astream: yields the output a Runnable as it is generated.\\nThe async astream_events: a more advanced streaming API that allows streaming intermediate steps and final output\\nThe legacy async astream_log: a legacy streaming API that streams intermediate steps and final output\\n\\nPlease refer to the Streaming Conceptual Guide for more details on how to stream in LangChain.\\nInput and output types‚Äã\\nEvery Runnable is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\\nRunnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types.\\n\\ninvoke: Accepts an input and returns an output.\\nbatch: Accepts a list of inputs and returns a list of outputs.\\nstream: Accepts an input and returns a generator that yields outputs.\\n\\nThe input type and output type vary by component:\\nComponentInput TypeOutput TypePromptdictionaryPromptValueChatModela string, list of chat messages or a PromptValueChatMessageLLMa string, list of chat messages or a PromptValueStringOutputParserthe output of an LLM or ChatModelDepends on the parserRetrievera stringList of DocumentsToola string or dictionary, depending on the toolDepends on the tool\\nPlease refer to the individual component documentation for more information on the input and output types and how to use them.\\nInspecting schemas‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users. You should probably\\nskip this section unless you have a specific need to inspect the schema of a Runnable.\\nIn more advanced use cases, you may want to programmatically inspect the Runnable and determine what input and output types the Runnable expects and produces.\\nThe Runnable interface provides methods to get the JSON Schema of the input and output types of a Runnable, as well as Pydantic schemas for the input and output types.\\nThese APIs are mostly used internally for unit-testing and by LangServe which uses the APIs for input validation and generation of OpenAPI documentation.\\nIn addition, to the input and output types, some Runnables have been set up with additional run time configuration options.\\nThere are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.\\nPlease see the Configurable Runnables section for more information.\\nMethodDescriptionget_input_schemaGives the Pydantic Schema of the input schema for the Runnable.get_output_schemaGives the Pydantic Schema of the output schema for the Runnable.config_schemaGives the Pydantic Schema of the config schema for the Runnable.get_input_jsonschemaGives the JSONSchema of the input schema for the Runnable.get_output_jsonschemaGives the JSONSchema of the output schema for the Runnable.get_config_jsonschemaGives the JSONSchema of the config schema for the Runnable.\\nWith_types‚Äã\\nLangChain will automatically try to infer the input and output types of a Runnable based on available information.\\nCurrently, this inference does not work well for more complex Runnables that are built using LCEL composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the with_types method (API Reference).\\nRunnableConfig‚Äã\\nAny of the methods that are used to execute the runnable (e.g., invoke, batch, stream, astream_events) accept a second argument called\\nRunnableConfig (API Reference). This argument is a dictionary that contains configuration for the Runnable that will be used\\nat run time during the execution of the runnable.\\nA RunnableConfig can have any of the following properties defined:\\nAttributeDescriptionrun_nameName used for the given Runnable (not inherited).run_idUnique identifier for this call. sub-calls will get their own unique run ids.tagsTags for this call and any sub-calls.metadataMetadata for this call and any sub-calls.callbacksCallbacks for this call and any sub-calls.max_concurrencyMaximum number of parallel calls to make (e.g., used by batch).recursion_limitMaximum number of times a call can recurse (e.g., used by Runnables that return Runnables)configurableRuntime values for configurable attributes of the Runnable.\\nPassing config to the invoke method is done like so:\\nsome_runnable.invoke(   some_input,    config={      \\'run_name\\': \\'my_run\\',       \\'tags\\': [\\'tag1\\', \\'tag2\\'],       \\'metadata\\': {\\'key\\': \\'value\\'}         })\\nPropagation of RunnableConfig‚Äã\\nMany Runnables are composed of other Runnables, and it is important that the RunnableConfig is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\\nIf this were not the case, it would be impossible to set and propagate callbacks or other configuration values like tags and metadata which\\nare expected to be inherited by all sub-calls.\\nThere are two main patterns by which new Runnables are created:\\n\\n\\nDeclaratively using LangChain Expression Language (LCEL):\\nchain = prompt | chat_model | output_parser\\n\\n\\nUsing a custom Runnable  (e.g., RunnableLambda) or using the @tool decorator:\\ndef foo(input):    # Note that .invoke() is used directly here    return bar_runnable.invoke(input)foo_runnable = RunnableLambda(foo)\\n\\n\\nLangChain will try to propagate RunnableConfig automatically for both of the patterns.\\nFor handling the second pattern, LangChain relies on Python\\'s contextvars.\\nIn Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the RunnableConfig to the sub-calls.\\nIn Python 3.9 and 3.10, if you are using async code, you need to manually pass the RunnableConfig through to the Runnable when invoking it.\\nThis is due to a limitation in asyncio\\'s tasks  in Python 3.9 and 3.10 which did\\nnot accept a context argument.\\nPropagating the RunnableConfig manually is done like so:\\nasync def foo(input, config): # <-- Note the config argument    return await bar_runnable.ainvoke(input, config=config)    foo_runnable = RunnableLambda(foo)\\ncautionWhen using Python 3.10 or lower and writing async code, RunnableConfig cannot be propagated\\nautomatically, and you will need to do it manually! This is a common pitfall when\\nattempting to stream data using astream_events and astream_log as these methods\\nrely on proper propagation of callbacks defined inside of RunnableConfig.\\nSetting custom run name, tags, and metadata‚Äã\\nThe run_name, tags, and metadata attributes of the RunnableConfig dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\\nThe run_name is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\\nThe tags and metadata attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in LangSmith as trace attributes that you can\\nfilter and search on.\\nThe attributes will also be propagated to callbacks, and will appear in streaming APIs like astream_events as part of each event in the stream.\\nRelated\\nHow-to trace with LangChain\\n\\nSetting run id‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.\\nYou may need to set a custom run_id for a given run, in case you want\\nto reference it later or correlate it with other systems.\\nThe run_id MUST be a valid UUID string and unique for each run. It is used to identify\\nthe parent run, sub-class will get their own unique run ids automatically.\\nTo set a custom run_id, you can pass it as a key-value pair in the config dictionary when invoking the Runnable:\\nimport uuidrun_id = uuid.uuid4()some_runnable.invoke(   some_input,    config={      \\'run_id\\': run_id   })# Do something with the run_id\\nSetting recursion limit‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.\\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a recursion_limit in the RunnableConfig dictionary. This will limit the number of times a Runnable can recurse.\\nSetting max concurrency‚Äã\\nIf using the batch or batch_as_completed methods, you can set the max_concurrency attribute in the RunnableConfig dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\\ntipIf you\\'re trying to rate limit the number of requests made by a Chat Model, you can use the built-in rate limiter instead of setting max_concurrency, which will be more effective.See the How to handle rate limits guide for more information.\\nSetting configurable‚Äã\\nThe configurable field is used to pass runtime values for configurable attributes of the Runnable.\\nIt is used frequently in LangGraph with\\nLangGraph Persistence\\nand memory.\\nIt is used for a similar purpose in RunnableWithMessageHistory to specify either\\na session_id / conversation_id to keep track of conversation history.\\nIn addition, you can use it to specify any custom configuration options to pass to any Configurable Runnable that they create.\\nSetting callbacks\\x00\\x00‚Äã\\nUse this option to configure callbacks for the runnable at\\nruntime. The callbacks will be passed to all sub-calls made by the runnable.\\nsome_runnable.invoke(   some_input,   {      \"callbacks\": [         SomeCallbackHandler(),         AnotherCallbackHandler(),      ]   })\\nPlease read the Callbacks Conceptual Guide for more information on how to use callbacks in LangChain.\\nimportantIf you\\'re using Python 3.9 or 3.10 in an async environment, you must propagate\\nthe RunnableConfig manually to sub-calls in some cases. Please see the\\nPropagating RunnableConfig section for more information.\\nCreating a runnable from a function‚Äã\\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\\nuseful if using LangChain Expression Language (LCEL) to compose\\nmultiple Runnables and you need to add custom processing logic in one of the steps.\\nThere are two ways to create a custom Runnable from a function:\\n\\nRunnableLambda: Use this for simple transformations where streaming is not required.\\nRunnableGenerator: use this for more complex transformations when streaming is needed.\\n\\nSee the How to run custom functions guide for more information on how to use RunnableLambda and RunnableGenerator.\\nimportantUsers should not try to subclass Runnables to create a new custom Runnable. It is\\nmuch more complex and error-prone than simply using RunnableLambda or RunnableGenerator.\\nConfigurable runnables‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.It helps with configuration of large \"chains\" created using the LangChain Expression Language (LCEL)\\nand is leveraged by LangServe for deployed Runnables.\\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\\n\\nconfigurable_fields: This method allows you to configure specific attributes in a Runnable. For example, the temperature attribute of a chat model.\\nconfigurable_alternatives: This method enables you to specify alternative Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\\n\\nSee the How to configure runtime chain internals guide for more information on how to configure runtime chain internals.Edit this pageWas this page helpful?PreviousRetrieversNextStreamingOverview of runnable interfaceOptimized parallel execution (batch)Asynchronous supportStreaming APIsInput and output typesInspecting schemasRunnableConfigPropagation of RunnableConfigSetting custom run name, tags, and metadataSetting run idSetting recursion limitSetting max concurrencySetting configurableSetting callbacksCreating a runnable from a functionConfigurable runnablesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/structured_outputs/', 'title': 'Structured outputs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Overview', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nStructured outputs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideStructured outputsOn this pageStructured outputs\\nOverview‚Äã\\nFor many applications, such as chatbots, models need to respond to users directly in natural language.\\nHowever, there are scenarios where we need models to output in a structured format.\\nFor example, we might want to store the model output in a database and ensure that the output conforms to the database schema.\\nThis need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\\n\\nKey concepts‚Äã\\n(1) Schema definition: The output structure is represented as a schema, which can be defined in several ways.\\n(2) Returning structured output: The model is given this schema, and is instructed to return output that conforms to it.\\nRecommended usage‚Äã\\nThis pseudocode illustrates the recommended workflow when using structured output.\\nLangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output.\\nThis helper function is available for all model providers that support structured output.\\n# Define schemaschema = {\"foo\": \"bar\"}# Bind schema to modelmodel_with_structure = model.with_structured_output(schema)# Invoke the model to produce structured output that matches the schemastructured_output = model_with_structure.invoke(user_input)\\nSchema definition‚Äã\\nThe central concept is that the output structure of model responses needs to be represented in some way.\\nWhile types of objects you can use depend on the model you\\'re working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\\nThe simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\\nJSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\\n{  \"answer\": \"The answer to the user\\'s question\",  \"followup_question\": \"A followup question the user could ask\"}\\nAs a second example, Pydantic is particularly useful for defining structured output schemas because it offers type hints and validation.\\nHere\\'s an example of a Pydantic schema:\\nfrom pydantic import BaseModel, Fieldclass ResponseFormatter(BaseModel):    \"\"\"Always use this tool to structure your response to the user.\"\"\"    answer: str = Field(description=\"The answer to the user\\'s question\")    followup_question: str = Field(description=\"A followup question the user could ask\")\\nReturning structured output‚Äã\\nWith a schema defined, we need a way to instruct the model to use it.\\nWhile one approach is to include this schema in the prompt and ask nicely for the model to use it, this is not recommended.\\nSeveral more powerful methods that utilizes native features in the model provider\\'s API are available.\\nUsing tool calling‚Äã\\nMany model providers support tool calling, a concept discussed in more detail in our tool calling guide.\\nIn short, tool calling involves binding a tool to a model and, when appropriate, the model can decide to call this tool and ensure its response conforms to the tool\\'s schema.\\nWith this in mind, the central concept is straightforward: simply bind our schema to a model as a tool!\\nHere is an example using the ResponseFormatter schema defined above:\\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)# Bind responseformatter schema as a tool to the modelmodel_with_tools = model.bind_tools([ResponseFormatter])# Invoke the modelai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")API Reference:ChatOpenAI\\nThe arguments of the tool call are already extracted as a dictionary.\\nThis dictionary can be optionally parsed into a Pydantic object, matching our original ResponseFormatter schema.\\n# Get the tool call argumentsai_msg.tool_calls[0][\"args\"]{\\'answer\\': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", \\'followup_question\\': \\'What is the function of ATP in the cell?\\'}# Parse the dictionary into a pydantic objectpydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\\nJSON mode‚Äã\\nIn addition to tool calling, some model providers support a feature called JSON mode.\\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\\nYou can find a table of model providers that support JSON mode here.\\nHere is an example of how to use JSON mode with OpenAI:\\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\").with_structured_output(method=\"json_mode\")ai_msg = model.invoke(\"Return a JSON object with key \\'random_ints\\' and a value of 10 random ints in [0-99]\")ai_msg{\\'random_ints\\': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}API Reference:ChatOpenAI\\nStructured output method‚Äã\\nThere are a few challenges when producing structured output with the above methods:\\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\\n(2) In addition, the model needs to be instructed to always use the tool when we want to enforce structured output, which is a provider specific setting.\\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\\nWith these challenges in mind, LangChain provides a helper function (with_structured_output()) to streamline the process.\\n\\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\\n# Bind the schema to the modelmodel_with_structure = model.with_structured_output(ResponseFormatter)# Invoke the modelstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")# Get back the pydantic objectstructured_outputResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question=\\'What is the function of ATP in the cell?\\')\\nFurther readingFor more details on usage, see our how-to guide.Edit this pageWas this page helpful?PreviousStreamingNextTestingOverviewKey conceptsRecommended usageSchema definitionReturning structured outputUsing tool callingJSON modeStructured output methodCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/testing/', 'title': 'Testing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Testing is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTesting | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTestingOn this pageTesting\\n\\nTesting is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.\\nIn the LangChain ecosystem, we have 2 main types of tests: unit tests and integration tests.\\nFor integrations that implement standard LangChain abstractions, we have a set of standard tests (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.\\nUnit Tests‚Äã\\nDefinition: Unit tests are designed to validate the smallest parts of your code‚Äîindividual functions or methods‚Äîensuring they work as expected in isolation. They do not rely on external systems or integrations.\\nExample: Testing the convert_langchain_aimessage_to_dict function to confirm it correctly converts an AI message to a dictionary format:\\nfrom langchain_core.messages import AIMessage, ToolCall, convert_to_openai_messagesdef test_convert_to_openai_messages():    ai_message = AIMessage(        content=\"Let me call that tool for you!\",        tool_calls=[            ToolCall(name=\\'parrot_multiply_tool\\', id=\\'1\\', args={\\'a\\': 2, \\'b\\': 3}),        ]    )        result = convert_to_openai_messages(ai_message)        expected = {        \"role\": \"assistant\",        \"tool_calls\": [            {                \"type\": \"function\",                \"id\": \"1\",                \"function\": {                    \"name\": \"parrot_multiply_tool\",                    \"arguments\": \\'{\"a\": 2, \"b\": 3}\\',                },            }        ],        \"content\": \"Let me call that tool for you!\",    }    assert result == expected  # Ensure conversion matches expected outputAPI Reference:AIMessage | ToolCall | convert_to_openai_messages\\n\\nIntegration Tests‚Äã\\nDefinition: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.\\nExample: Testing ParrotMultiplyTool with access to an API service that multiplies two numbers and adds 80:\\ndef test_integration_with_service():    tool = ParrotMultiplyTool()    result = tool.invoke({\"a\": 2, \"b\": 3})    assert result == 86\\n\\nStandard Tests‚Äã\\nDefinition: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.\\nExample: Subclassing LangChain\\'s ToolsUnitTests or ToolsIntegrationTests to automatically run standard tests:\\nfrom langchain_tests.unit_tests import ToolsUnitTestsclass TestParrotMultiplyToolUnit(ToolsUnitTests):    @property    def tool_constructor(self):        return ParrotMultiplyTool    def tool_invoke_params_example(self):        return {\"a\": 2, \"b\": 3}API Reference:ToolsUnitTests\\nTo learn more, check out our guide on how to add standard tests to an integration.Edit this pageWas this page helpful?PreviousStructured outputsNextString-in, string-out llmsUnit TestsIntegration TestsStandard TestsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/text_splitters/', 'title': 'Text splitters | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Documents', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nText splitters | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideText splittersOn this pageText splitters\\n\\nPrerequisites\\nDocuments\\nTokenization(/docs/concepts/tokens)\\n\\nOverview‚Äã\\nDocument splitting is often a crucial preprocessing step for many applications.\\nIt involves breaking down large texts into smaller, manageable chunks.\\nThis process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\\nThere are several strategies for splitting documents, each with its own advantages.\\nKey concepts‚Äã\\n\\nText splitters split documents into smaller chunks for use in downstream applications.\\nWhy split documents?‚Äã\\nThere are several reasons to split documents:\\n\\nHandling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\\nOvercoming model limitations: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\\nImproving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\\nEnhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\\nOptimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\\n\\nNow, the next question is how to split the documents into chunks! There are several strategies, each with its own advantages.\\nFurther reading\\nSee Greg Kamradt\\'s chunkviz to visualize different splitting strategies discussed below.\\n\\nApproaches‚Äã\\nLength-based‚Äã\\nThe most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn\\'t exceed a specified size limit.\\nKey benefits of length-based splitting:\\n\\nStraightforward implementation\\nConsistent chunk sizes\\nEasily adaptable to different model requirements\\n\\nTypes of length-based splitting:\\n\\nToken-based: Splits text based on the number of tokens, which is useful when working with language models.\\nCharacter-based: Splits text based on the number of characters, which can be more consistent across different types of text.\\n\\nExample implementation using LangChain\\'s CharacterTextSplitter with token-based splitting:\\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:CharacterTextSplitter\\nFurther reading\\nSee the how-to guide for token-based splitting.\\nSee the how-to guide for character-based splitting.\\n\\nText-structured based‚Äã\\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words.\\nWe can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\\nLangChain\\'s RecursiveCharacterTextSplitter implements this concept:\\n\\nThe RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\\nIf a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\\nThis process continues down to the word level if necessary.\\n\\nHere is example usage:\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:RecursiveCharacterTextSplitter\\nFurther reading\\nSee the how-to guide for recursive text splitting.\\n\\nDocument-structured based‚Äã\\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files.\\nIn these cases, it\\'s beneficial to split the document based on its structure, as it often naturally groups semantically related text.\\nKey benefits of structure-based splitting:\\n\\nPreserves the logical organization of the document\\nMaintains context within each chunk\\nCan be more effective for downstream tasks like retrieval or summarization\\n\\nExamples of structure-based splitting:\\n\\nMarkdown: Split based on headers (e.g., #, ##, ###)\\nHTML: Split using tags\\nJSON: Split by object or array elements\\nCode: Split by functions, classes, or logical blocks\\n\\nFurther reading\\nSee the how-to guide for Markdown splitting.\\nSee the how-to guide for Recursive JSON splitting.\\nSee the how-to guide for Code splitting.\\nSee the how-to guide for HTML splitting.\\n\\nSemantic meaning based‚Äã\\nUnlike the previous methods, semantic-based splitting actually considers the content of the text.\\nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text\\'s semantics.\\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text meaning.\\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:\\n\\nStart with the first few sentences and generate an embedding.\\nMove to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\\nCompare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\\n\\nThis technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.\\nFurther reading\\nSee the how-to guide for splitting text based on semantic meaning.\\nSee Greg Kamradt\\'s notebook showcasing semantic splitting.\\nEdit this pageWas this page helpful?PreviousString-in, string-out llmsNextTokensOverviewKey conceptsWhy split documents?ApproachesLength-basedText-structured basedDocument-structured basedSemantic meaning basedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tool_calling/', 'title': 'Tool calling | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Tools', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTool calling | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTool callingOn this pageTool calling\\nPrerequisites\\nTools\\nChat Models\\n\\nOverview‚Äã\\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\\nBut what about cases where we want a model to also interact directly with systems, such as databases or an API?\\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\\nThis need motivates the concept of tool calling. You can use tool calling to request model responses that match a particular schema.\\ninfoYou will sometimes hear the term function calling. We use this term interchangeably with tool calling.\\n\\nKey concepts‚Äã\\n(1) Tool Creation: Use the @tool decorator to create a tool. A tool is an association between a function and its schema.\\n(2) Tool Binding: The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.\\n(3) Tool Calling: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool\\'s input schema.\\n(4) Tool Execution: The tool can be executed using the arguments provided by the model.\\n\\nRecommended usage‚Äã\\nThis pseudocode illustrates the recommended workflow for using tool calling.\\nCreated tools are passed to .bind_tools() method as a list.\\nThis model can be called, as usual. If a tool call is made, model\\'s response will contain the tool call arguments.\\nThe tool call arguments can be passed directly to the tool.\\n# Tool creationtools = [my_tool]# Tool bindingmodel_with_tools = model.bind_tools(tools)# Tool calling response = model_with_tools.invoke(user_input)\\nTool creation‚Äã\\nThe recommended way to create a tool is using the @tool decorator.\\nfrom langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.\"\"\"    return a * bAPI Reference:tool\\nFurther reading\\nSee our conceptual guide on tools for more details.\\nSee our model integrations that support tool calling.\\nSee our how-to guide on tool calling.\\n\\nTool binding‚Äã\\nMany model providers support tool calling.\\ntipSee our model integration page for a list of providers that support tool calling.\\nThe central concept to understand is that LangChain provides a standardized interface for connecting tools to models.\\nThe .bind_tools() method can be used to specify which tools are available for a model to call.\\nmodel_with_tools = model.bind_tools(tools_list)\\nAs a specific example, let\\'s take a function multiply and bind it as a tool to a model that supports tool calling.\\ndef multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.    Args:        a: first int        b: second int    \"\"\"    return a * bllm_with_tools = tool_calling_model.bind_tools([multiply])\\nTool calling‚Äã\\n\\nA key principle of tool calling is that the model decides when to use a tool based on the input\\'s relevance. The model doesn\\'t always need to call a tool.\\nFor example, given an unrelated input, the model would not call the tool:\\nresult = llm_with_tools.invoke(\"Hello world!\")\\nThe result would be an AIMessage containing the model\\'s response in natural language (e.g., \"Hello!\").\\nHowever, if we pass an input relevant to the tool, the model should choose to call it:\\nresult = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\\nAs before, the output result will be an AIMessage.\\nBut, if the tool was called, result will have a tool_calls attribute.\\nThis attribute includes everything needed to execute the tool, including the tool name and input arguments:\\nresult.tool_calls{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 2, \\'b\\': 3}, \\'id\\': \\'xxx\\', \\'type\\': \\'tool_call\\'}\\nFor more details on usage, see our how-to guides!\\nTool execution‚Äã\\nTools implement the Runnable interface, which means that they can be invoked (e.g., tool.invoke(args)) directly.\\nLangGraph offers pre-built components (e.g., ToolNode) that will often invoke the tool in behalf of the user.\\nFurther reading\\nSee our how-to guide on tool calling.\\nSee the LangGraph documentation on using ToolNode.\\n\\nBest practices‚Äã\\nWhen designing tools to be used by a model, it is important to keep in mind that:\\n\\nModels that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.\\nModels will perform better if the tools have well-chosen names and descriptions.\\nSimple, narrowly scoped tools are easier for models to use than complex tools.\\nAsking the model to select from a large list of tools poses challenges for the model.\\nEdit this pageWas this page helpful?PreviousTokensNextToolsOverviewKey conceptsRecommended usageTool creationTool bindingTool callingTool executionBest practicesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tools/', 'title': 'Tools | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Chat models', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTools | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideToolsOn this pageTools\\nPrerequisites\\nChat models\\n\\nOverview‚Äã\\nThe tool abstraction in LangChain associates a Python function with a schema that defines the function\\'s name, description and expected arguments.\\nTools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.\\nKey concepts‚Äã\\n\\nTools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.\\nCreate tools using the @tool decorator, which simplifies the process of tool creation, supporting the following:\\n\\nAutomatically infer the tool\\'s name, description and expected arguments, while also supporting customization.\\nDefining tools that return artifacts (e.g. images, dataframes, etc.)\\nHiding input arguments from the schema (and hence from the model) using injected tool arguments.\\n\\n\\n\\nTool interface‚Äã\\nThe tool interface is defined in the BaseTool class which is a subclass of the Runnable Interface.\\nThe key attributes that correspond to the tool\\'s schema:\\n\\nname: The name of the tool.\\ndescription: A description of what the tool does.\\nargs: Property that returns the JSON schema for the tool\\'s arguments.\\n\\nThe key methods to execute the function associated with the tool:\\n\\ninvoke: Invokes the tool with the given arguments.\\nainvoke: Invokes the tool with the given arguments, asynchronously. Used for async programming with Langchain.\\n\\nCreate tools using the @tool decorator‚Äã\\nThe recommended way to create tools is using the @tool decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with @tool to create a tool that implements the Tool Interface.\\nfrom langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:   \"\"\"Multiply two numbers.\"\"\"   return a * bAPI Reference:tool\\nFor more details on how to create tools, see the how to create custom tools guide.\\nnoteLangChain has a few other ways to create tools; e.g., by sub-classing the BaseTool class or by using StructuredTool. These methods are shown in the how to create custom tools guide, but\\nwe generally recommend using the @tool decorator for most cases.\\nUse the tool directly‚Äã\\nOnce you have defined a tool, you can use it directly by calling the function. For example, to use the multiply tool defined above:\\nmultiply.invoke({\"a\": 2, \"b\": 3})\\nInspect‚Äã\\nYou can also inspect the tool\\'s schema and other properties:\\nprint(multiply.name) # multiplyprint(multiply.description) # Multiply two numbers.print(multiply.args) # {# \\'type\\': \\'object\\', # \\'properties\\': {\\'a\\': {\\'type\\': \\'integer\\'}, \\'b\\': {\\'type\\': \\'integer\\'}}, # \\'required\\': [\\'a\\', \\'b\\']# }\\nnoteIf you\\'re using pre-built LangChain or LangGraph components like create_react_agent,you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.\\nConfiguring the schema‚Äã\\nThe @tool decorator offers additional options to configure the schema of the tool (e.g., modify name, description\\nor parse the function\\'s doc-string to infer the schema).\\nPlease see the API reference for @tool for more details and review the how to create custom tools guide for examples.\\nTool artifacts‚Äã\\nTools are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool\\'s execution that we want to make accessible to downstream components in our chain or agent, but that we don\\'t want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\\n@tool(response_format=\"content_and_artifact\")def some_tool(...) -> Tuple[str, Any]:    \"\"\"Tool that does something.\"\"\"    ...    return \\'Message for chat model\\', some_artifact \\nSee how to return artifacts from tools for more details.\\nSpecial type annotations‚Äã\\nThere are a number of special type annotations that can be used in the tool\\'s function signature to configure the run time behavior of the tool.\\nThe following type annotations will end up removing the argument from the tool\\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\\n\\nInjectedToolArg: Value should be injected manually at runtime using .invoke or .ainvoke.\\nRunnableConfig: Pass in the RunnableConfig object to the tool.\\nInjectedState: Pass in the overall state of the LangGraph graph to the tool.\\nInjectedStore: Pass in the LangGraph store object to the tool.\\n\\nYou can also use the Annotated type with a string literal to provide a description for the corresponding argument that WILL be exposed in the tool\\'s schema.\\n\\nAnnotated[..., \"string literal\"] -- Adds a description to the argument that will be exposed in the tool\\'s schema.\\n\\nInjectedToolArg‚Äã\\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the InjectedToolArg annotation, which allows certain parameters to be hidden from the tool\\'s schema.\\nFor example, if a tool requires a user_id to be injected dynamically at runtime, it can be structured in this way:\\nfrom langchain_core.tools import tool, InjectedToolArg@tooldef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:    \"\"\"Tool that processes input data.\"\"\"    return f\"User {user_id} processed {input_data}\"API Reference:tool | InjectedToolArg\\nAnnotating the user_id argument with InjectedToolArg tells LangChain that this argument should not be exposed as part of the\\ntool\\'s schema.\\nSee how to pass run time values to tools for more details on how to use InjectedToolArg.\\nRunnableConfig‚Äã\\nYou can use the RunnableConfig object to pass custom run time values to tools.\\nIf you need to access the RunnableConfig object from within a tool. This can be done by using the RunnableConfig annotation in the tool\\'s function signature.\\nfrom langchain_core.runnables import RunnableConfig@toolasync def some_func(..., config: RunnableConfig) -> ...:    \"\"\"Tool that does something.\"\"\"    # do something with config    ...await some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})API Reference:RunnableConfig\\nThe config will not be part of the tool\\'s schema and will be injected at runtime with appropriate values.\\nnoteYou may need to access the config object to manually propagate it to subclass. This happens if you\\'re working with python 3.9 / 3.10 in an async environment and need to manually propagate the config object to sub-calls.Please read Propagation RunnableConfig for more details to learn how to propagate the RunnableConfig down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\\nInjectedState‚Äã\\nPlease see the InjectedState documentation for more details.\\nInjectedStore‚Äã\\nPlease see the InjectedStore documentation for more details.\\nBest practices‚Äã\\nWhen designing tools to be used by models, keep the following in mind:\\n\\nTools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\\nDesign simple and narrowly scoped tools, as they are easier for models to use correctly.\\nUse chat models that support tool-calling APIs to take advantage of tools.\\n\\nToolkits‚Äã\\n\\nLangChain has a concept of toolkits. This a very thin abstraction that groups tools together that\\nare designed to be used together for specific tasks.\\nInterface‚Äã\\nAll Toolkits expose a get_tools method which returns a list of tools. You can therefore do:\\n# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()\\nRelated resources‚Äã\\nSee the following resources for more information:\\n\\nAPI Reference for @tool\\nHow to create custom tools\\nHow to pass run time values to tools\\nAll LangChain tool how-to guides\\nAdditional how-to guides that show usage with LangGraph\\nTool integrations, see the tool integration docs.\\nEdit this pageWas this page helpful?PreviousTool callingNextTracingOverviewKey conceptsTool interfaceCreate tools using the @tool decoratorUse the tool directlyInspectConfiguring the schemaTool artifactsSpecial type annotationsInjectedToolArgRunnableConfigInjectedStateInjectedStoreBest practicesToolkitsInterfaceRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tracing/', 'title': 'Tracing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'A trace is essentially a series of steps that your application takes to go from input to output.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTracing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTracingTracing\\n\\nA trace is essentially a series of steps that your application takes to go from input to output.\\nTraces contain individual steps called runs. These can be individual calls from a model, retriever,\\ntool, or sub-chains.\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\nFor a deeper dive, check out this LangSmith conceptual guide.Edit this pageWas this page helpful?PreviousToolsNextVector storesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/vectorstores/', 'title': 'Vector stores | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Embeddings', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nVector stores | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideVector storesOn this pageVector stores\\n\\nPrerequisites\\nEmbeddings\\nText splitters\\n\\nNoteThis conceptual overview focuses on text-based indexing and retrieval for simplicity.\\nHowever, embedding models can be multi-modal\\nand vector stores can be used to store and retrieve a variety of data types beyond text.\\nOverview‚Äã\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\\nThese vectors, called embeddings, capture the semantic meaning of data that has been embedded.\\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n\\nIntegrations‚Äã\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\nPlease see the full list of LangChain vectorstore integrations.\\nInterface‚Äã\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\\nThe key methods are:\\n\\nadd_documents: Add a list of texts to the vector store.\\ndelete: Delete a list of documents from the vector store.\\nsimilarity_search: Search for similar documents to a given query.\\n\\nInitialization‚Äã\\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\\nWe will use LangChain\\'s InMemoryVectorStore implementation to illustrate the API.\\nfrom langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())API Reference:InMemoryVectorStore\\nAdding documents‚Äã\\nTo add documents, use the add_documents method.\\nThis API works with a list of Document objects.\\nDocument objects all have page_content and metadata attributes, making them a universal way to store unstructured text and associated metadata.\\nfrom langchain_core.documents import Documentdocument_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)API Reference:Document\\nYou should usually provide IDs for the documents you add to the vector store, so\\nthat instead of adding the same document multiple times, you can update the existing document.\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\\nDelete‚Äã\\nTo delete documents, use the delete method which takes a list of document IDs to delete.\\nvector_store.delete(ids=[\"doc1\"])\\nSearch‚Äã\\nVector stores embed and store the documents that added.\\nIf we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.\\nThis captures two important concepts: first, there needs to be a way to measure the similarity between the query and any embedded document.\\nSecond, there needs to be an algorithm to efficiently perform this similarity search across all embedded documents.\\nSimilarity metrics‚Äã\\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\\n\\nCosine Similarity: Measures the cosine of the angle between two vectors.\\nEuclidean Distance: Measures the straight-line distance between two points.\\nDot Product: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer\\nto the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\\nFurther reading\\nSee this documentation from Google on similarity metrics to consider with embeddings.\\nSee Pinecone\\'s blog post on similarity metrics.\\nSee OpenAI\\'s FAQ on what similarity metric to use with OpenAI embeddings.\\n\\nSimilarity search‚Äã\\nGiven a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over all the embedded documents to find the most similar ones.\\nThere are various ways to do this. As an example, many vectorstores implement HNSW (Hierarchical Navigable Small World), a graph-based index structure that allows for efficient similarity search.\\nRegardless of the search algorithm used under the hood, the LangChain vectorstore interface has a similarity_search method for all integrations.\\nThis will take the search query, create an embedding, find similar documents, and return them as a list of Documents.\\nquery = \"my query\"docs = vectorstore.similarity_search(query)\\nMany vectorstores support search parameters to be passed with the similarity_search method. See the documentation for the specific vectorstore you are using to see what parameters are supported.\\nAs an example Pinecone several parameters that are important general concepts:\\nMany vectorstores support the k, which controls the number of Documents to return, and filter, which allows for filtering documents by metadata.\\n\\nquery (str) ‚Äì Text to look up documents similar to.\\nk (int) ‚Äì Number of Documents to return. Defaults to 4.\\nfilter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata\\n\\nFurther reading\\nSee the how-to guide for more details on how to use the similarity_search method.\\nSee the integrations page for more details on arguments that can be passed in to the similarity_search method for specific vectorstores.\\n\\nMetadata filtering‚Äã\\nWhile vectorstore implement a search algorithm to efficiently search over all the embedded documents to find the most similar ones, many also support filtering on metadata.\\nMetadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:\\n\\nSemantic search: Query the unstructured data directly, often via embedding or keyword similarity.\\nMetadata search: Apply structured query to the metadata, filtering specific documents.\\n\\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\\nHere is example usage with Pinecone, showing that we filter for all documents that have the metadata key source with value tweet.\\nvectorstore.similarity_search(    \"LangChain provides abstractions to make working with LLMs easy\",    k=2,    filter={\"source\": \"tweet\"},)\\nFurther reading\\nSee Pinecone\\'s documentation on filtering with metadata.\\nSee the list of LangChain vectorstore integrations that support metadata filtering.\\n\\nAdvanced search and retrieval techniques‚Äã\\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\\nFor example, maximal marginal relevance is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\\nAs a second example, some vector stores offer built-in hybrid-search to combine keyword and semantic similarity search, which marries the benefits of both approaches.\\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with similarity_search.\\nSee this how-to guide on hybrid search for more details.\\nNameWhen to useDescriptionHybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.Edit this pageWas this page helpful?PreviousTracingNextWhy LangChain?OverviewIntegrationsInterfaceInitializationAdding documentsDeleteSearchSimilarity metricsSimilarity searchMetadata filteringAdvanced search and retrieval techniquesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_sorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hujkk1svZH5",
        "outputId": "9a35d461-c392-4253-b51c-4ee824d1b803"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(d_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jhv2rmcEvMey",
        "outputId": "42515465-4af4-436f-8c25-29f68a63956a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://python.langchain.com/docs/concepts/vectorstores/', 'title': 'Vector stores | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Embeddings', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nVector stores | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideVector storesOn this pageVector stores\\n\\nPrerequisites\\nEmbeddings\\nText splitters\\n\\nNoteThis conceptual overview focuses on text-based indexing and retrieval for simplicity.\\nHowever, embedding models can be multi-modal\\nand vector stores can be used to store and retrieve a variety of data types beyond text.\\nOverview‚Äã\\nVector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\\nThese vectors, called embeddings, capture the semantic meaning of data that has been embedded.\\nVector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\\n\\nIntegrations‚Äã\\nLangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\\nPlease see the full list of LangChain vectorstore integrations.\\nInterface‚Äã\\nLangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\\nThe interface consists of basic methods for writing, deleting and searching for documents in the vector store.\\nThe key methods are:\\n\\nadd_documents: Add a list of texts to the vector store.\\ndelete: Delete a list of documents from the vector store.\\nsimilarity_search: Search for similar documents to a given query.\\n\\nInitialization‚Äã\\nMost vectors in LangChain accept an embedding model as an argument when initializing the vector store.\\nWe will use LangChain\\'s InMemoryVectorStore implementation to illustrate the API.\\nfrom langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())API Reference:InMemoryVectorStore\\nAdding documents‚Äã\\nTo add documents, use the add_documents method.\\nThis API works with a list of Document objects.\\nDocument objects all have page_content and metadata attributes, making them a universal way to store unstructured text and associated metadata.\\nfrom langchain_core.documents import Documentdocument_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)API Reference:Document\\nYou should usually provide IDs for the documents you add to the vector store, so\\nthat instead of adding the same document multiple times, you can update the existing document.\\nvector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\\nDelete‚Äã\\nTo delete documents, use the delete method which takes a list of document IDs to delete.\\nvector_store.delete(ids=[\"doc1\"])\\nSearch‚Äã\\nVector stores embed and store the documents that added.\\nIf we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.\\nThis captures two important concepts: first, there needs to be a way to measure the similarity between the query and any embedded document.\\nSecond, there needs to be an algorithm to efficiently perform this similarity search across all embedded documents.\\nSimilarity metrics‚Äã\\nA critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\\n\\nCosine Similarity: Measures the cosine of the angle between two vectors.\\nEuclidean Distance: Measures the straight-line distance between two points.\\nDot Product: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer\\nto the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\\nFurther reading\\nSee this documentation from Google on similarity metrics to consider with embeddings.\\nSee Pinecone\\'s blog post on similarity metrics.\\nSee OpenAI\\'s FAQ on what similarity metric to use with OpenAI embeddings.\\n\\nSimilarity search‚Äã\\nGiven a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over all the embedded documents to find the most similar ones.\\nThere are various ways to do this. As an example, many vectorstores implement HNSW (Hierarchical Navigable Small World), a graph-based index structure that allows for efficient similarity search.\\nRegardless of the search algorithm used under the hood, the LangChain vectorstore interface has a similarity_search method for all integrations.\\nThis will take the search query, create an embedding, find similar documents, and return them as a list of Documents.\\nquery = \"my query\"docs = vectorstore.similarity_search(query)\\nMany vectorstores support search parameters to be passed with the similarity_search method. See the documentation for the specific vectorstore you are using to see what parameters are supported.\\nAs an example Pinecone several parameters that are important general concepts:\\nMany vectorstores support the k, which controls the number of Documents to return, and filter, which allows for filtering documents by metadata.\\n\\nquery (str) ‚Äì Text to look up documents similar to.\\nk (int) ‚Äì Number of Documents to return. Defaults to 4.\\nfilter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata\\n\\nFurther reading\\nSee the how-to guide for more details on how to use the similarity_search method.\\nSee the integrations page for more details on arguments that can be passed in to the similarity_search method for specific vectorstores.\\n\\nMetadata filtering‚Äã\\nWhile vectorstore implement a search algorithm to efficiently search over all the embedded documents to find the most similar ones, many also support filtering on metadata.\\nMetadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:\\n\\nSemantic search: Query the unstructured data directly, often via embedding or keyword similarity.\\nMetadata search: Apply structured query to the metadata, filtering specific documents.\\n\\nVector store support for metadata filtering is typically dependent on the underlying vector store implementation.\\nHere is example usage with Pinecone, showing that we filter for all documents that have the metadata key source with value tweet.\\nvectorstore.similarity_search(    \"LangChain provides abstractions to make working with LLMs easy\",    k=2,    filter={\"source\": \"tweet\"},)\\nFurther reading\\nSee Pinecone\\'s documentation on filtering with metadata.\\nSee the list of LangChain vectorstore integrations that support metadata filtering.\\n\\nAdvanced search and retrieval techniques‚Äã\\nWhile algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\\nFor example, maximal marginal relevance is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\\nAs a second example, some vector stores offer built-in hybrid-search to combine keyword and semantic similarity search, which marries the benefits of both approaches.\\nAt the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with similarity_search.\\nSee this how-to guide on hybrid search for more details.\\nNameWhen to useDescriptionHybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.Edit this pageWas this page helpful?PreviousTracingNextWhy LangChain?OverviewIntegrationsInterfaceInitializationAdding documentsDeleteSearchSimilarity metricsSimilarity searchMetadata filteringAdvanced search and retrieval techniquesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tracing/', 'title': 'Tracing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'A trace is essentially a series of steps that your application takes to go from input to output.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTracing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTracingTracing\\n\\nA trace is essentially a series of steps that your application takes to go from input to output.\\nTraces contain individual steps called runs. These can be individual calls from a model, retriever,\\ntool, or sub-chains.\\nTracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\\nFor a deeper dive, check out this LangSmith conceptual guide.Edit this pageWas this page helpful?PreviousToolsNextVector storesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tools/', 'title': 'Tools | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Chat models', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTools | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideToolsOn this pageTools\\nPrerequisites\\nChat models\\n\\nOverview‚Äã\\nThe tool abstraction in LangChain associates a Python function with a schema that defines the function\\'s name, description and expected arguments.\\nTools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.\\nKey concepts‚Äã\\n\\nTools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.\\nCreate tools using the @tool decorator, which simplifies the process of tool creation, supporting the following:\\n\\nAutomatically infer the tool\\'s name, description and expected arguments, while also supporting customization.\\nDefining tools that return artifacts (e.g. images, dataframes, etc.)\\nHiding input arguments from the schema (and hence from the model) using injected tool arguments.\\n\\n\\n\\nTool interface‚Äã\\nThe tool interface is defined in the BaseTool class which is a subclass of the Runnable Interface.\\nThe key attributes that correspond to the tool\\'s schema:\\n\\nname: The name of the tool.\\ndescription: A description of what the tool does.\\nargs: Property that returns the JSON schema for the tool\\'s arguments.\\n\\nThe key methods to execute the function associated with the tool:\\n\\ninvoke: Invokes the tool with the given arguments.\\nainvoke: Invokes the tool with the given arguments, asynchronously. Used for async programming with Langchain.\\n\\nCreate tools using the @tool decorator‚Äã\\nThe recommended way to create tools is using the @tool decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with @tool to create a tool that implements the Tool Interface.\\nfrom langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:   \"\"\"Multiply two numbers.\"\"\"   return a * bAPI Reference:tool\\nFor more details on how to create tools, see the how to create custom tools guide.\\nnoteLangChain has a few other ways to create tools; e.g., by sub-classing the BaseTool class or by using StructuredTool. These methods are shown in the how to create custom tools guide, but\\nwe generally recommend using the @tool decorator for most cases.\\nUse the tool directly‚Äã\\nOnce you have defined a tool, you can use it directly by calling the function. For example, to use the multiply tool defined above:\\nmultiply.invoke({\"a\": 2, \"b\": 3})\\nInspect‚Äã\\nYou can also inspect the tool\\'s schema and other properties:\\nprint(multiply.name) # multiplyprint(multiply.description) # Multiply two numbers.print(multiply.args) # {# \\'type\\': \\'object\\', # \\'properties\\': {\\'a\\': {\\'type\\': \\'integer\\'}, \\'b\\': {\\'type\\': \\'integer\\'}}, # \\'required\\': [\\'a\\', \\'b\\']# }\\nnoteIf you\\'re using pre-built LangChain or LangGraph components like create_react_agent,you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.\\nConfiguring the schema‚Äã\\nThe @tool decorator offers additional options to configure the schema of the tool (e.g., modify name, description\\nor parse the function\\'s doc-string to infer the schema).\\nPlease see the API reference for @tool for more details and review the how to create custom tools guide for examples.\\nTool artifacts‚Äã\\nTools are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool\\'s execution that we want to make accessible to downstream components in our chain or agent, but that we don\\'t want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\\n@tool(response_format=\"content_and_artifact\")def some_tool(...) -> Tuple[str, Any]:    \"\"\"Tool that does something.\"\"\"    ...    return \\'Message for chat model\\', some_artifact \\nSee how to return artifacts from tools for more details.\\nSpecial type annotations‚Äã\\nThere are a number of special type annotations that can be used in the tool\\'s function signature to configure the run time behavior of the tool.\\nThe following type annotations will end up removing the argument from the tool\\'s schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\\n\\nInjectedToolArg: Value should be injected manually at runtime using .invoke or .ainvoke.\\nRunnableConfig: Pass in the RunnableConfig object to the tool.\\nInjectedState: Pass in the overall state of the LangGraph graph to the tool.\\nInjectedStore: Pass in the LangGraph store object to the tool.\\n\\nYou can also use the Annotated type with a string literal to provide a description for the corresponding argument that WILL be exposed in the tool\\'s schema.\\n\\nAnnotated[..., \"string literal\"] -- Adds a description to the argument that will be exposed in the tool\\'s schema.\\n\\nInjectedToolArg‚Äã\\nThere are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the InjectedToolArg annotation, which allows certain parameters to be hidden from the tool\\'s schema.\\nFor example, if a tool requires a user_id to be injected dynamically at runtime, it can be structured in this way:\\nfrom langchain_core.tools import tool, InjectedToolArg@tooldef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:    \"\"\"Tool that processes input data.\"\"\"    return f\"User {user_id} processed {input_data}\"API Reference:tool | InjectedToolArg\\nAnnotating the user_id argument with InjectedToolArg tells LangChain that this argument should not be exposed as part of the\\ntool\\'s schema.\\nSee how to pass run time values to tools for more details on how to use InjectedToolArg.\\nRunnableConfig‚Äã\\nYou can use the RunnableConfig object to pass custom run time values to tools.\\nIf you need to access the RunnableConfig object from within a tool. This can be done by using the RunnableConfig annotation in the tool\\'s function signature.\\nfrom langchain_core.runnables import RunnableConfig@toolasync def some_func(..., config: RunnableConfig) -> ...:    \"\"\"Tool that does something.\"\"\"    # do something with config    ...await some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})API Reference:RunnableConfig\\nThe config will not be part of the tool\\'s schema and will be injected at runtime with appropriate values.\\nnoteYou may need to access the config object to manually propagate it to subclass. This happens if you\\'re working with python 3.9 / 3.10 in an async environment and need to manually propagate the config object to sub-calls.Please read Propagation RunnableConfig for more details to learn how to propagate the RunnableConfig down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\\nInjectedState‚Äã\\nPlease see the InjectedState documentation for more details.\\nInjectedStore‚Äã\\nPlease see the InjectedStore documentation for more details.\\nBest practices‚Äã\\nWhen designing tools to be used by models, keep the following in mind:\\n\\nTools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\\nDesign simple and narrowly scoped tools, as they are easier for models to use correctly.\\nUse chat models that support tool-calling APIs to take advantage of tools.\\n\\nToolkits‚Äã\\n\\nLangChain has a concept of toolkits. This a very thin abstraction that groups tools together that\\nare designed to be used together for specific tasks.\\nInterface‚Äã\\nAll Toolkits expose a get_tools method which returns a list of tools. You can therefore do:\\n# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()\\nRelated resources‚Äã\\nSee the following resources for more information:\\n\\nAPI Reference for @tool\\nHow to create custom tools\\nHow to pass run time values to tools\\nAll LangChain tool how-to guides\\nAdditional how-to guides that show usage with LangGraph\\nTool integrations, see the tool integration docs.\\nEdit this pageWas this page helpful?PreviousTool callingNextTracingOverviewKey conceptsTool interfaceCreate tools using the @tool decoratorUse the tool directlyInspectConfiguring the schemaTool artifactsSpecial type annotationsInjectedToolArgRunnableConfigInjectedStateInjectedStoreBest practicesToolkitsInterfaceRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/tool_calling/', 'title': 'Tool calling | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Tools', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTool calling | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTool callingOn this pageTool calling\\nPrerequisites\\nTools\\nChat Models\\n\\nOverview‚Äã\\nMany AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\\nBut what about cases where we want a model to also interact directly with systems, such as databases or an API?\\nThese systems often have a particular input schema; for example, APIs frequently have a required payload structure.\\nThis need motivates the concept of tool calling. You can use tool calling to request model responses that match a particular schema.\\ninfoYou will sometimes hear the term function calling. We use this term interchangeably with tool calling.\\n\\nKey concepts‚Äã\\n(1) Tool Creation: Use the @tool decorator to create a tool. A tool is an association between a function and its schema.\\n(2) Tool Binding: The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.\\n(3) Tool Calling: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool\\'s input schema.\\n(4) Tool Execution: The tool can be executed using the arguments provided by the model.\\n\\nRecommended usage‚Äã\\nThis pseudocode illustrates the recommended workflow for using tool calling.\\nCreated tools are passed to .bind_tools() method as a list.\\nThis model can be called, as usual. If a tool call is made, model\\'s response will contain the tool call arguments.\\nThe tool call arguments can be passed directly to the tool.\\n# Tool creationtools = [my_tool]# Tool bindingmodel_with_tools = model.bind_tools(tools)# Tool calling response = model_with_tools.invoke(user_input)\\nTool creation‚Äã\\nThe recommended way to create a tool is using the @tool decorator.\\nfrom langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.\"\"\"    return a * bAPI Reference:tool\\nFurther reading\\nSee our conceptual guide on tools for more details.\\nSee our model integrations that support tool calling.\\nSee our how-to guide on tool calling.\\n\\nTool binding‚Äã\\nMany model providers support tool calling.\\ntipSee our model integration page for a list of providers that support tool calling.\\nThe central concept to understand is that LangChain provides a standardized interface for connecting tools to models.\\nThe .bind_tools() method can be used to specify which tools are available for a model to call.\\nmodel_with_tools = model.bind_tools(tools_list)\\nAs a specific example, let\\'s take a function multiply and bind it as a tool to a model that supports tool calling.\\ndef multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.    Args:        a: first int        b: second int    \"\"\"    return a * bllm_with_tools = tool_calling_model.bind_tools([multiply])\\nTool calling‚Äã\\n\\nA key principle of tool calling is that the model decides when to use a tool based on the input\\'s relevance. The model doesn\\'t always need to call a tool.\\nFor example, given an unrelated input, the model would not call the tool:\\nresult = llm_with_tools.invoke(\"Hello world!\")\\nThe result would be an AIMessage containing the model\\'s response in natural language (e.g., \"Hello!\").\\nHowever, if we pass an input relevant to the tool, the model should choose to call it:\\nresult = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\\nAs before, the output result will be an AIMessage.\\nBut, if the tool was called, result will have a tool_calls attribute.\\nThis attribute includes everything needed to execute the tool, including the tool name and input arguments:\\nresult.tool_calls{\\'name\\': \\'multiply\\', \\'args\\': {\\'a\\': 2, \\'b\\': 3}, \\'id\\': \\'xxx\\', \\'type\\': \\'tool_call\\'}\\nFor more details on usage, see our how-to guides!\\nTool execution‚Äã\\nTools implement the Runnable interface, which means that they can be invoked (e.g., tool.invoke(args)) directly.\\nLangGraph offers pre-built components (e.g., ToolNode) that will often invoke the tool in behalf of the user.\\nFurther reading\\nSee our how-to guide on tool calling.\\nSee the LangGraph documentation on using ToolNode.\\n\\nBest practices‚Äã\\nWhen designing tools to be used by a model, it is important to keep in mind that:\\n\\nModels that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.\\nModels will perform better if the tools have well-chosen names and descriptions.\\nSimple, narrowly scoped tools are easier for models to use than complex tools.\\nAsking the model to select from a large list of tools poses challenges for the model.\\nEdit this pageWas this page helpful?PreviousTokensNextToolsOverviewKey conceptsRecommended usageTool creationTool bindingTool callingTool executionBest practicesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/text_splitters/', 'title': 'Text splitters | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Documents', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nText splitters | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideText splittersOn this pageText splitters\\n\\nPrerequisites\\nDocuments\\nTokenization(/docs/concepts/tokens)\\n\\nOverview‚Äã\\nDocument splitting is often a crucial preprocessing step for many applications.\\nIt involves breaking down large texts into smaller, manageable chunks.\\nThis process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\\nThere are several strategies for splitting documents, each with its own advantages.\\nKey concepts‚Äã\\n\\nText splitters split documents into smaller chunks for use in downstream applications.\\nWhy split documents?‚Äã\\nThere are several reasons to split documents:\\n\\nHandling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\\nOvercoming model limitations: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\\nImproving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\\nEnhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\\nOptimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\\n\\nNow, the next question is how to split the documents into chunks! There are several strategies, each with its own advantages.\\nFurther reading\\nSee Greg Kamradt\\'s chunkviz to visualize different splitting strategies discussed below.\\n\\nApproaches‚Äã\\nLength-based‚Äã\\nThe most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn\\'t exceed a specified size limit.\\nKey benefits of length-based splitting:\\n\\nStraightforward implementation\\nConsistent chunk sizes\\nEasily adaptable to different model requirements\\n\\nTypes of length-based splitting:\\n\\nToken-based: Splits text based on the number of tokens, which is useful when working with language models.\\nCharacter-based: Splits text based on the number of characters, which can be more consistent across different types of text.\\n\\nExample implementation using LangChain\\'s CharacterTextSplitter with token-based splitting:\\nfrom langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:CharacterTextSplitter\\nFurther reading\\nSee the how-to guide for token-based splitting.\\nSee the how-to guide for character-based splitting.\\n\\nText-structured based‚Äã\\nText is naturally organized into hierarchical units such as paragraphs, sentences, and words.\\nWe can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\\nLangChain\\'s RecursiveCharacterTextSplitter implements this concept:\\n\\nThe RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\\nIf a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\\nThis process continues down to the word level if necessary.\\n\\nHere is example usage:\\nfrom langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:RecursiveCharacterTextSplitter\\nFurther reading\\nSee the how-to guide for recursive text splitting.\\n\\nDocument-structured based‚Äã\\nSome documents have an inherent structure, such as HTML, Markdown, or JSON files.\\nIn these cases, it\\'s beneficial to split the document based on its structure, as it often naturally groups semantically related text.\\nKey benefits of structure-based splitting:\\n\\nPreserves the logical organization of the document\\nMaintains context within each chunk\\nCan be more effective for downstream tasks like retrieval or summarization\\n\\nExamples of structure-based splitting:\\n\\nMarkdown: Split based on headers (e.g., #, ##, ###)\\nHTML: Split using tags\\nJSON: Split by object or array elements\\nCode: Split by functions, classes, or logical blocks\\n\\nFurther reading\\nSee the how-to guide for Markdown splitting.\\nSee the how-to guide for Recursive JSON splitting.\\nSee the how-to guide for Code splitting.\\nSee the how-to guide for HTML splitting.\\n\\nSemantic meaning based‚Äã\\nUnlike the previous methods, semantic-based splitting actually considers the content of the text.\\nWhile other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text\\'s semantics.\\nThere are several ways to implement this, but conceptually the approach is split text when there are significant changes in text meaning.\\nAs an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:\\n\\nStart with the first few sentences and generate an embedding.\\nMove to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\\nCompare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\\n\\nThis technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.\\nFurther reading\\nSee the how-to guide for splitting text based on semantic meaning.\\nSee Greg Kamradt\\'s notebook showcasing semantic splitting.\\nEdit this pageWas this page helpful?PreviousString-in, string-out llmsNextTokensOverviewKey conceptsWhy split documents?ApproachesLength-basedText-structured basedDocument-structured basedSemantic meaning basedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/testing/', 'title': 'Testing | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Testing is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nTesting | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTestingOn this pageTesting\\n\\nTesting is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.\\nIn the LangChain ecosystem, we have 2 main types of tests: unit tests and integration tests.\\nFor integrations that implement standard LangChain abstractions, we have a set of standard tests (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.\\nUnit Tests‚Äã\\nDefinition: Unit tests are designed to validate the smallest parts of your code‚Äîindividual functions or methods‚Äîensuring they work as expected in isolation. They do not rely on external systems or integrations.\\nExample: Testing the convert_langchain_aimessage_to_dict function to confirm it correctly converts an AI message to a dictionary format:\\nfrom langchain_core.messages import AIMessage, ToolCall, convert_to_openai_messagesdef test_convert_to_openai_messages():    ai_message = AIMessage(        content=\"Let me call that tool for you!\",        tool_calls=[            ToolCall(name=\\'parrot_multiply_tool\\', id=\\'1\\', args={\\'a\\': 2, \\'b\\': 3}),        ]    )        result = convert_to_openai_messages(ai_message)        expected = {        \"role\": \"assistant\",        \"tool_calls\": [            {                \"type\": \"function\",                \"id\": \"1\",                \"function\": {                    \"name\": \"parrot_multiply_tool\",                    \"arguments\": \\'{\"a\": 2, \"b\": 3}\\',                },            }        ],        \"content\": \"Let me call that tool for you!\",    }    assert result == expected  # Ensure conversion matches expected outputAPI Reference:AIMessage | ToolCall | convert_to_openai_messages\\n\\nIntegration Tests‚Äã\\nDefinition: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.\\nExample: Testing ParrotMultiplyTool with access to an API service that multiplies two numbers and adds 80:\\ndef test_integration_with_service():    tool = ParrotMultiplyTool()    result = tool.invoke({\"a\": 2, \"b\": 3})    assert result == 86\\n\\nStandard Tests‚Äã\\nDefinition: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.\\nExample: Subclassing LangChain\\'s ToolsUnitTests or ToolsIntegrationTests to automatically run standard tests:\\nfrom langchain_tests.unit_tests import ToolsUnitTestsclass TestParrotMultiplyToolUnit(ToolsUnitTests):    @property    def tool_constructor(self):        return ParrotMultiplyTool    def tool_invoke_params_example(self):        return {\"a\": 2, \"b\": 3}API Reference:ToolsUnitTests\\nTo learn more, check out our guide on how to add standard tests to an integration.Edit this pageWas this page helpful?PreviousStructured outputsNextString-in, string-out llmsUnit TestsIntegration TestsStandard TestsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/structured_outputs/', 'title': 'Structured outputs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Overview', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nStructured outputs | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideStructured outputsOn this pageStructured outputs\\nOverview‚Äã\\nFor many applications, such as chatbots, models need to respond to users directly in natural language.\\nHowever, there are scenarios where we need models to output in a structured format.\\nFor example, we might want to store the model output in a database and ensure that the output conforms to the database schema.\\nThis need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\\n\\nKey concepts‚Äã\\n(1) Schema definition: The output structure is represented as a schema, which can be defined in several ways.\\n(2) Returning structured output: The model is given this schema, and is instructed to return output that conforms to it.\\nRecommended usage‚Äã\\nThis pseudocode illustrates the recommended workflow when using structured output.\\nLangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output.\\nThis helper function is available for all model providers that support structured output.\\n# Define schemaschema = {\"foo\": \"bar\"}# Bind schema to modelmodel_with_structure = model.with_structured_output(schema)# Invoke the model to produce structured output that matches the schemastructured_output = model_with_structure.invoke(user_input)\\nSchema definition‚Äã\\nThe central concept is that the output structure of model responses needs to be represented in some way.\\nWhile types of objects you can use depend on the model you\\'re working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\\nThe simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\\nJSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\\n{  \"answer\": \"The answer to the user\\'s question\",  \"followup_question\": \"A followup question the user could ask\"}\\nAs a second example, Pydantic is particularly useful for defining structured output schemas because it offers type hints and validation.\\nHere\\'s an example of a Pydantic schema:\\nfrom pydantic import BaseModel, Fieldclass ResponseFormatter(BaseModel):    \"\"\"Always use this tool to structure your response to the user.\"\"\"    answer: str = Field(description=\"The answer to the user\\'s question\")    followup_question: str = Field(description=\"A followup question the user could ask\")\\nReturning structured output‚Äã\\nWith a schema defined, we need a way to instruct the model to use it.\\nWhile one approach is to include this schema in the prompt and ask nicely for the model to use it, this is not recommended.\\nSeveral more powerful methods that utilizes native features in the model provider\\'s API are available.\\nUsing tool calling‚Äã\\nMany model providers support tool calling, a concept discussed in more detail in our tool calling guide.\\nIn short, tool calling involves binding a tool to a model and, when appropriate, the model can decide to call this tool and ensure its response conforms to the tool\\'s schema.\\nWith this in mind, the central concept is straightforward: simply bind our schema to a model as a tool!\\nHere is an example using the ResponseFormatter schema defined above:\\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)# Bind responseformatter schema as a tool to the modelmodel_with_tools = model.bind_tools([ResponseFormatter])# Invoke the modelai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")API Reference:ChatOpenAI\\nThe arguments of the tool call are already extracted as a dictionary.\\nThis dictionary can be optionally parsed into a Pydantic object, matching our original ResponseFormatter schema.\\n# Get the tool call argumentsai_msg.tool_calls[0][\"args\"]{\\'answer\\': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", \\'followup_question\\': \\'What is the function of ATP in the cell?\\'}# Parse the dictionary into a pydantic objectpydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\\nJSON mode‚Äã\\nIn addition to tool calling, some model providers support a feature called JSON mode.\\nThis supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\\nYou can find a table of model providers that support JSON mode here.\\nHere is an example of how to use JSON mode with OpenAI:\\nfrom langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\").with_structured_output(method=\"json_mode\")ai_msg = model.invoke(\"Return a JSON object with key \\'random_ints\\' and a value of 10 random ints in [0-99]\")ai_msg{\\'random_ints\\': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}API Reference:ChatOpenAI\\nStructured output method‚Äã\\nThere are a few challenges when producing structured output with the above methods:\\n(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\\n(2) In addition, the model needs to be instructed to always use the tool when we want to enforce structured output, which is a provider specific setting.\\n(3) When JSON mode is used, the output needs to be parsed into a JSON object.\\nWith these challenges in mind, LangChain provides a helper function (with_structured_output()) to streamline the process.\\n\\nThis both binds the schema to the model as a tool and parses the output to the specified output schema.\\n# Bind the schema to the modelmodel_with_structure = model.with_structured_output(ResponseFormatter)# Invoke the modelstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")# Get back the pydantic objectstructured_outputResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell\\'s supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question=\\'What is the function of ATP in the cell?\\')\\nFurther readingFor more details on usage, see our how-to guide.Edit this pageWas this page helpful?PreviousStreamingNextTestingOverviewKey conceptsRecommended usageSchema definitionReturning structured outputUsing tool callingJSON modeStructured output methodCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/runnables/', 'title': 'Runnable interface | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as language models, output parsers, retrievers, [compiled LangGraph graphs](\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRunnable interface | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRunnable interfaceOn this pageRunnable interface\\nThe Runnable interface is the foundation for working with LangChain components, and it\\'s implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\\nThis guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\\nRelated Resources\\nThe \"Runnable\" Interface API Reference provides a detailed overview of the Runnable interface and its methods.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using the LangChain Expression Language (LCEL).\\n\\nOverview of runnable interface‚Äã\\nThe Runnable way defines a standard interface that allows a Runnable component to be:\\n\\nInvoked: A single input is transformed into an output.\\nBatched: Multiple inputs are efficiently transformed into outputs.\\nStreamed: Outputs are streamed as they are produced.\\nInspected: Schematic information about Runnable\\'s input, output, and configuration can be accessed.\\nComposed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.\\n\\nPlease review the LCEL Cheatsheet for some common patterns that involve the Runnable interface and LCEL expressions.\\n\\nOptimized parallel execution (batch)‚Äã\\n\\nLangChain Runnables offer a built-in batch (and batch_as_completed) API that allow you to process multiple inputs in parallel.\\nUsing these methods can significantly improve performance when needing to process multiple independent inputs, as the\\nprocessing can be done in parallel instead of sequentially.\\nThe two batching options are:\\n\\nbatch: Process multiple inputs in parallel, returning results in the same order as the inputs.\\nbatch_as_completed: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\\n\\nThe default implementation of batch and batch_as_completed use a thread pool executor to run the invoke method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\\nSome Runnables may provide their own implementations of batch and batch_as_completed that are optimized for their specific use case (e.g.,\\nrely on a batch API provided by a model provider).\\nnoteThe async versions of abatch and abatch_as_completed relies on asyncio\\'s gather and as_completed functions to run the ainvoke method in parallel.\\ntipWhen processing a large number of inputs using batch or batch_as_completed, users may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary. See the RunnableConfig for more information.Chat Models also have a built-in rate limiter that can be used to control the rate at which requests are made.\\nAsynchronous support‚Äã\\n\\nRunnables expose an asynchronous API, allowing them to be called using the await syntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g., ainvoke, abatch, astream, abatch_as_completed).\\nPlease refer to the Async Programming with LangChain guide for more details.\\nStreaming APIs‚Äã\\n\\nStreaming is critical in making applications based on LLMs feel responsive to end-users.\\nRunnables expose the following three streaming APIs:\\n\\nsync stream and async astream: yields the output a Runnable as it is generated.\\nThe async astream_events: a more advanced streaming API that allows streaming intermediate steps and final output\\nThe legacy async astream_log: a legacy streaming API that streams intermediate steps and final output\\n\\nPlease refer to the Streaming Conceptual Guide for more details on how to stream in LangChain.\\nInput and output types‚Äã\\nEvery Runnable is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\\nRunnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types.\\n\\ninvoke: Accepts an input and returns an output.\\nbatch: Accepts a list of inputs and returns a list of outputs.\\nstream: Accepts an input and returns a generator that yields outputs.\\n\\nThe input type and output type vary by component:\\nComponentInput TypeOutput TypePromptdictionaryPromptValueChatModela string, list of chat messages or a PromptValueChatMessageLLMa string, list of chat messages or a PromptValueStringOutputParserthe output of an LLM or ChatModelDepends on the parserRetrievera stringList of DocumentsToola string or dictionary, depending on the toolDepends on the tool\\nPlease refer to the individual component documentation for more information on the input and output types and how to use them.\\nInspecting schemas‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users. You should probably\\nskip this section unless you have a specific need to inspect the schema of a Runnable.\\nIn more advanced use cases, you may want to programmatically inspect the Runnable and determine what input and output types the Runnable expects and produces.\\nThe Runnable interface provides methods to get the JSON Schema of the input and output types of a Runnable, as well as Pydantic schemas for the input and output types.\\nThese APIs are mostly used internally for unit-testing and by LangServe which uses the APIs for input validation and generation of OpenAPI documentation.\\nIn addition, to the input and output types, some Runnables have been set up with additional run time configuration options.\\nThere are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.\\nPlease see the Configurable Runnables section for more information.\\nMethodDescriptionget_input_schemaGives the Pydantic Schema of the input schema for the Runnable.get_output_schemaGives the Pydantic Schema of the output schema for the Runnable.config_schemaGives the Pydantic Schema of the config schema for the Runnable.get_input_jsonschemaGives the JSONSchema of the input schema for the Runnable.get_output_jsonschemaGives the JSONSchema of the output schema for the Runnable.get_config_jsonschemaGives the JSONSchema of the config schema for the Runnable.\\nWith_types‚Äã\\nLangChain will automatically try to infer the input and output types of a Runnable based on available information.\\nCurrently, this inference does not work well for more complex Runnables that are built using LCEL composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the with_types method (API Reference).\\nRunnableConfig‚Äã\\nAny of the methods that are used to execute the runnable (e.g., invoke, batch, stream, astream_events) accept a second argument called\\nRunnableConfig (API Reference). This argument is a dictionary that contains configuration for the Runnable that will be used\\nat run time during the execution of the runnable.\\nA RunnableConfig can have any of the following properties defined:\\nAttributeDescriptionrun_nameName used for the given Runnable (not inherited).run_idUnique identifier for this call. sub-calls will get their own unique run ids.tagsTags for this call and any sub-calls.metadataMetadata for this call and any sub-calls.callbacksCallbacks for this call and any sub-calls.max_concurrencyMaximum number of parallel calls to make (e.g., used by batch).recursion_limitMaximum number of times a call can recurse (e.g., used by Runnables that return Runnables)configurableRuntime values for configurable attributes of the Runnable.\\nPassing config to the invoke method is done like so:\\nsome_runnable.invoke(   some_input,    config={      \\'run_name\\': \\'my_run\\',       \\'tags\\': [\\'tag1\\', \\'tag2\\'],       \\'metadata\\': {\\'key\\': \\'value\\'}         })\\nPropagation of RunnableConfig‚Äã\\nMany Runnables are composed of other Runnables, and it is important that the RunnableConfig is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\\nIf this were not the case, it would be impossible to set and propagate callbacks or other configuration values like tags and metadata which\\nare expected to be inherited by all sub-calls.\\nThere are two main patterns by which new Runnables are created:\\n\\n\\nDeclaratively using LangChain Expression Language (LCEL):\\nchain = prompt | chat_model | output_parser\\n\\n\\nUsing a custom Runnable  (e.g., RunnableLambda) or using the @tool decorator:\\ndef foo(input):    # Note that .invoke() is used directly here    return bar_runnable.invoke(input)foo_runnable = RunnableLambda(foo)\\n\\n\\nLangChain will try to propagate RunnableConfig automatically for both of the patterns.\\nFor handling the second pattern, LangChain relies on Python\\'s contextvars.\\nIn Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the RunnableConfig to the sub-calls.\\nIn Python 3.9 and 3.10, if you are using async code, you need to manually pass the RunnableConfig through to the Runnable when invoking it.\\nThis is due to a limitation in asyncio\\'s tasks  in Python 3.9 and 3.10 which did\\nnot accept a context argument.\\nPropagating the RunnableConfig manually is done like so:\\nasync def foo(input, config): # <-- Note the config argument    return await bar_runnable.ainvoke(input, config=config)    foo_runnable = RunnableLambda(foo)\\ncautionWhen using Python 3.10 or lower and writing async code, RunnableConfig cannot be propagated\\nautomatically, and you will need to do it manually! This is a common pitfall when\\nattempting to stream data using astream_events and astream_log as these methods\\nrely on proper propagation of callbacks defined inside of RunnableConfig.\\nSetting custom run name, tags, and metadata‚Äã\\nThe run_name, tags, and metadata attributes of the RunnableConfig dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\\nThe run_name is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\\nThe tags and metadata attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\\nUsing these attributes can be useful for tracking and debugging runs, as they will be surfaced in LangSmith as trace attributes that you can\\nfilter and search on.\\nThe attributes will also be propagated to callbacks, and will appear in streaming APIs like astream_events as part of each event in the stream.\\nRelated\\nHow-to trace with LangChain\\n\\nSetting run id‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.\\nYou may need to set a custom run_id for a given run, in case you want\\nto reference it later or correlate it with other systems.\\nThe run_id MUST be a valid UUID string and unique for each run. It is used to identify\\nthe parent run, sub-class will get their own unique run ids automatically.\\nTo set a custom run_id, you can pass it as a key-value pair in the config dictionary when invoking the Runnable:\\nimport uuidrun_id = uuid.uuid4()some_runnable.invoke(   some_input,    config={      \\'run_id\\': run_id   })# Do something with the run_id\\nSetting recursion limit‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.\\nSome Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a recursion_limit in the RunnableConfig dictionary. This will limit the number of times a Runnable can recurse.\\nSetting max concurrency‚Äã\\nIf using the batch or batch_as_completed methods, you can set the max_concurrency attribute in the RunnableConfig dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\\ntipIf you\\'re trying to rate limit the number of requests made by a Chat Model, you can use the built-in rate limiter instead of setting max_concurrency, which will be more effective.See the How to handle rate limits guide for more information.\\nSetting configurable‚Äã\\nThe configurable field is used to pass runtime values for configurable attributes of the Runnable.\\nIt is used frequently in LangGraph with\\nLangGraph Persistence\\nand memory.\\nIt is used for a similar purpose in RunnableWithMessageHistory to specify either\\na session_id / conversation_id to keep track of conversation history.\\nIn addition, you can use it to specify any custom configuration options to pass to any Configurable Runnable that they create.\\nSetting callbacks\\x00\\x00‚Äã\\nUse this option to configure callbacks for the runnable at\\nruntime. The callbacks will be passed to all sub-calls made by the runnable.\\nsome_runnable.invoke(   some_input,   {      \"callbacks\": [         SomeCallbackHandler(),         AnotherCallbackHandler(),      ]   })\\nPlease read the Callbacks Conceptual Guide for more information on how to use callbacks in LangChain.\\nimportantIf you\\'re using Python 3.9 or 3.10 in an async environment, you must propagate\\nthe RunnableConfig manually to sub-calls in some cases. Please see the\\nPropagating RunnableConfig section for more information.\\nCreating a runnable from a function‚Äã\\nYou may need to create a custom Runnable that runs arbitrary logic. This is especially\\nuseful if using LangChain Expression Language (LCEL) to compose\\nmultiple Runnables and you need to add custom processing logic in one of the steps.\\nThere are two ways to create a custom Runnable from a function:\\n\\nRunnableLambda: Use this for simple transformations where streaming is not required.\\nRunnableGenerator: use this for more complex transformations when streaming is needed.\\n\\nSee the How to run custom functions guide for more information on how to use RunnableLambda and RunnableGenerator.\\nimportantUsers should not try to subclass Runnables to create a new custom Runnable. It is\\nmuch more complex and error-prone than simply using RunnableLambda or RunnableGenerator.\\nConfigurable runnables‚Äã\\nnoteThis is an advanced feature that is unnecessary for most users.It helps with configuration of large \"chains\" created using the LangChain Expression Language (LCEL)\\nand is leveraged by LangServe for deployed Runnables.\\nSometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\\nTo simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\\n\\nconfigurable_fields: This method allows you to configure specific attributes in a Runnable. For example, the temperature attribute of a chat model.\\nconfigurable_alternatives: This method enables you to specify alternative Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\\n\\nSee the How to configure runtime chain internals guide for more information on how to configure runtime chain internals.Edit this pageWas this page helpful?PreviousRetrieversNextStreamingOverview of runnable interfaceOptimized parallel execution (batch)Asynchronous supportStreaming APIsInput and output typesInspecting schemasRunnableConfigPropagation of RunnableConfigSetting custom run name, tags, and metadataSetting run idSetting recursion limitSetting max concurrencySetting configurableSetting callbacksCreating a runnable from a functionConfigurable runnablesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/retrievers/', 'title': 'Retrievers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Vector stores', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRetrievers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrieversOn this pageRetrievers\\n\\nPrerequisites\\nVector stores\\nEmbeddings\\nText splitters\\n\\nOverview‚Äã\\nMany different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.\\nWith the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., RAG).\\nBecause of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.\\nThe LangChain retriever interface is straightforward:\\n\\nInput: A query (string)\\nOutput: A list of documents (standardized LangChain Document objects)\\n\\nKey concept‚Äã\\n\\nAll retrievers implement a simple interface for retrieving documents using natural language queries.\\nInterface‚Äã\\nThe only requirement for a retriever is the ability to accepts a query and return documents.\\nIn particular, LangChain\\'s retriever class only requires that the _get_relevant_documents method is implemented, which takes a query: str and returns a list of Document objects that are most relevant to the query.\\nThe underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.\\nA LangChain retriever is a runnable, which is a standard interface is for LangChain components.\\nThis means that it has a few common methods, including invoke, that are used to interact with it. A retriever can be invoked with a query:\\ndocs = retriever.invoke(query)\\nRetrievers return a list of Document objects, which have two attributes:\\n\\npage_content: The content of this document. Currently is a string.\\nmetadata: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).\\n\\nFurther reading\\nSee our how-to guide on building your own custom retriever.\\n\\nCommon types‚Äã\\nDespite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\\nSearch apis‚Äã\\nIt\\'s important to note that retrievers don\\'t need to actually store documents.\\nFor example, we can be built retrievers on top of search APIs that simply return search results!\\nSee our retriever integrations with Amazon Kendra or Wikipedia Search.\\nRelational or graph database‚Äã\\nRetrievers can be built on top of relational or graph databases.\\nIn these cases, query analysis techniques to construct a structured query from natural language is critical.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\\nFurther reading\\nSee our tutorial for context on how to build a retreiver using a SQL database and text-to-SQL.\\nSee our tutorial for context on how to build a retreiver using a graph database and text-to-Cypher.\\n\\nLexical search‚Äã\\nAs discussed in our conceptual review of retrieval, many search engines are based upon matching words in a query to the words in each document.\\nBM25 and TF-IDF are two popular lexical search algorithms.\\nLangChain has retrievers for many popular lexical search algorithms / engines.\\nFurther reading\\nSee the BM25 retriever integration.\\nSee the TF-IDF retriever integration.\\nSee the Elasticsearch retriever integration.\\n\\nVector store‚Äã\\nVector stores are a powerful and efficient way to index and retrieve unstructured data.\\nA vectorstore can be used as a retriever by calling the as_retriever() method.\\nvectorstore = MyVectorStore()retriever = vectorstore.as_retriever()\\nAdvanced retrieval patterns‚Äã\\nEnsemble‚Äã\\nBecause the retriever interface is so simple, returning a list of Document objects given a search query, it is possible to combine multiple retrievers using ensembling.\\nThis is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\\nIt is easy to create an ensemble retriever that combines multiple retrievers with linear weighted scores:\\n# Initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5])\\nWhen ensembling, how do we combine search results from many retrievers?\\nThis motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as Reciprocal Rank Fusion (RRF).\\nSource document retention‚Äã\\nMany retrievers utilize some kind of index to make documents easily searchable.\\nThe process of indexing can include a transformation step (e.g., vectorstores often use document splitting).\\nWhatever transformation is used, can be very useful to retain a link between the transformed document and the original, giving the retriever the ability to return the original document.\\n\\nThis is particularly useful in AI applications, because it ensures no loss in document context for the model.\\nFor example, you may use small chunk size for indexing documents in a vectorstore.\\nIf you return only the chunks as the retrieval result, then the model will have lost the original document context for the chunks.\\nLangChain has two different retrievers that can be used to address this challenge.\\nThe Multi-Vector retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.\\nThe ParentDocument retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.\\nNameIndex TypeUses an LLMWhen to UseDescriptionParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.\\nFurther reading\\nSee our how-to guide on using the ParentDocument retriever.\\nSee our how-to guide on using the MultiVector retriever.\\nSee our RAG from Scratch video on the multi vector retriever.\\nEdit this pageWas this page helpful?PreviousRetrievalNextRunnable interfaceOverviewKey conceptInterfaceCommon typesSearch apisRelational or graph databaseLexical searchVector storeAdvanced retrieval patternsEnsembleSource document retentionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/retrieval/', 'title': 'Retrieval | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Retrievers', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nRetrieval | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrievalOn this pageRetrieval\\nPrerequisites\\nRetrievers\\nVector stores\\nEmbeddings\\nText splitters\\n\\nSecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\\nThere are inherent risks in doing this.\\nMake sure that your database connection permissions are scoped as narrowly as possible for your application\\'s needs.\\nThis will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\\nFor more on general security best practices, see our security guide.\\nOverview‚Äã\\nRetrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\\nThese systems accommodate various data formats:\\n\\nUnstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\\nStructured data is typically housed in relational or graph databases with defined schemas.\\n\\nDespite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\\nModels play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\\nThis translation enables more intuitive and flexible interactions with complex data structures.\\nKey concepts‚Äã\\n\\n(1) Query analysis: A process where models transform or construct search queries to optimize retrieval.\\n(2) Information retrieval: Search queries are used to fetch information from various retrieval systems.\\nQuery analysis‚Äã\\nWhile users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\\nQuery analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\\n\\nQuery Re-writing: Queries can be re-written or expanded to improve semantic or lexical searches.\\nQuery Construction: Search indexes may require structured queries (e.g., SQL for databases).\\n\\nQuery analysis employs models to transform or construct optimized search queries from raw user input.\\nQuery re-writing‚Äã\\nRetrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\\nTo achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\\nThis transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\\nHere are some key benefits of using models for query analysis in unstructured data retrieval:\\n\\nQuery Clarification: Models can rephrase ambiguous or poorly worded queries for clarity.\\nSemantic Understanding: They can capture the intent behind a query, going beyond literal keyword matching.\\nQuery Expansion: Models can generate related terms or concepts to broaden the search scope.\\nComplex Query Handling: They can break down multi-part questions into simpler sub-queries.\\n\\nVarious techniques have been developed to leverage models for query re-writing, including:\\nNameWhen to useDescriptionMulti-queryWhen you want to ensure high recall in retrieval by providing multiple phrasings of a question.Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. Paper.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. Paper.\\nAs an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.\\nThese can then be run sequentially or in parallel on a downstream retrieval system.\\nfrom typing import Listfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import SystemMessage, HumanMessage# Define a pydantic model to enforce the output structureclass Questions(BaseModel):    questions: List[str] = Field(        description=\"A list of sub-questions related to the input query.\"    )# Create an instance of the model and enforce the output structuremodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) structured_model = model.with_structured_output(Questions)# Define the system promptsystem = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\\\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \\\\n\"\"\"# Pass the question to the modelquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"questions = structured_model.invoke([SystemMessage(content=system)]+[HumanMessage(content=question)])API Reference:ChatOpenAI | SystemMessage | HumanMessage\\ntipSee our RAG from Scratch videos for a few different specific approaches:\\nMulti-query\\nDecomposition\\nStep-back\\nHyDE\\n\\nQuery construction‚Äã\\nQuery analysis also can focus on translating natural language queries into specialized query languages or filters.\\nThis translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.\\n\\n\\nStructured Data examples: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.\\n\\nText-to-SQL: Converts natural language to SQL for relational databases.\\nText-to-Cypher: Converts natural language to Cypher for graph databases.\\n\\n\\n\\nSemi-structured Data examples: For vectorstores, queries can combine semantic search with metadata filtering.\\n\\nNatural Language to Metadata Filters: Converts user queries into appropriate metadata filters.\\n\\n\\n\\nThese approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\\nNameWhen to UseDescriptionSelf QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).Text to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.\\nAs an example, here is how to use the SelfQueryRetriever to convert natural language queries into metadata filters.\\nmetadata_field_info = schema_for_metadata document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)\\nFurther reading\\nSee our tutorials on text-to-SQL, text-to-Cypher, and query analysis for metadata filters.\\nSee our blog post overview.\\nSee our RAG from Scratch video on query construction.\\n\\nInformation retrieval‚Äã\\nCommon retrieval systems‚Äã\\nLexical search indexes‚Äã\\nMany search engines are based upon matching words in a query to the words in each document.\\nThis approach is called lexical retrieval, using search algorithms that are typically based upon word frequencies.\\nThe intution is simple: a word appears frequently both in the user‚Äôs query and a particular document, then this document might be a good match.\\nThe particular data structure used to implement this is often an inverted index.\\nThis types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\\nUsing this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\\nBM25 and TF-IDF are two popular lexical search algorithms.\\nFurther reading\\nSee the BM25 retriever integration.\\nSee the Elasticsearch retriever integration.\\n\\nVector indexes‚Äã\\nVector indexes are an alternative way to index and store unstructured data.\\nSee our conceptual guide on vectorstores for a detailed overview.\\nIn short, rather than using word frequencies, vectorstores use an embedding model to compress documents into high-dimensional vector representation.\\nThis allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\\nFurther reading\\nSee our how-to guide for more details on working with vectorstores.\\nSee our list of vectorstore integrations.\\nSee Cameron Wolfe\\'s blog post on the basics of vector search.\\n\\nRelational databases‚Äã\\nRelational databases are a fundamental type of structured data storage used in many applications.\\nThey organize data into tables with predefined schemas, where each table represents an entity or relationship.\\nData is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).\\nRelational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.\\nFurther reading\\nSee our tutorial for working with SQL databases.\\nSee our SQL database toolkit.\\n\\nGraph databases‚Äã\\nGraph databases are a specialized type of database designed to store and manage highly interconnected data.\\nUnlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.\\nThis structure allows for efficient representation and querying of complex, interconnected data.\\nGraph databases store data in a graph structure, with nodes, edges, and properties.\\nThey are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services\\nFurther reading\\nSee our tutorial for working with graph databases.\\nSee our list of graph database integrations.\\nSee Neo4j\\'s starter kit for LangChain.\\n\\nRetriever‚Äã\\nLangChain provides a unified interface for interacting with various retrieval systems through the retriever concept. The interface is straightforward:\\n\\nInput: A query (string)\\nOutput: A list of documents (standardized LangChain Document objects)\\n\\nYou can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\\nFor example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\\nRegardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple invoke method:\\ndocs = retriever.invoke(query)\\nFurther reading\\nSee our conceptual guide on retrievers.\\nSee our how-to guide on working with retrievers.\\nEdit this pageWas this page helpful?PreviousRetrieval augmented generation (RAG)NextRetrieversOverviewKey conceptsQuery analysisQuery re-writingQuery constructionInformation retrievalCommon retrieval systemsRetrieverCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/prompt_templates/', 'title': 'Prompt Templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Prompt templates help to translate user input and parameters into instructions for a language model.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nPrompt Templates | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guidePrompt TemplatesOn this pagePrompt Templates\\nPrompt templates help to translate user input and parameters into instructions for a language model.\\nThis can be used to guide a model\\'s response, helping it understand the context and generate relevant and coherent language-based output.\\nPrompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\\nPrompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\\nThe reason this PromptValue exists is to make it easy to switch between strings and messages.\\nThere are a few different types of prompt templates:\\nString PromptTemplates‚Äã\\nThese prompt templates are used to format a single string, and generally are used for simpler inputs.\\nFor example, a common way to construct and use a PromptTemplate is as follows:\\nfrom langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplate\\nChatPromptTemplates‚Äã\\nThese prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\\nFor example, a common way to construct and use a ChatPromptTemplate is as follows:\\nfrom langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplate\\nIn the above example, this ChatPromptTemplate will construct two messages when called.\\nThe first is a system message, that has no variables to format.\\nThe second is a HumanMessage, and will be formatted by the topic variable the user passes in.\\nMessagesPlaceholder‚Äã\\n\\nThis prompt template is responsible for adding a list of messages in a particular place.\\nIn the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\\nBut what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\\nThis is how you use MessagesPlaceholder.\\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\\nThis will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\\nIf we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\\nThis is useful for letting a list of messages be slotted into a particular spot.\\nAn alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\\nprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])\\nFor specifics on how to use prompt templates, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousOutput parsersNextRetrieval augmented generation (RAG)String PromptTemplatesChatPromptTemplatesMessagesPlaceholderCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/output_parsers/', 'title': 'Output parsers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'The information here refers to parsers that take a text output from a model try to parse it into a more structured representation.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nOutput parsers | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideOutput parsersOutput parsers\\n\\nnoteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\\nMore and more models are supporting function (or tool) calling, which handles this automatically.\\nIt is recommended to use function/tool calling rather than output parsing.\\nSee documentation for that here.\\nOutput parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\\nUseful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\\nLangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\\n\\nName: The name of the output parser\\nSupports Streaming: Whether the output parser supports streaming.\\nHas Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\\nCalls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\\nInput Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\\nOutput Type: The output type of the object returned by the parser.\\nDescription: Our commentary on this output parser and when to use it.\\n\\nNameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionStr‚úÖstr | MessageStringParses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).JSON‚úÖ‚úÖstr | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML‚úÖ‚úÖstr | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic\\'s).CSV‚úÖ‚úÖstr | MessageList[str]Returns a list of comma separated values.OutputFixing‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame‚úÖstr | MessagedictUseful for doing operations with pandas DataFrames.Enum‚úÖstr | MessageEnumParses response into one of the provided enum values.Datetime‚úÖstr | Messagedatetime.datetimeParses response into a datetime string.Structured‚úÖstr | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.\\nFor specifics on how to use output parsers, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousMultimodalityNextPrompt TemplatesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/messages/', 'title': 'Messages | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Chat Models', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nMessages | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideMessagesOn this pageMessages\\nPrerequisites\\nChat Models\\n\\nOverview‚Äã\\nMessages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\\nEach message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\\nLangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\nWhat is inside a message?‚Äã\\nA message typically consists of the following pieces of information:\\n\\nRole: The role of the message (e.g., \"user\", \"assistant\").\\nContent: The content of the message (e.g., text, multimodal data).\\nAdditional metadata: id, name, token usage and other model-specific metadata.\\n\\nRole‚Äã\\nRoles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\\nRoleDescriptionsystemUsed to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.userRepresents input from a user interacting with the model, usually in the form of text or other interactive input.assistantRepresents a response from the model, which can include text or a request to invoke tools.toolA message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.function (legacy)This is a legacy role, corresponding to OpenAI\\'s legacy function-calling API. tool role should be used instead.\\nContent‚Äã\\nThe content of a message text or a list of dictionaries representing multimodal data (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\\nCurrently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\\nFor more information see:\\n\\nSystemMessage -- for content which should be passed to direct the conversation\\nHumanMessage -- for content in the input from the user.\\nAIMessage -- for content in the response from the model.\\nMultimodality -- for more information on multimodal content.\\n\\nOther Message Data‚Äã\\nDepending on the chat model provider, messages can include other data such as:\\n\\nID: An optional unique identifier for the message.\\nName: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this!\\nMetadata: Additional information about the message, such as timestamps, token usage, etc.\\nTool Calls: A request made by the model to call one or more tools> See tool calling for more information.\\n\\nConversation Structure‚Äã\\nThe sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\\nFor example, a typical conversation structure might look like this:\\n\\nUser Message: \"Hello, how are you?\"\\nAssistant Message: \"I\\'m doing well, thank you for asking.\"\\nUser Message: \"Can you tell me a joke?\"\\nAssistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\\n\\nPlease read the chat history guide for more information on managing chat history and ensuring that the conversation structure is correct.\\nLangChain Messages‚Äã\\nLangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\\nLangChain messages are Python objects that subclass from a BaseMessage.\\nThe five main message types are:\\n\\nSystemMessage: corresponds to system role\\nHumanMessage: corresponds to user role\\nAIMessage: corresponds to assistant role\\nAIMessageChunk: corresponds to assistant role, used for streaming responses\\nToolMessage: corresponds to tool role\\n\\nOther important messages include:\\n\\nRemoveMessage -- does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history.\\nLegacy FunctionMessage: corresponds to the function role in OpenAI\\'s legacy function-calling API.\\n\\nYou can find more information about messages in the API Reference.\\nSystemMessage‚Äã\\nA SystemMessage is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\\nDifferent chat providers may support system message in one of the following ways:\\n\\nThrough a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\\nThrough a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\\nNo support for system messages: Some models do not support system messages at all.\\n\\nMost major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider‚Äôs capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\\nIf no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message\\'s content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the langchain-community package) it is recommended to check the specific documentation for that model.\\nHumanMessage‚Äã\\nThe HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model.\\nText Content‚Äã\\nMost chat models expect the user input to be in the form of text.\\nfrom langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hello, how are you?\")])API Reference:HumanMessage\\ntipWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing.model.invoke(\"Hello, how are you?\")\\nMulti-modal Content‚Äã\\nSome chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\\nPlease see the multimodality guide for more information.\\nAIMessage‚Äã\\nAIMessage is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.\\nfrom langchain_core.messages import HumanMessageai_message = model.invoke([HumanMessage(\"Tell me a joke\")])ai_message # <-- AIMessageAPI Reference:HumanMessage\\nAn AIMessage has the following attributes. The attributes which are standardized are the ones that LangChain attempts to standardize across different chat model providers. raw fields are specific to the model provider and may vary.\\nAttributeStandardized/RawDescriptioncontentRawUsually a string, but can be a list of content blocks. See content for details.tool_callsStandardizedTool calls associated with the message. See tool calling for details.invalid_tool_callsStandardizedTool calls with parsing errors associated with the message. See tool calling for details.usage_metadataStandardizedUsage metadata for a message, such as token counts. See Usage Metadata API Reference.idStandardizedAn optional unique identifier for the message, ideally provided by the provider/model that created the message.response_metadataRawResponse metadata, e.g., response headers, logprobs, token counts.\\ncontent‚Äã\\nThe content property of an AIMessage represents the response generated by the chat model.\\nThe content is either:\\n\\ntext -- the norm for virtually all chat models.\\nA list of dictionaries -- Each dictionary represents a content block and is associated with a type.\\n\\nUsed by Anthropic for surfacing agent thought process when doing tool calling.\\nUsed by OpenAI for audio outputs. Please see multi-modal content for more information.\\n\\n\\n\\nimportantThe content property is not standardized across different chat model providers, mostly because there are\\nstill few examples to generalize from.\\nAIMessageChunk‚Äã\\nIt is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\\nIt is returned from the stream, astream and astream_events methods of the chat model.\\nFor example,\\nfor chunk in model.stream([HumanMessage(\"what color is the sky?\")]):    print(chunk)\\nAIMessageChunk follows nearly the same structure as AIMessage, but uses a different ToolCallChunk\\nto be able to stream tool calling in a standardized manner.\\nAggregating‚Äã\\nAIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user.\\nai_message = chunk1 + chunk2 + chunk3 + ...\\nToolMessage‚Äã\\nThis represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has:\\n\\na tool_call_id field which conveys the id of the call to the tool that was called to produce this result.\\nan artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\\n\\nPlease see tool calling for more information.\\nRemoveMessage‚Äã\\nThis is a special message type that does not correspond to any roles. It is used\\nfor managing chat history in LangGraph.\\nPlease see the following for more information on how to use the RemoveMessage:\\n\\nMemory conceptual guide\\nHow to delete messages\\n\\n(Legacy) FunctionMessage‚Äã\\nThis is a legacy message type, corresponding to OpenAI\\'s legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.\\nOpenAI Format‚Äã\\nInputs‚Äã\\nChat models also accept OpenAI\\'s format as inputs to chat models:\\nchat_model.invoke([    {        \"role\": \"user\",        \"content\": \"Hello, how are you?\",    },    {        \"role\": \"assistant\",        \"content\": \"I\\'m doing well, thank you for asking.\",    },    {        \"role\": \"user\",        \"content\": \"Can you tell me a joke?\",    }])\\nOutputs‚Äã\\nAt the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you\\nneed OpenAI format for the output as well.\\nThe convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.Edit this pageWas this page helpful?PreviousLangChain Expression Language (LCEL)NextMultimodalityOverviewWhat is inside a message?RoleContentOther Message DataConversation StructureLangChain MessagesSystemMessageHumanMessageAIMessageAIMessageChunkToolMessageRemoveMessage(Legacy) FunctionMessageOpenAI FormatInputsOutputsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/lcel/', 'title': 'LangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Runnable Interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangChain Expression Language (LCEL) | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\\nPrerequisites\\nRunnable Interface\\n\\nThe LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\\nThis means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\\nWe often refer to a Runnable created using LCEL as a \"chain\". It\\'s important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\\nnote\\nThe LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\\nPlease see the following list of how-to guides that cover common tasks with LCEL.\\nA list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\\n\\nBenefits of LCEL‚Äã\\nLangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\\n\\nOptimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\\nGuaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\\nSimplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\\n\\nOther benefits include:\\n\\nSeamless LangSmith tracing\\nAs your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\\nWith LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\\nStandard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\\nDeployable with LangServe: Chains built with LCEL can be deployed using for production use.\\n\\nShould I use LCEL?‚Äã\\nLCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\\nWhile we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\\nIn LangGraph, users define graphs that specify the application\\'s flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\\nHere are some guidelines:\\n\\nIf you are making a single LLM call, you don\\'t need LCEL; instead call the underlying chat model directly.\\nIf you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you\\'re taking advantage of the LCEL benefits.\\nIf you\\'re building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\\n\\nComposition Primitives‚Äã\\nLCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\\nMany other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\\nnoteYou can find a list of all composition primitives in the LangChain Core API Reference.\\nRunnableSequence‚Äã\\nRunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\\nfrom langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\ncorresponds to the following:\\noutput1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\\nnoterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\\nRunnableParallel‚Äã\\nRunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\\nfrom langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\\nInvoking the chain with some input:\\nfinal_output = chain.invoke(some_input)\\nWill yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\\n{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\\nRecall, that the runnables are executed in parallel, so while the result is the same as\\ndictionary comprehension shown above, the execution time is much faster.\\nnoteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\\nFor synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\\nFor asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\\n\\nComposition Syntax‚Äã\\nThe usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\\nto make the code more readable and concise.\\nThe | operator‚Äã\\nWe have overloaded the | operator to create a RunnableSequence from two Runnables.\\nchain = runnable1 | runnable2\\nis Equivalent to:\\nchain = RunnableSequence([runnable1, runnable2])\\nThe .pipe method‚Äã\\nIf you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\\nchain = runnable1.pipe(runnable2)\\nCoercion‚Äã\\nLCEL applies automatic type coercion to make it easier to compose chains.\\nIf you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\\nThis will make the code more verbose, but it will also make it more explicit.\\nDictionary to RunnableParallel‚Äã\\nInside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\\nFor example, the following code:\\nmapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableParallel(mapping), runnable3])\\ncautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\\nFunction to RunnableLambda‚Äã\\nInside an LCEL expression, a function is automatically converted to a RunnableLambda.\\ndef some_func(x):    return xchain = some_func | runnable1\\nIt gets automatically converted to the following:\\nchain = RunnableSequence([RunnableLambda(some_func), runnable1])\\ncautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\\nLegacy chains‚Äã\\nLCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\\nConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\\nof viable models emerge, customization has become more and more important.\\nIf you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\\nFor guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/evaluation/', 'title': 'Evaluation | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEvaluation | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEvaluationEvaluation\\n\\nEvaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\\nIt involves testing the model\\'s responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\\nThis process is vital for building reliable applications.\\n\\nLangSmith helps with this process in a few ways:\\n\\nIt makes it easier to create and curate datasets via its tracing and annotation features\\nIt provides an evaluation framework that helps you define metrics and run your app against your dataset\\nIt allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\\n\\nTo learn more, check out this LangSmith guide.Edit this pageWas this page helpful?PreviousEmbedding modelsNextExample selectorsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/embedding_models/', 'title': 'Embedding models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '* Documents', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nEmbedding models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEmbedding modelsOn this pageEmbedding models\\n\\nPrerequisites\\nDocuments\\n\\nNoteThis conceptual overview focuses on text-based embedding models.Embedding models can also be multimodal though such models are not currently supported by LangChain.\\nImagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.\\nThis is the power of embedding models, which lie at the heart of many retrieval systems.\\nEmbedding models transform human language into a format that machines can understand and compare with speed and accuracy.\\nThese models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text\\'s semantic meaning.\\nEmbeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\\nKey concepts‚Äã\\n\\n(1) Embed text as a vector: Embeddings transform text into a numerical vector representation.\\n(2) Measure similarity: Embedding vectors can be compared using simple mathematical operations.\\nEmbedding‚Äã\\nHistorical context‚Äã\\nThe landscape of embedding models has evolved significantly over the years.\\nA pivotal moment came in 2018 when Google introduced BERT (Bidirectional Encoder Representations from Transformers).\\nBERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.\\nHowever, BERT wasn\\'t optimized for generating sentence embeddings efficiently.\\nThis limitation spurred the creation of SBERT (Sentence-BERT), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.\\nToday, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.\\nTo navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) here for objective comparisons.\\nFurther reading\\nSee the seminal BERT paper.\\nSee Cameron Wolfe\\'s excellent review of embedding models.\\nSee the Massive Text Embedding Benchmark (MTEB) leaderboard for a comprehensive overview of embedding models.\\n\\nInterface‚Äã\\nLangChain provides a universal interface for working with them, providing standard methods for common operations.\\nThis common interface simplifies interaction with various embedding providers through two central methods:\\n\\nembed_documents: For embedding multiple texts (documents)\\nembed_query: For embedding a single text (query)\\n\\nThis distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\\nTo illustrate, here\\'s a practical example using LangChain\\'s .embed_documents method to embed a list of strings:\\nfrom langchain_openai import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings()embeddings = embeddings_model.embed_documents(    [        \"Hi there!\",        \"Oh, hello!\",        \"What\\'s your name?\",        \"My friends call me World\",        \"Hello World!\"    ])len(embeddings), len(embeddings[0])(5, 1536)API Reference:OpenAIEmbeddings\\nFor convenience, you can also use the embed_query method to embed a single text:\\nquery_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")\\nFurther reading\\nSee the full list of LangChain embedding model integrations.\\nSee these how-to guides for working with embedding models.\\n\\nIntegrations‚Äã\\nLangChain offers many embedding model integrations which you can find on the embedding models integrations page.\\nMeasure similarity‚Äã\\nEach embedding is essentially a set of coordinates, often in a high-dimensional space.\\nIn this space, the position of each point (embedding) reflects the meaning of its corresponding text.\\nJust as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\\nThis allows for intuitive comparisons between different pieces of text.\\nBy reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\\nSome common similarity metrics include:\\n\\nCosine Similarity: Measures the cosine of the angle between two vectors.\\nEuclidean Distance: Measures the straight-line distance between two points.\\nDot Product: Measures the projection of one vector onto another.\\n\\nThe choice of similarity metric should be chosen based on the model.\\nAs an example, OpenAI suggests cosine similarity for their embeddings, which can be easily implemented:\\nimport numpy as npdef cosine_similarity(vec1, vec2):    dot_product = np.dot(vec1, vec2)    norm_vec1 = np.linalg.norm(vec1)    norm_vec2 = np.linalg.norm(vec2)    return dot_product / (norm_vec1 * norm_vec2)similarity = cosine_similarity(query_result, document_result)print(\"Cosine Similarity:\", similarity)\\nFurther reading\\nSee Simon Willison‚Äôs nice blog post and video on embeddings and similarity metrics.\\nSee this documentation from Google on similarity metrics to consider with embeddings.\\nSee Pinecone\\'s blog post on similarity metrics.\\nSee OpenAI\\'s FAQ on what similarity metric to use with OpenAI embeddings.\\nEdit this pageWas this page helpful?PreviousDocument loadersNextEvaluationKey conceptsEmbeddingHistorical contextInterfaceIntegrationsMeasure similarityCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/chat_models/', 'title': 'Chat models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': 'Overview', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nChat models | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat modelsOn this pageChat models\\nOverview‚Äã\\nLarge Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\\nModern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output.\\nThe newest generation of chat models offer additional capabilities:\\n\\nTool calling: Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\\nStructured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\\nMultimodality: The ability to work with data other than text; for example, images, audio, and video.\\n\\nFeatures‚Äã\\nLangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\\n\\nIntegrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see chat model integrations for an up-to-date list of supported models.\\nUse either LangChain\\'s messages format or OpenAI format.\\nStandard tool calling API: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\\nStandard API for structuring outputs via the with_structured_output method.\\nProvides support for async programming, efficient batching, a rich streaming API.\\nIntegration with LangSmith for monitoring and debugging production-grade applications based on LLMs.\\nAdditional features like standardized token usage, rate limiting, caching and more.\\n\\nIntegrations‚Äã\\nLangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\\nThese integrations are one of two types:\\n\\nOfficial models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages.\\nCommunity models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package.\\n\\nLangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\\nPlease review the chat model integrations for a list of supported models.\\nnoteModels that do not include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\\nInterface‚Äã\\nLangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface, chat models support a standard streaming interface, async programming, optimized batching, and more. Please see the Runnable Interface for more details.\\nMany of the key methods of chat models operate on messages as input and return messages as output.\\nChat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\\nnoteIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., Ollama, Anthropic, OpenAI, etc.).\\nThese models implement the BaseLLM interface and may be named with the \"LLM\" suffix (e.g., OllamaLLM, AnthropicLLM, OpenAILLM, etc.). Generally, users should not use these models.\\nKey methods‚Äã\\nThe key methods of a chat model are:\\n\\ninvoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\\nstream: A method that allows you to stream the output of a chat model as it is generated.\\nbatch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\\nbind_tools: A method that allows you to bind a tool to a chat model for use in the model\\'s execution context.\\nwith_structured_output: A wrapper around the invoke method for models that natively support structured output.\\n\\nOther important methods can be found in the BaseChatModel API Reference.\\nInputs and outputs‚Äã\\nModern LLMs are typically accessed through a chat model interface that takes messages as input and returns messages as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\\nLangChain supports two message formats to interact with chat models:\\n\\nLangChain Message Format: LangChain\\'s own message format, which is used by default and is used internally by LangChain.\\nOpenAI\\'s Message Format: OpenAI\\'s message format.\\n\\nStandard parameters‚Äã\\nMany chat models have standardized parameters that can be used to configure the model:\\nParameterDescriptionmodelThe name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\").temperatureControls the randomness of the model\\'s output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.timeoutThe maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn‚Äôt hang indefinitely.max_tokensLimits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.stopSpecifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.max_retriesThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.api_keyThe API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.base_urlThe URL of the API endpoint where requests are sent. This is typically provided by the model\\'s provider and is necessary for directing your requests.rate_limiterAn optional BaseRateLimiter to space out requests to avoid exceeding rate limits.  See rate-limiting below for more details.\\nSome important things to note:\\n\\nStandard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can\\'t be supported on these.\\nStandard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they\\'re not enforced on models in langchain-community.\\n\\nChat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective API reference for that model.\\nTool calling‚Äã\\nChat models can call tools to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\\nsee the tool calling guide for more information.\\nStructured outputs‚Äã\\nChat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\\nuseful for information extraction tasks. Please read more about\\nthe technique in the structured outputs guide.\\nMultimodality‚Äã\\nLarge Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as multimodality.\\nCurrently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\\nContext window‚Äã\\nA chat model\\'s context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\\nIf the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the memory.\\nThe size of the input is measured in tokens which are the unit of processing that the model uses.\\nAdvanced topics‚Äã\\nRate-limiting‚Äã\\nMany chat model providers impose a limit on the number of requests that can be made in a given time period.\\nIf you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\\nYou have a few options to deal with rate limits:\\n\\nTry to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the how to handle rate limits for more information on how to use this feature.\\nTry to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a max_retries parameter that can be used to control the number of retries. See the standard parameters section for more information.\\nFallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\\n\\nCaching‚Äã\\nChat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\\nThe reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the exact inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\\nAn alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\\nA semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an embedding model to convert text to a vector representation), and it\\'s not guaranteed to capture the meaning of the input accurately.\\nHowever, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\\nPlease see the how to cache chat model responses guide for more details.\\nRelated resources‚Äã\\n\\nHow-to guides on using chat models: how-to guides.\\nList of supported chat models: chat model integrations.\\n\\nConceptual guides‚Äã\\n\\nMessages\\nTool calling\\nMultimodality\\nStructured outputs\\nTokens\\nEdit this pageWas this page helpful?PreviousChat historyNextDocument loadersOverviewFeaturesIntegrationsInterfaceKey methodsInputs and outputsStandard parametersTool callingStructured outputsMultimodalityContext windowAdvanced topicsRate-limitingCachingRelated resourcesConceptual guidesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/chat_history/', 'title': 'Chat history | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Messages', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nChat history | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat historyOn this pageChat history\\nPrerequisites\\nMessages\\nChat models\\nTool calling\\n\\nChat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of messages, each of which is associated with a specific role, such as \"user\", \"assistant\", \"system\", or \"tool\".\\nConversation patterns‚Äã\\n\\nMost conversations start with a system message that sets the context for the conversation. This is followed by a user message containing the user\\'s input, and then an assistant message containing the model\\'s response.\\nThe assistant may respond directly to the user or if configured with tools request that a tool be invoked to perform a specific task.\\nA full conversation often involves a combination of two patterns of alternating messages:\\n\\nThe user and the assistant representing a back-and-forth conversation.\\nThe assistant and tool messages representing an \"agentic\" workflow where the assistant is invoking tools to perform specific tasks.\\n\\nManaging chat history‚Äã\\nSince chat models have a maximum limit on input size, it\\'s important to manage chat history and trim it as needed to avoid exceeding the context window.\\nWhile processing chat history, it\\'s essential to preserve a correct conversation structure.\\nKey guidelines for managing chat history:\\n\\nThe conversation should follow one of these structures:\\n\\nThe first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\\nThe last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\\n\\n\\nWhen using tool calling, a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\\n\\ntipUnderstanding correct conversation structure is essential for being able to properly implement\\nmemory in chat models.\\nRelated resources‚Äã\\n\\nHow to trim messages\\nMemory guide for information on implementing short-term and long-term memory in chat models using LangGraph.\\nEdit this pageWas this page helpful?PreviousCallbacksNextChat modelsConversation patternsManaging chat historyRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/callbacks/', 'title': 'Callbacks | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': '- Runnable interface', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nCallbacks | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideCallbacksOn this pageCallbacks\\nPrerequisites\\nRunnable interface\\n\\nLangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\\nYou can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\\nCallback events‚Äã\\nEventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retry\\nCallback handlers‚Äã\\nCallback handlers can either be sync or async:\\n\\nSync callback handlers implement the BaseCallbackHandler interface.\\nAsync callback handlers implement the AsyncCallbackHandler interface.\\n\\nDuring run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\\nPassing callbacks‚Äã\\nThe callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\\n\\nRequest time callbacks: Passed at the time of the request in addition to the input data.\\nAvailable on all standard Runnable objects. These callbacks are INHERITED by all children\\nof the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).\\nConstructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\\nare passed as arguments to the constructor of the object. The callbacks are scoped\\nonly to the object they are defined on, and are not inherited by any children of the object.\\n\\nwarningConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\\nof the object.\\nIf you\\'re creating a custom chain or runnable, you need to remember to propagate request time\\ncallbacks to any child objects.\\nAsync in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\\nand is running async in python<=3.10, will have to propagate callbacks to child\\nobjects manually. This is because LangChain cannot automatically propagate\\ncallbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\\nrunnables or tools.\\nFor specifics on how to use callbacks, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousAsync programming with langchainNextChat historyCallback eventsCallback handlersPassing callbacksCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n'),\n",
              " Document(metadata={'source': 'https://python.langchain.com/docs/concepts/agents/', 'title': 'Agents | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain', 'description': \"By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\", 'language': 'en'}, page_content='\\n\\n\\n\\n\\nAgents | \\uf8ffü¶úÔ∏è\\uf8ffüîó LangChain\\n\\n\\n\\n\\n\\n\\nSkip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1\\uf8ffüí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain\\'s stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystem\\uf8ffü¶ú\\uf8ffüõ†Ô∏è LangSmith\\uf8ffü¶ú\\uf8ffüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideAgentsOn this pageAgents\\nBy themselves, language models can\\'t take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\\nLangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.\\nPlease see the following resources for more information:\\n\\nLangGraph docs on common agent architectures\\nPre-built agents in LangGraph\\n\\nLegacy agent concept: AgentExecutor‚Äã\\nLangChain previously introduced the AgentExecutor as a runtime for agents.\\nWhile it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.\\nAs a result, we\\'re gradually phasing out AgentExecutor in favor of more flexible solutions in LangGraph.\\nTransitioning from AgentExecutor to langgraph‚Äã\\nIf you\\'re currently using AgentExecutor, don\\'t worry! We\\'ve prepared resources to help you:\\n\\n\\nFor those who still need to use AgentExecutor, we offer a comprehensive guide on how to use AgentExecutor.\\n\\n\\nHowever, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we\\'ve created a detailed migration guide to help you move from AgentExecutor to LangGraph seamlessly.\\n\\nEdit this pageWas this page helpful?PreviousConceptual guideNextArchitectureLegacy agent concept: AgentExecutorTransitioning from AgentExecutor to langgraphCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\\n\\n')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d_reversed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJ8VZ_0JvMcW",
        "outputId": "0c67383a-ca4c-479f-954b-93d3f2669369"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Vector stores | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideVector storesOn this pageVector stores\n",
            "\n",
            "Prerequisites\n",
            "Embeddings\n",
            "Text splitters\n",
            "\n",
            "NoteThis conceptual overview focuses on text-based indexing and retrieval for simplicity.\n",
            "However, embedding models can be multi-modal\n",
            "and vector stores can be used to store and retrieve a variety of data types beyond text.\n",
            "Overview‚Äã\n",
            "Vector stores are specialized data stores that enable indexing and retrieving information based on vector representations.\n",
            "These vectors, called embeddings, capture the semantic meaning of data that has been embedded.\n",
            "Vector stores are frequently used to search over unstructured data, such as text, images, and audio, to retrieve relevant information based on semantic similarity rather than exact keyword matches.\n",
            "\n",
            "Integrations‚Äã\n",
            "LangChain has a large number of vectorstore integrations, allowing users to easily switch between different vectorstore implementations.\n",
            "Please see the full list of LangChain vectorstore integrations.\n",
            "Interface‚Äã\n",
            "LangChain provides a standard interface for working with vector stores, allowing users to easily switch between different vectorstore implementations.\n",
            "The interface consists of basic methods for writing, deleting and searching for documents in the vector store.\n",
            "The key methods are:\n",
            "\n",
            "add_documents: Add a list of texts to the vector store.\n",
            "delete: Delete a list of documents from the vector store.\n",
            "similarity_search: Search for similar documents to a given query.\n",
            "\n",
            "Initialization‚Äã\n",
            "Most vectors in LangChain accept an embedding model as an argument when initializing the vector store.\n",
            "We will use LangChain's InMemoryVectorStore implementation to illustrate the API.\n",
            "from langchain_core.vectorstores import InMemoryVectorStore# Initialize with an embedding modelvector_store = InMemoryVectorStore(embedding=SomeEmbeddingModel())API Reference:InMemoryVectorStore\n",
            "Adding documents‚Äã\n",
            "To add documents, use the add_documents method.\n",
            "This API works with a list of Document objects.\n",
            "Document objects all have page_content and metadata attributes, making them a universal way to store unstructured text and associated metadata.\n",
            "from langchain_core.documents import Documentdocument_1 = Document(    page_content=\"I had chocalate chip pancakes and scrambled eggs for breakfast this morning.\",    metadata={\"source\": \"tweet\"},)document_2 = Document(    page_content=\"The weather forecast for tomorrow is cloudy and overcast, with a high of 62 degrees.\",    metadata={\"source\": \"news\"},)documents = [document_1, document_2]vector_store.add_documents(documents=documents)API Reference:Document\n",
            "You should usually provide IDs for the documents you add to the vector store, so\n",
            "that instead of adding the same document multiple times, you can update the existing document.\n",
            "vector_store.add_documents(documents=documents, ids=[\"doc1\", \"doc2\"])\n",
            "Delete‚Äã\n",
            "To delete documents, use the delete method which takes a list of document IDs to delete.\n",
            "vector_store.delete(ids=[\"doc1\"])\n",
            "Search‚Äã\n",
            "Vector stores embed and store the documents that added.\n",
            "If we pass in a query, the vectorstore will embed the query, perform a similarity search over the embedded documents, and return the most similar ones.\n",
            "This captures two important concepts: first, there needs to be a way to measure the similarity between the query and any embedded document.\n",
            "Second, there needs to be an algorithm to efficiently perform this similarity search across all embedded documents.\n",
            "Similarity metrics‚Äã\n",
            "A critical advantage of embeddings vectors is they can be compared using many simple mathematical operations:\n",
            "\n",
            "Cosine Similarity: Measures the cosine of the angle between two vectors.\n",
            "Euclidean Distance: Measures the straight-line distance between two points.\n",
            "Dot Product: Measures the projection of one vector onto another.\n",
            "\n",
            "The choice of similarity metric can sometimes be selected when initializing the vectorstore. Please refer\n",
            "to the documentation of the specific vectorstore you are using to see what similarity metrics are supported.\n",
            "Further reading\n",
            "See this documentation from Google on similarity metrics to consider with embeddings.\n",
            "See Pinecone's blog post on similarity metrics.\n",
            "See OpenAI's FAQ on what similarity metric to use with OpenAI embeddings.\n",
            "\n",
            "Similarity search‚Äã\n",
            "Given a similarity metric to measure the distance between the embedded query and any embedded document, we need an algorithm to efficiently search over all the embedded documents to find the most similar ones.\n",
            "There are various ways to do this. As an example, many vectorstores implement HNSW (Hierarchical Navigable Small World), a graph-based index structure that allows for efficient similarity search.\n",
            "Regardless of the search algorithm used under the hood, the LangChain vectorstore interface has a similarity_search method for all integrations.\n",
            "This will take the search query, create an embedding, find similar documents, and return them as a list of Documents.\n",
            "query = \"my query\"docs = vectorstore.similarity_search(query)\n",
            "Many vectorstores support search parameters to be passed with the similarity_search method. See the documentation for the specific vectorstore you are using to see what parameters are supported.\n",
            "As an example Pinecone several parameters that are important general concepts:\n",
            "Many vectorstores support the k, which controls the number of Documents to return, and filter, which allows for filtering documents by metadata.\n",
            "\n",
            "query (str) ‚Äì Text to look up documents similar to.\n",
            "k (int) ‚Äì Number of Documents to return. Defaults to 4.\n",
            "filter (dict | None) ‚Äì Dictionary of argument(s) to filter on metadata\n",
            "\n",
            "Further reading\n",
            "See the how-to guide for more details on how to use the similarity_search method.\n",
            "See the integrations page for more details on arguments that can be passed in to the similarity_search method for specific vectorstores.\n",
            "\n",
            "Metadata filtering‚Äã\n",
            "While vectorstore implement a search algorithm to efficiently search over all the embedded documents to find the most similar ones, many also support filtering on metadata.\n",
            "Metadata filtering helps narrow down the search by applying specific conditions such as retrieving documents from a particular source or date range. These two concepts work well together:\n",
            "\n",
            "Semantic search: Query the unstructured data directly, often via embedding or keyword similarity.\n",
            "Metadata search: Apply structured query to the metadata, filtering specific documents.\n",
            "\n",
            "Vector store support for metadata filtering is typically dependent on the underlying vector store implementation.\n",
            "Here is example usage with Pinecone, showing that we filter for all documents that have the metadata key source with value tweet.\n",
            "vectorstore.similarity_search(    \"LangChain provides abstractions to make working with LLMs easy\",    k=2,    filter={\"source\": \"tweet\"},)\n",
            "Further reading\n",
            "See Pinecone's documentation on filtering with metadata.\n",
            "See the list of LangChain vectorstore integrations that support metadata filtering.\n",
            "\n",
            "Advanced search and retrieval techniques‚Äã\n",
            "While algorithms like HNSW provide the foundation for efficient similarity search in many cases, additional techniques can be employed to improve search quality and diversity.\n",
            "For example, maximal marginal relevance is a re-ranking algorithm used to diversify search results, which is applied after the initial similarity search to ensure a more diverse set of results.\n",
            "As a second example, some vector stores offer built-in hybrid-search to combine keyword and semantic similarity search, which marries the benefits of both approaches.\n",
            "At the moment, there is no unified way to perform hybrid search using LangChain vectorstores, but it is generally exposed as a keyword argument that is passed in with similarity_search.\n",
            "See this how-to guide on hybrid search for more details.\n",
            "NameWhen to useDescriptionHybrid searchWhen combining keyword-based and semantic similarity.Hybrid search combines keyword and semantic similarity, marrying the benefits of both approaches. Paper.Maximal Marginal Relevance (MMR)When needing to diversify search results.MMR attempts to diversify the results of a search to avoid returning similar and redundant documents.Edit this pageWas this page helpful?PreviousTracingNextWhy LangChain?OverviewIntegrationsInterfaceInitializationAdding documentsDeleteSearchSimilarity metricsSimilarity searchMetadata filteringAdvanced search and retrieval techniquesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tracing | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTracingTracing\n",
            "\n",
            "A trace is essentially a series of steps that your application takes to go from input to output.\n",
            "Traces contain individual steps called runs. These can be individual calls from a model, retriever,\n",
            "tool, or sub-chains.\n",
            "Tracing gives you observability inside your chains and agents, and is vital in diagnosing issues.\n",
            "For a deeper dive, check out this LangSmith conceptual guide.Edit this pageWas this page helpful?PreviousToolsNextVector storesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tools | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideToolsOn this pageTools\n",
            "Prerequisites\n",
            "Chat models\n",
            "\n",
            "Overview‚Äã\n",
            "The tool abstraction in LangChain associates a Python function with a schema that defines the function's name, description and expected arguments.\n",
            "Tools can be passed to chat models that support tool calling allowing the model to request the execution of a specific function with specific inputs.\n",
            "Key concepts‚Äã\n",
            "\n",
            "Tools are a way to encapsulate a function and its schema in a way that can be passed to a chat model.\n",
            "Create tools using the @tool decorator, which simplifies the process of tool creation, supporting the following:\n",
            "\n",
            "Automatically infer the tool's name, description and expected arguments, while also supporting customization.\n",
            "Defining tools that return artifacts (e.g. images, dataframes, etc.)\n",
            "Hiding input arguments from the schema (and hence from the model) using injected tool arguments.\n",
            "\n",
            "\n",
            "\n",
            "Tool interface‚Äã\n",
            "The tool interface is defined in the BaseTool class which is a subclass of the Runnable Interface.\n",
            "The key attributes that correspond to the tool's schema:\n",
            "\n",
            "name: The name of the tool.\n",
            "description: A description of what the tool does.\n",
            "args: Property that returns the JSON schema for the tool's arguments.\n",
            "\n",
            "The key methods to execute the function associated with the tool:\n",
            "\n",
            "invoke: Invokes the tool with the given arguments.\n",
            "ainvoke: Invokes the tool with the given arguments, asynchronously. Used for async programming with Langchain.\n",
            "\n",
            "Create tools using the @tool decorator‚Äã\n",
            "The recommended way to create tools is using the @tool decorator. This decorator is designed to simplify the process of tool creation and should be used in most cases. After defining a function, you can decorate it with @tool to create a tool that implements the Tool Interface.\n",
            "from langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:   \"\"\"Multiply two numbers.\"\"\"   return a * bAPI Reference:tool\n",
            "For more details on how to create tools, see the how to create custom tools guide.\n",
            "noteLangChain has a few other ways to create tools; e.g., by sub-classing the BaseTool class or by using StructuredTool. These methods are shown in the how to create custom tools guide, but\n",
            "we generally recommend using the @tool decorator for most cases.\n",
            "Use the tool directly‚Äã\n",
            "Once you have defined a tool, you can use it directly by calling the function. For example, to use the multiply tool defined above:\n",
            "multiply.invoke({\"a\": 2, \"b\": 3})\n",
            "Inspect‚Äã\n",
            "You can also inspect the tool's schema and other properties:\n",
            "print(multiply.name) # multiplyprint(multiply.description) # Multiply two numbers.print(multiply.args) # {# 'type': 'object', # 'properties': {'a': {'type': 'integer'}, 'b': {'type': 'integer'}}, # 'required': ['a', 'b']# }\n",
            "noteIf you're using pre-built LangChain or LangGraph components like create_react_agent,you might not need to interact with tools directly. However, understanding how to use them can be valuable for debugging and testing. Additionally, when building custom LangGraph workflows, you may find it necessary to work with tools directly.\n",
            "Configuring the schema‚Äã\n",
            "The @tool decorator offers additional options to configure the schema of the tool (e.g., modify name, description\n",
            "or parse the function's doc-string to infer the schema).\n",
            "Please see the API reference for @tool for more details and review the how to create custom tools guide for examples.\n",
            "Tool artifacts‚Äã\n",
            "Tools are utilities that can be called by a model, and whose outputs are designed to be fed back to a model. Sometimes, however, there are artifacts of a tool's execution that we want to make accessible to downstream components in our chain or agent, but that we don't want to expose to the model itself. For example if a tool returns a custom object, a dataframe or an image, we may want to pass some metadata about this output to the model without passing the actual output to the model. At the same time, we may want to be able to access this full output elsewhere, for example in downstream tools.\n",
            "@tool(response_format=\"content_and_artifact\")def some_tool(...) -> Tuple[str, Any]:    \"\"\"Tool that does something.\"\"\"    ...    return 'Message for chat model', some_artifact \n",
            "See how to return artifacts from tools for more details.\n",
            "Special type annotations‚Äã\n",
            "There are a number of special type annotations that can be used in the tool's function signature to configure the run time behavior of the tool.\n",
            "The following type annotations will end up removing the argument from the tool's schema. This can be useful for arguments that should not be exposed to the model and that the model should not be able to control.\n",
            "\n",
            "InjectedToolArg: Value should be injected manually at runtime using .invoke or .ainvoke.\n",
            "RunnableConfig: Pass in the RunnableConfig object to the tool.\n",
            "InjectedState: Pass in the overall state of the LangGraph graph to the tool.\n",
            "InjectedStore: Pass in the LangGraph store object to the tool.\n",
            "\n",
            "You can also use the Annotated type with a string literal to provide a description for the corresponding argument that WILL be exposed in the tool's schema.\n",
            "\n",
            "Annotated[..., \"string literal\"] -- Adds a description to the argument that will be exposed in the tool's schema.\n",
            "\n",
            "InjectedToolArg‚Äã\n",
            "There are cases where certain arguments need to be passed to a tool at runtime but should not be generated by the model itself. For this, we use the InjectedToolArg annotation, which allows certain parameters to be hidden from the tool's schema.\n",
            "For example, if a tool requires a user_id to be injected dynamically at runtime, it can be structured in this way:\n",
            "from langchain_core.tools import tool, InjectedToolArg@tooldef user_specific_tool(input_data: str, user_id: InjectedToolArg) -> str:    \"\"\"Tool that processes input data.\"\"\"    return f\"User {user_id} processed {input_data}\"API Reference:tool | InjectedToolArg\n",
            "Annotating the user_id argument with InjectedToolArg tells LangChain that this argument should not be exposed as part of the\n",
            "tool's schema.\n",
            "See how to pass run time values to tools for more details on how to use InjectedToolArg.\n",
            "RunnableConfig‚Äã\n",
            "You can use the RunnableConfig object to pass custom run time values to tools.\n",
            "If you need to access the RunnableConfig object from within a tool. This can be done by using the RunnableConfig annotation in the tool's function signature.\n",
            "from langchain_core.runnables import RunnableConfig@toolasync def some_func(..., config: RunnableConfig) -> ...:    \"\"\"Tool that does something.\"\"\"    # do something with config    ...await some_func.ainvoke(..., config={\"configurable\": {\"value\": \"some_value\"}})API Reference:RunnableConfig\n",
            "The config will not be part of the tool's schema and will be injected at runtime with appropriate values.\n",
            "noteYou may need to access the config object to manually propagate it to subclass. This happens if you're working with python 3.9 / 3.10 in an async environment and need to manually propagate the config object to sub-calls.Please read Propagation RunnableConfig for more details to learn how to propagate the RunnableConfig down the call chain manually (or upgrade to Python 3.11 where this is no longer an issue).\n",
            "InjectedState‚Äã\n",
            "Please see the InjectedState documentation for more details.\n",
            "InjectedStore‚Äã\n",
            "Please see the InjectedStore documentation for more details.\n",
            "Best practices‚Äã\n",
            "When designing tools to be used by models, keep the following in mind:\n",
            "\n",
            "Tools that are well-named, correctly-documented and properly type-hinted are easier for models to use.\n",
            "Design simple and narrowly scoped tools, as they are easier for models to use correctly.\n",
            "Use chat models that support tool-calling APIs to take advantage of tools.\n",
            "\n",
            "Toolkits‚Äã\n",
            "\n",
            "LangChain has a concept of toolkits. This a very thin abstraction that groups tools together that\n",
            "are designed to be used together for specific tasks.\n",
            "Interface‚Äã\n",
            "All Toolkits expose a get_tools method which returns a list of tools. You can therefore do:\n",
            "# Initialize a toolkittoolkit = ExampleTookit(...)# Get list of toolstools = toolkit.get_tools()\n",
            "Related resources‚Äã\n",
            "See the following resources for more information:\n",
            "\n",
            "API Reference for @tool\n",
            "How to create custom tools\n",
            "How to pass run time values to tools\n",
            "All LangChain tool how-to guides\n",
            "Additional how-to guides that show usage with LangGraph\n",
            "Tool integrations, see the tool integration docs.\n",
            "Edit this pageWas this page helpful?PreviousTool callingNextTracingOverviewKey conceptsTool interfaceCreate tools using the @tool decoratorUse the tool directlyInspectConfiguring the schemaTool artifactsSpecial type annotationsInjectedToolArgRunnableConfigInjectedStateInjectedStoreBest practicesToolkitsInterfaceRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Tool calling | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTool callingOn this pageTool calling\n",
            "Prerequisites\n",
            "Tools\n",
            "Chat Models\n",
            "\n",
            "Overview‚Äã\n",
            "Many AI applications interact directly with humans. In these cases, it is appropriate for models to respond in natural language.\n",
            "But what about cases where we want a model to also interact directly with systems, such as databases or an API?\n",
            "These systems often have a particular input schema; for example, APIs frequently have a required payload structure.\n",
            "This need motivates the concept of tool calling. You can use tool calling to request model responses that match a particular schema.\n",
            "infoYou will sometimes hear the term function calling. We use this term interchangeably with tool calling.\n",
            "\n",
            "Key concepts‚Äã\n",
            "(1) Tool Creation: Use the @tool decorator to create a tool. A tool is an association between a function and its schema.\n",
            "(2) Tool Binding: The tool needs to be connected to a model that supports tool calling. This gives the model awareness of the tool and the associated input schema required by the tool.\n",
            "(3) Tool Calling: When appropriate, the model can decide to call a tool and ensure its response conforms to the tool's input schema.\n",
            "(4) Tool Execution: The tool can be executed using the arguments provided by the model.\n",
            "\n",
            "Recommended usage‚Äã\n",
            "This pseudocode illustrates the recommended workflow for using tool calling.\n",
            "Created tools are passed to .bind_tools() method as a list.\n",
            "This model can be called, as usual. If a tool call is made, model's response will contain the tool call arguments.\n",
            "The tool call arguments can be passed directly to the tool.\n",
            "# Tool creationtools = [my_tool]# Tool bindingmodel_with_tools = model.bind_tools(tools)# Tool calling response = model_with_tools.invoke(user_input)\n",
            "Tool creation‚Äã\n",
            "The recommended way to create a tool is using the @tool decorator.\n",
            "from langchain_core.tools import tool@tooldef multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.\"\"\"    return a * bAPI Reference:tool\n",
            "Further reading\n",
            "See our conceptual guide on tools for more details.\n",
            "See our model integrations that support tool calling.\n",
            "See our how-to guide on tool calling.\n",
            "\n",
            "Tool binding‚Äã\n",
            "Many model providers support tool calling.\n",
            "tipSee our model integration page for a list of providers that support tool calling.\n",
            "The central concept to understand is that LangChain provides a standardized interface for connecting tools to models.\n",
            "The .bind_tools() method can be used to specify which tools are available for a model to call.\n",
            "model_with_tools = model.bind_tools(tools_list)\n",
            "As a specific example, let's take a function multiply and bind it as a tool to a model that supports tool calling.\n",
            "def multiply(a: int, b: int) -> int:    \"\"\"Multiply a and b.    Args:        a: first int        b: second int    \"\"\"    return a * bllm_with_tools = tool_calling_model.bind_tools([multiply])\n",
            "Tool calling‚Äã\n",
            "\n",
            "A key principle of tool calling is that the model decides when to use a tool based on the input's relevance. The model doesn't always need to call a tool.\n",
            "For example, given an unrelated input, the model would not call the tool:\n",
            "result = llm_with_tools.invoke(\"Hello world!\")\n",
            "The result would be an AIMessage containing the model's response in natural language (e.g., \"Hello!\").\n",
            "However, if we pass an input relevant to the tool, the model should choose to call it:\n",
            "result = llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n",
            "As before, the output result will be an AIMessage.\n",
            "But, if the tool was called, result will have a tool_calls attribute.\n",
            "This attribute includes everything needed to execute the tool, including the tool name and input arguments:\n",
            "result.tool_calls{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'xxx', 'type': 'tool_call'}\n",
            "For more details on usage, see our how-to guides!\n",
            "Tool execution‚Äã\n",
            "Tools implement the Runnable interface, which means that they can be invoked (e.g., tool.invoke(args)) directly.\n",
            "LangGraph offers pre-built components (e.g., ToolNode) that will often invoke the tool in behalf of the user.\n",
            "Further reading\n",
            "See our how-to guide on tool calling.\n",
            "See the LangGraph documentation on using ToolNode.\n",
            "\n",
            "Best practices‚Äã\n",
            "When designing tools to be used by a model, it is important to keep in mind that:\n",
            "\n",
            "Models that have explicit tool-calling APIs will be better at tool calling than non-fine-tuned models.\n",
            "Models will perform better if the tools have well-chosen names and descriptions.\n",
            "Simple, narrowly scoped tools are easier for models to use than complex tools.\n",
            "Asking the model to select from a large list of tools poses challenges for the model.\n",
            "Edit this pageWas this page helpful?PreviousTokensNextToolsOverviewKey conceptsRecommended usageTool creationTool bindingTool callingTool executionBest practicesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Text splitters | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideText splittersOn this pageText splitters\n",
            "\n",
            "Prerequisites\n",
            "Documents\n",
            "Tokenization(/docs/concepts/tokens)\n",
            "\n",
            "Overview‚Äã\n",
            "Document splitting is often a crucial preprocessing step for many applications.\n",
            "It involves breaking down large texts into smaller, manageable chunks.\n",
            "This process offers several benefits, such as ensuring consistent processing of varying document lengths, overcoming input size limitations of models, and improving the quality of text representations used in retrieval systems.\n",
            "There are several strategies for splitting documents, each with its own advantages.\n",
            "Key concepts‚Äã\n",
            "\n",
            "Text splitters split documents into smaller chunks for use in downstream applications.\n",
            "Why split documents?‚Äã\n",
            "There are several reasons to split documents:\n",
            "\n",
            "Handling non-uniform document lengths: Real-world document collections often contain texts of varying sizes. Splitting ensures consistent processing across all documents.\n",
            "Overcoming model limitations: Many embedding models and language models have maximum input size constraints. Splitting allows us to process documents that would otherwise exceed these limits.\n",
            "Improving representation quality: For longer documents, the quality of embeddings or other representations may degrade as they try to capture too much information. Splitting can lead to more focused and accurate representations of each section.\n",
            "Enhancing retrieval precision: In information retrieval systems, splitting can improve the granularity of search results, allowing for more precise matching of queries to relevant document sections.\n",
            "Optimizing computational resources: Working with smaller chunks of text can be more memory-efficient and allow for better parallelization of processing tasks.\n",
            "\n",
            "Now, the next question is how to split the documents into chunks! There are several strategies, each with its own advantages.\n",
            "Further reading\n",
            "See Greg Kamradt's chunkviz to visualize different splitting strategies discussed below.\n",
            "\n",
            "Approaches‚Äã\n",
            "Length-based‚Äã\n",
            "The most intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit.\n",
            "Key benefits of length-based splitting:\n",
            "\n",
            "Straightforward implementation\n",
            "Consistent chunk sizes\n",
            "Easily adaptable to different model requirements\n",
            "\n",
            "Types of length-based splitting:\n",
            "\n",
            "Token-based: Splits text based on the number of tokens, which is useful when working with language models.\n",
            "Character-based: Splits text based on the number of characters, which can be more consistent across different types of text.\n",
            "\n",
            "Example implementation using LangChain's CharacterTextSplitter with token-based splitting:\n",
            "from langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter.from_tiktoken_encoder(    encoding_name=\"cl100k_base\", chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:CharacterTextSplitter\n",
            "Further reading\n",
            "See the how-to guide for token-based splitting.\n",
            "See the how-to guide for character-based splitting.\n",
            "\n",
            "Text-structured based‚Äã\n",
            "Text is naturally organized into hierarchical units such as paragraphs, sentences, and words.\n",
            "We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity.\n",
            "LangChain's RecursiveCharacterTextSplitter implements this concept:\n",
            "\n",
            "The RecursiveCharacterTextSplitter attempts to keep larger units (e.g., paragraphs) intact.\n",
            "If a unit exceeds the chunk size, it moves to the next level (e.g., sentences).\n",
            "This process continues down to the word level if necessary.\n",
            "\n",
            "Here is example usage:\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplittertext_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=0)texts = text_splitter.split_text(document)API Reference:RecursiveCharacterTextSplitter\n",
            "Further reading\n",
            "See the how-to guide for recursive text splitting.\n",
            "\n",
            "Document-structured based‚Äã\n",
            "Some documents have an inherent structure, such as HTML, Markdown, or JSON files.\n",
            "In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text.\n",
            "Key benefits of structure-based splitting:\n",
            "\n",
            "Preserves the logical organization of the document\n",
            "Maintains context within each chunk\n",
            "Can be more effective for downstream tasks like retrieval or summarization\n",
            "\n",
            "Examples of structure-based splitting:\n",
            "\n",
            "Markdown: Split based on headers (e.g., #, ##, ###)\n",
            "HTML: Split using tags\n",
            "JSON: Split by object or array elements\n",
            "Code: Split by functions, classes, or logical blocks\n",
            "\n",
            "Further reading\n",
            "See the how-to guide for Markdown splitting.\n",
            "See the how-to guide for Recursive JSON splitting.\n",
            "See the how-to guide for Code splitting.\n",
            "See the how-to guide for HTML splitting.\n",
            "\n",
            "Semantic meaning based‚Äã\n",
            "Unlike the previous methods, semantic-based splitting actually considers the content of the text.\n",
            "While other approaches use document or text structure as proxies for semantic meaning, this method directly analyzes the text's semantics.\n",
            "There are several ways to implement this, but conceptually the approach is split text when there are significant changes in text meaning.\n",
            "As an example, we can use a sliding window approach to generate embeddings, and compare the embeddings to find significant differences:\n",
            "\n",
            "Start with the first few sentences and generate an embedding.\n",
            "Move to the next group of sentences and generate another embedding (e.g., using a sliding window approach).\n",
            "Compare the embeddings to find significant differences, which indicate potential \"break points\" between semantic sections.\n",
            "\n",
            "This technique helps create chunks that are more semantically coherent, potentially improving the quality of downstream tasks like retrieval or summarization.\n",
            "Further reading\n",
            "See the how-to guide for splitting text based on semantic meaning.\n",
            "See Greg Kamradt's notebook showcasing semantic splitting.\n",
            "Edit this pageWas this page helpful?PreviousString-in, string-out llmsNextTokensOverviewKey conceptsWhy split documents?ApproachesLength-basedText-structured basedDocument-structured basedSemantic meaning basedCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Testing | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideTestingOn this pageTesting\n",
            "\n",
            "Testing is a critical part of the development process that ensures your code works as expected and meets the desired quality standards.\n",
            "In the LangChain ecosystem, we have 2 main types of tests: unit tests and integration tests.\n",
            "For integrations that implement standard LangChain abstractions, we have a set of standard tests (both unit and integration) that help maintain compatibility between different components and ensure reliability of high-usage ones.\n",
            "Unit Tests‚Äã\n",
            "Definition: Unit tests are designed to validate the smallest parts of your code‚Äîindividual functions or methods‚Äîensuring they work as expected in isolation. They do not rely on external systems or integrations.\n",
            "Example: Testing the convert_langchain_aimessage_to_dict function to confirm it correctly converts an AI message to a dictionary format:\n",
            "from langchain_core.messages import AIMessage, ToolCall, convert_to_openai_messagesdef test_convert_to_openai_messages():    ai_message = AIMessage(        content=\"Let me call that tool for you!\",        tool_calls=[            ToolCall(name='parrot_multiply_tool', id='1', args={'a': 2, 'b': 3}),        ]    )        result = convert_to_openai_messages(ai_message)        expected = {        \"role\": \"assistant\",        \"tool_calls\": [            {                \"type\": \"function\",                \"id\": \"1\",                \"function\": {                    \"name\": \"parrot_multiply_tool\",                    \"arguments\": '{\"a\": 2, \"b\": 3}',                },            }        ],        \"content\": \"Let me call that tool for you!\",    }    assert result == expected  # Ensure conversion matches expected outputAPI Reference:AIMessage | ToolCall | convert_to_openai_messages\n",
            "\n",
            "Integration Tests‚Äã\n",
            "Definition: Integration tests validate that multiple components or systems work together as expected. For tools or integrations relying on external services, these tests often ensure end-to-end functionality.\n",
            "Example: Testing ParrotMultiplyTool with access to an API service that multiplies two numbers and adds 80:\n",
            "def test_integration_with_service():    tool = ParrotMultiplyTool()    result = tool.invoke({\"a\": 2, \"b\": 3})    assert result == 86\n",
            "\n",
            "Standard Tests‚Äã\n",
            "Definition: Standard tests are pre-defined tests provided by LangChain to ensure consistency and reliability across all tools and integrations. They include both unit and integration test templates tailored for LangChain components.\n",
            "Example: Subclassing LangChain's ToolsUnitTests or ToolsIntegrationTests to automatically run standard tests:\n",
            "from langchain_tests.unit_tests import ToolsUnitTestsclass TestParrotMultiplyToolUnit(ToolsUnitTests):    @property    def tool_constructor(self):        return ParrotMultiplyTool    def tool_invoke_params_example(self):        return {\"a\": 2, \"b\": 3}API Reference:ToolsUnitTests\n",
            "To learn more, check out our guide on how to add standard tests to an integration.Edit this pageWas this page helpful?PreviousStructured outputsNextString-in, string-out llmsUnit TestsIntegration TestsStandard TestsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Structured outputs | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideStructured outputsOn this pageStructured outputs\n",
            "Overview‚Äã\n",
            "For many applications, such as chatbots, models need to respond to users directly in natural language.\n",
            "However, there are scenarios where we need models to output in a structured format.\n",
            "For example, we might want to store the model output in a database and ensure that the output conforms to the database schema.\n",
            "This need motivates the concept of structured output, where models can be instructed to respond with a particular output structure.\n",
            "\n",
            "Key concepts‚Äã\n",
            "(1) Schema definition: The output structure is represented as a schema, which can be defined in several ways.\n",
            "(2) Returning structured output: The model is given this schema, and is instructed to return output that conforms to it.\n",
            "Recommended usage‚Äã\n",
            "This pseudocode illustrates the recommended workflow when using structured output.\n",
            "LangChain provides a method, with_structured_output(), that automates the process of binding the schema to the model and parsing the output.\n",
            "This helper function is available for all model providers that support structured output.\n",
            "# Define schemaschema = {\"foo\": \"bar\"}# Bind schema to modelmodel_with_structure = model.with_structured_output(schema)# Invoke the model to produce structured output that matches the schemastructured_output = model_with_structure.invoke(user_input)\n",
            "Schema definition‚Äã\n",
            "The central concept is that the output structure of model responses needs to be represented in some way.\n",
            "While types of objects you can use depend on the model you're working with, there are common types of objects that are typically allowed or recommended for structured output in Python.\n",
            "The simplest and most common format for structured output is a JSON-like structure, which in Python can be represented as a dictionary (dict) or list (list).\n",
            "JSON objects (or dicts in Python) are often used directly when the tool requires raw, flexible, and minimal-overhead structured data.\n",
            "{  \"answer\": \"The answer to the user's question\",  \"followup_question\": \"A followup question the user could ask\"}\n",
            "As a second example, Pydantic is particularly useful for defining structured output schemas because it offers type hints and validation.\n",
            "Here's an example of a Pydantic schema:\n",
            "from pydantic import BaseModel, Fieldclass ResponseFormatter(BaseModel):    \"\"\"Always use this tool to structure your response to the user.\"\"\"    answer: str = Field(description=\"The answer to the user's question\")    followup_question: str = Field(description=\"A followup question the user could ask\")\n",
            "Returning structured output‚Äã\n",
            "With a schema defined, we need a way to instruct the model to use it.\n",
            "While one approach is to include this schema in the prompt and ask nicely for the model to use it, this is not recommended.\n",
            "Several more powerful methods that utilizes native features in the model provider's API are available.\n",
            "Using tool calling‚Äã\n",
            "Many model providers support tool calling, a concept discussed in more detail in our tool calling guide.\n",
            "In short, tool calling involves binding a tool to a model and, when appropriate, the model can decide to call this tool and ensure its response conforms to the tool's schema.\n",
            "With this in mind, the central concept is straightforward: simply bind our schema to a model as a tool!\n",
            "Here is an example using the ResponseFormatter schema defined above:\n",
            "from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)# Bind responseformatter schema as a tool to the modelmodel_with_tools = model.bind_tools([ResponseFormatter])# Invoke the modelai_msg = model_with_tools.invoke(\"What is the powerhouse of the cell?\")API Reference:ChatOpenAI\n",
            "The arguments of the tool call are already extracted as a dictionary.\n",
            "This dictionary can be optionally parsed into a Pydantic object, matching our original ResponseFormatter schema.\n",
            "# Get the tool call argumentsai_msg.tool_calls[0][\"args\"]{'answer': \"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", 'followup_question': 'What is the function of ATP in the cell?'}# Parse the dictionary into a pydantic objectpydantic_object = ResponseFormatter.model_validate(ai_msg.tool_calls[0][\"args\"])\n",
            "JSON mode‚Äã\n",
            "In addition to tool calling, some model providers support a feature called JSON mode.\n",
            "This supports JSON schema definition as input and enforces the model to produce a conforming JSON output.\n",
            "You can find a table of model providers that support JSON mode here.\n",
            "Here is an example of how to use JSON mode with OpenAI:\n",
            "from langchain_openai import ChatOpenAImodel = ChatOpenAI(model=\"gpt-4o\").with_structured_output(method=\"json_mode\")ai_msg = model.invoke(\"Return a JSON object with key 'random_ints' and a value of 10 random ints in [0-99]\")ai_msg{'random_ints': [45, 67, 12, 34, 89, 23, 78, 56, 90, 11]}API Reference:ChatOpenAI\n",
            "Structured output method‚Äã\n",
            "There are a few challenges when producing structured output with the above methods:\n",
            "(1) When tool calling is used, tool call arguments needs to be parsed from a dictionary back to the original schema.\n",
            "(2) In addition, the model needs to be instructed to always use the tool when we want to enforce structured output, which is a provider specific setting.\n",
            "(3) When JSON mode is used, the output needs to be parsed into a JSON object.\n",
            "With these challenges in mind, LangChain provides a helper function (with_structured_output()) to streamline the process.\n",
            "\n",
            "This both binds the schema to the model as a tool and parses the output to the specified output schema.\n",
            "# Bind the schema to the modelmodel_with_structure = model.with_structured_output(ResponseFormatter)# Invoke the modelstructured_output = model_with_structure.invoke(\"What is the powerhouse of the cell?\")# Get back the pydantic objectstructured_outputResponseFormatter(answer=\"The powerhouse of the cell is the mitochondrion. Mitochondria are organelles that generate most of the cell's supply of adenosine triphosphate (ATP), which is used as a source of chemical energy.\", followup_question='What is the function of ATP in the cell?')\n",
            "Further readingFor more details on usage, see our how-to guide.Edit this pageWas this page helpful?PreviousStreamingNextTestingOverviewKey conceptsRecommended usageSchema definitionReturning structured outputUsing tool callingJSON modeStructured output methodCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Runnable interface | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRunnable interfaceOn this pageRunnable interface\n",
            "The Runnable interface is the foundation for working with LangChain components, and it's implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\n",
            "This guide covers the main concepts and methods of the Runnable interface, which allows developers to interact with various LangChain components in a consistent and predictable manner.\n",
            "Related Resources\n",
            "The \"Runnable\" Interface API Reference provides a detailed overview of the Runnable interface and its methods.\n",
            "A list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using the LangChain Expression Language (LCEL).\n",
            "\n",
            "Overview of runnable interface‚Äã\n",
            "The Runnable way defines a standard interface that allows a Runnable component to be:\n",
            "\n",
            "Invoked: A single input is transformed into an output.\n",
            "Batched: Multiple inputs are efficiently transformed into outputs.\n",
            "Streamed: Outputs are streamed as they are produced.\n",
            "Inspected: Schematic information about Runnable's input, output, and configuration can be accessed.\n",
            "Composed: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.\n",
            "\n",
            "Please review the LCEL Cheatsheet for some common patterns that involve the Runnable interface and LCEL expressions.\n",
            "\n",
            "Optimized parallel execution (batch)‚Äã\n",
            "\n",
            "LangChain Runnables offer a built-in batch (and batch_as_completed) API that allow you to process multiple inputs in parallel.\n",
            "Using these methods can significantly improve performance when needing to process multiple independent inputs, as the\n",
            "processing can be done in parallel instead of sequentially.\n",
            "The two batching options are:\n",
            "\n",
            "batch: Process multiple inputs in parallel, returning results in the same order as the inputs.\n",
            "batch_as_completed: Process multiple inputs in parallel, returning results as they complete. Results may arrive out of order, but each includes the input index for matching.\n",
            "\n",
            "The default implementation of batch and batch_as_completed use a thread pool executor to run the invoke method in parallel. This allows for efficient parallel execution without the need for users to manage threads, and speeds up code that is I/O-bound (e.g., making API requests, reading files, etc.). It will not be as effective for CPU-bound operations, as the GIL (Global Interpreter Lock) in Python will prevent true parallel execution.\n",
            "Some Runnables may provide their own implementations of batch and batch_as_completed that are optimized for their specific use case (e.g.,\n",
            "rely on a batch API provided by a model provider).\n",
            "noteThe async versions of abatch and abatch_as_completed relies on asyncio's gather and as_completed functions to run the ainvoke method in parallel.\n",
            "tipWhen processing a large number of inputs using batch or batch_as_completed, users may want to control the maximum number of parallel calls. This can be done by setting the max_concurrency attribute in the RunnableConfig dictionary. See the RunnableConfig for more information.Chat Models also have a built-in rate limiter that can be used to control the rate at which requests are made.\n",
            "Asynchronous support‚Äã\n",
            "\n",
            "Runnables expose an asynchronous API, allowing them to be called using the await syntax in Python. Asynchronous methods can be identified by the \"a\" prefix (e.g., ainvoke, abatch, astream, abatch_as_completed).\n",
            "Please refer to the Async Programming with LangChain guide for more details.\n",
            "Streaming APIs‚Äã\n",
            "\n",
            "Streaming is critical in making applications based on LLMs feel responsive to end-users.\n",
            "Runnables expose the following three streaming APIs:\n",
            "\n",
            "sync stream and async astream: yields the output a Runnable as it is generated.\n",
            "The async astream_events: a more advanced streaming API that allows streaming intermediate steps and final output\n",
            "The legacy async astream_log: a legacy streaming API that streams intermediate steps and final output\n",
            "\n",
            "Please refer to the Streaming Conceptual Guide for more details on how to stream in LangChain.\n",
            "Input and output types‚Äã\n",
            "Every Runnable is characterized by an input and output type. These input and output types can be any Python object, and are defined by the Runnable itself.\n",
            "Runnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types.\n",
            "\n",
            "invoke: Accepts an input and returns an output.\n",
            "batch: Accepts a list of inputs and returns a list of outputs.\n",
            "stream: Accepts an input and returns a generator that yields outputs.\n",
            "\n",
            "The input type and output type vary by component:\n",
            "ComponentInput TypeOutput TypePromptdictionaryPromptValueChatModela string, list of chat messages or a PromptValueChatMessageLLMa string, list of chat messages or a PromptValueStringOutputParserthe output of an LLM or ChatModelDepends on the parserRetrievera stringList of DocumentsToola string or dictionary, depending on the toolDepends on the tool\n",
            "Please refer to the individual component documentation for more information on the input and output types and how to use them.\n",
            "Inspecting schemas‚Äã\n",
            "noteThis is an advanced feature that is unnecessary for most users. You should probably\n",
            "skip this section unless you have a specific need to inspect the schema of a Runnable.\n",
            "In more advanced use cases, you may want to programmatically inspect the Runnable and determine what input and output types the Runnable expects and produces.\n",
            "The Runnable interface provides methods to get the JSON Schema of the input and output types of a Runnable, as well as Pydantic schemas for the input and output types.\n",
            "These APIs are mostly used internally for unit-testing and by LangServe which uses the APIs for input validation and generation of OpenAPI documentation.\n",
            "In addition, to the input and output types, some Runnables have been set up with additional run time configuration options.\n",
            "There are corresponding APIs to get the Pydantic Schema and JSON Schema of the configuration options for the Runnable.\n",
            "Please see the Configurable Runnables section for more information.\n",
            "MethodDescriptionget_input_schemaGives the Pydantic Schema of the input schema for the Runnable.get_output_schemaGives the Pydantic Schema of the output schema for the Runnable.config_schemaGives the Pydantic Schema of the config schema for the Runnable.get_input_jsonschemaGives the JSONSchema of the input schema for the Runnable.get_output_jsonschemaGives the JSONSchema of the output schema for the Runnable.get_config_jsonschemaGives the JSONSchema of the config schema for the Runnable.\n",
            "With_types‚Äã\n",
            "LangChain will automatically try to infer the input and output types of a Runnable based on available information.\n",
            "Currently, this inference does not work well for more complex Runnables that are built using LCEL composition, and the inferred input and / or output types may be incorrect. In these cases, we recommend that users override the inferred input and output types using the with_types method (API Reference).\n",
            "RunnableConfig‚Äã\n",
            "Any of the methods that are used to execute the runnable (e.g., invoke, batch, stream, astream_events) accept a second argument called\n",
            "RunnableConfig (API Reference). This argument is a dictionary that contains configuration for the Runnable that will be used\n",
            "at run time during the execution of the runnable.\n",
            "A RunnableConfig can have any of the following properties defined:\n",
            "AttributeDescriptionrun_nameName used for the given Runnable (not inherited).run_idUnique identifier for this call. sub-calls will get their own unique run ids.tagsTags for this call and any sub-calls.metadataMetadata for this call and any sub-calls.callbacksCallbacks for this call and any sub-calls.max_concurrencyMaximum number of parallel calls to make (e.g., used by batch).recursion_limitMaximum number of times a call can recurse (e.g., used by Runnables that return Runnables)configurableRuntime values for configurable attributes of the Runnable.\n",
            "Passing config to the invoke method is done like so:\n",
            "some_runnable.invoke(   some_input,    config={      'run_name': 'my_run',       'tags': ['tag1', 'tag2'],       'metadata': {'key': 'value'}         })\n",
            "Propagation of RunnableConfig‚Äã\n",
            "Many Runnables are composed of other Runnables, and it is important that the RunnableConfig is propagated to all sub-calls made by the Runnable. This allows providing run time configuration values to the parent Runnable that are inherited by all sub-calls.\n",
            "If this were not the case, it would be impossible to set and propagate callbacks or other configuration values like tags and metadata which\n",
            "are expected to be inherited by all sub-calls.\n",
            "There are two main patterns by which new Runnables are created:\n",
            "\n",
            "\n",
            "Declaratively using LangChain Expression Language (LCEL):\n",
            "chain = prompt | chat_model | output_parser\n",
            "\n",
            "\n",
            "Using a custom Runnable  (e.g., RunnableLambda) or using the @tool decorator:\n",
            "def foo(input):    # Note that .invoke() is used directly here    return bar_runnable.invoke(input)foo_runnable = RunnableLambda(foo)\n",
            "\n",
            "\n",
            "LangChain will try to propagate RunnableConfig automatically for both of the patterns.\n",
            "For handling the second pattern, LangChain relies on Python's contextvars.\n",
            "In Python 3.11 and above, this works out of the box, and you do not need to do anything special to propagate the RunnableConfig to the sub-calls.\n",
            "In Python 3.9 and 3.10, if you are using async code, you need to manually pass the RunnableConfig through to the Runnable when invoking it.\n",
            "This is due to a limitation in asyncio's tasks  in Python 3.9 and 3.10 which did\n",
            "not accept a context argument.\n",
            "Propagating the RunnableConfig manually is done like so:\n",
            "async def foo(input, config): # <-- Note the config argument    return await bar_runnable.ainvoke(input, config=config)    foo_runnable = RunnableLambda(foo)\n",
            "cautionWhen using Python 3.10 or lower and writing async code, RunnableConfig cannot be propagated\n",
            "automatically, and you will need to do it manually! This is a common pitfall when\n",
            "attempting to stream data using astream_events and astream_log as these methods\n",
            "rely on proper propagation of callbacks defined inside of RunnableConfig.\n",
            "Setting custom run name, tags, and metadata‚Äã\n",
            "The run_name, tags, and metadata attributes of the RunnableConfig dictionary can be used to set custom values for the run name, tags, and metadata for a given Runnable.\n",
            "The run_name is a string that can be used to set a custom name for the run. This name will be used in logs and other places to identify the run. It is not inherited by sub-calls.\n",
            "The tags and metadata attributes are lists and dictionaries, respectively, that can be used to set custom tags and metadata for the run. These values are inherited by sub-calls.\n",
            "Using these attributes can be useful for tracking and debugging runs, as they will be surfaced in LangSmith as trace attributes that you can\n",
            "filter and search on.\n",
            "The attributes will also be propagated to callbacks, and will appear in streaming APIs like astream_events as part of each event in the stream.\n",
            "Related\n",
            "How-to trace with LangChain\n",
            "\n",
            "Setting run id‚Äã\n",
            "noteThis is an advanced feature that is unnecessary for most users.\n",
            "You may need to set a custom run_id for a given run, in case you want\n",
            "to reference it later or correlate it with other systems.\n",
            "The run_id MUST be a valid UUID string and unique for each run. It is used to identify\n",
            "the parent run, sub-class will get their own unique run ids automatically.\n",
            "To set a custom run_id, you can pass it as a key-value pair in the config dictionary when invoking the Runnable:\n",
            "import uuidrun_id = uuid.uuid4()some_runnable.invoke(   some_input,    config={      'run_id': run_id   })# Do something with the run_id\n",
            "Setting recursion limit‚Äã\n",
            "noteThis is an advanced feature that is unnecessary for most users.\n",
            "Some Runnables may return other Runnables, which can lead to infinite recursion if not handled properly. To prevent this, you can set a recursion_limit in the RunnableConfig dictionary. This will limit the number of times a Runnable can recurse.\n",
            "Setting max concurrency‚Äã\n",
            "If using the batch or batch_as_completed methods, you can set the max_concurrency attribute in the RunnableConfig dictionary to control the maximum number of parallel calls to make. This can be useful when you want to limit the number of parallel calls to prevent overloading a server or API.\n",
            "tipIf you're trying to rate limit the number of requests made by a Chat Model, you can use the built-in rate limiter instead of setting max_concurrency, which will be more effective.See the How to handle rate limits guide for more information.\n",
            "Setting configurable‚Äã\n",
            "The configurable field is used to pass runtime values for configurable attributes of the Runnable.\n",
            "It is used frequently in LangGraph with\n",
            "LangGraph Persistence\n",
            "and memory.\n",
            "It is used for a similar purpose in RunnableWithMessageHistory to specify either\n",
            "a session_id / conversation_id to keep track of conversation history.\n",
            "In addition, you can use it to specify any custom configuration options to pass to any Configurable Runnable that they create.\n",
            "Setting callbacks\u0000\u0000‚Äã\n",
            "Use this option to configure callbacks for the runnable at\n",
            "runtime. The callbacks will be passed to all sub-calls made by the runnable.\n",
            "some_runnable.invoke(   some_input,   {      \"callbacks\": [         SomeCallbackHandler(),         AnotherCallbackHandler(),      ]   })\n",
            "Please read the Callbacks Conceptual Guide for more information on how to use callbacks in LangChain.\n",
            "importantIf you're using Python 3.9 or 3.10 in an async environment, you must propagate\n",
            "the RunnableConfig manually to sub-calls in some cases. Please see the\n",
            "Propagating RunnableConfig section for more information.\n",
            "Creating a runnable from a function‚Äã\n",
            "You may need to create a custom Runnable that runs arbitrary logic. This is especially\n",
            "useful if using LangChain Expression Language (LCEL) to compose\n",
            "multiple Runnables and you need to add custom processing logic in one of the steps.\n",
            "There are two ways to create a custom Runnable from a function:\n",
            "\n",
            "RunnableLambda: Use this for simple transformations where streaming is not required.\n",
            "RunnableGenerator: use this for more complex transformations when streaming is needed.\n",
            "\n",
            "See the How to run custom functions guide for more information on how to use RunnableLambda and RunnableGenerator.\n",
            "importantUsers should not try to subclass Runnables to create a new custom Runnable. It is\n",
            "much more complex and error-prone than simply using RunnableLambda or RunnableGenerator.\n",
            "Configurable runnables‚Äã\n",
            "noteThis is an advanced feature that is unnecessary for most users.It helps with configuration of large \"chains\" created using the LangChain Expression Language (LCEL)\n",
            "and is leveraged by LangServe for deployed Runnables.\n",
            "Sometimes you may want to experiment with, or even expose to the end user, multiple different ways of doing things with your Runnable. This could involve adjusting parameters like the temperature in a chat model or even switching between different chat models.\n",
            "To simplify this process, the Runnable interface provides two methods for creating configurable Runnables at runtime:\n",
            "\n",
            "configurable_fields: This method allows you to configure specific attributes in a Runnable. For example, the temperature attribute of a chat model.\n",
            "configurable_alternatives: This method enables you to specify alternative Runnables that can be run during runtime. For example, you could specify a list of different chat models that can be used.\n",
            "\n",
            "See the How to configure runtime chain internals guide for more information on how to configure runtime chain internals.Edit this pageWas this page helpful?PreviousRetrieversNextStreamingOverview of runnable interfaceOptimized parallel execution (batch)Asynchronous supportStreaming APIsInput and output typesInspecting schemasRunnableConfigPropagation of RunnableConfigSetting custom run name, tags, and metadataSetting run idSetting recursion limitSetting max concurrencySetting configurableSetting callbacksCreating a runnable from a functionConfigurable runnablesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Retrievers | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrieversOn this pageRetrievers\n",
            "\n",
            "Prerequisites\n",
            "Vector stores\n",
            "Embeddings\n",
            "Text splitters\n",
            "\n",
            "Overview‚Äã\n",
            "Many different types of retrieval systems exist, including vectorstores, graph databases, and relational databases.\n",
            "With the rise on popularity of large language models, retrieval systems have become an important component in AI application (e.g., RAG).\n",
            "Because of their importance and variability, LangChain provides a uniform interface for interacting with different types of retrieval systems.\n",
            "The LangChain retriever interface is straightforward:\n",
            "\n",
            "Input: A query (string)\n",
            "Output: A list of documents (standardized LangChain Document objects)\n",
            "\n",
            "Key concept‚Äã\n",
            "\n",
            "All retrievers implement a simple interface for retrieving documents using natural language queries.\n",
            "Interface‚Äã\n",
            "The only requirement for a retriever is the ability to accepts a query and return documents.\n",
            "In particular, LangChain's retriever class only requires that the _get_relevant_documents method is implemented, which takes a query: str and returns a list of Document objects that are most relevant to the query.\n",
            "The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application.\n",
            "A LangChain retriever is a runnable, which is a standard interface is for LangChain components.\n",
            "This means that it has a few common methods, including invoke, that are used to interact with it. A retriever can be invoked with a query:\n",
            "docs = retriever.invoke(query)\n",
            "Retrievers return a list of Document objects, which have two attributes:\n",
            "\n",
            "page_content: The content of this document. Currently is a string.\n",
            "metadata: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).\n",
            "\n",
            "Further reading\n",
            "See our how-to guide on building your own custom retriever.\n",
            "\n",
            "Common types‚Äã\n",
            "Despite the flexibility of the retriever interface, a few common types of retrieval systems are frequently used.\n",
            "Search apis‚Äã\n",
            "It's important to note that retrievers don't need to actually store documents.\n",
            "For example, we can be built retrievers on top of search APIs that simply return search results!\n",
            "See our retriever integrations with Amazon Kendra or Wikipedia Search.\n",
            "Relational or graph database‚Äã\n",
            "Retrievers can be built on top of relational or graph databases.\n",
            "In these cases, query analysis techniques to construct a structured query from natural language is critical.\n",
            "For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) retriever to be transformed into a SQL query behind the scenes.\n",
            "Further reading\n",
            "See our tutorial for context on how to build a retreiver using a SQL database and text-to-SQL.\n",
            "See our tutorial for context on how to build a retreiver using a graph database and text-to-Cypher.\n",
            "\n",
            "Lexical search‚Äã\n",
            "As discussed in our conceptual review of retrieval, many search engines are based upon matching words in a query to the words in each document.\n",
            "BM25 and TF-IDF are two popular lexical search algorithms.\n",
            "LangChain has retrievers for many popular lexical search algorithms / engines.\n",
            "Further reading\n",
            "See the BM25 retriever integration.\n",
            "See the TF-IDF retriever integration.\n",
            "See the Elasticsearch retriever integration.\n",
            "\n",
            "Vector store‚Äã\n",
            "Vector stores are a powerful and efficient way to index and retrieve unstructured data.\n",
            "A vectorstore can be used as a retriever by calling the as_retriever() method.\n",
            "vectorstore = MyVectorStore()retriever = vectorstore.as_retriever()\n",
            "Advanced retrieval patterns‚Äã\n",
            "Ensemble‚Äã\n",
            "Because the retriever interface is so simple, returning a list of Document objects given a search query, it is possible to combine multiple retrievers using ensembling.\n",
            "This is particularly useful when you have multiple retrievers that are good at finding different types of relevant documents.\n",
            "It is easy to create an ensemble retriever that combines multiple retrievers with linear weighted scores:\n",
            "# Initialize the ensemble retrieverensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, vector_store_retriever], weights=[0.5, 0.5])\n",
            "When ensembling, how do we combine search results from many retrievers?\n",
            "This motivates the concept of re-ranking, which takes the output of multiple retrievers and combines them using a more sophisticated algorithm such as Reciprocal Rank Fusion (RRF).\n",
            "Source document retention‚Äã\n",
            "Many retrievers utilize some kind of index to make documents easily searchable.\n",
            "The process of indexing can include a transformation step (e.g., vectorstores often use document splitting).\n",
            "Whatever transformation is used, can be very useful to retain a link between the transformed document and the original, giving the retriever the ability to return the original document.\n",
            "\n",
            "This is particularly useful in AI applications, because it ensures no loss in document context for the model.\n",
            "For example, you may use small chunk size for indexing documents in a vectorstore.\n",
            "If you return only the chunks as the retrieval result, then the model will have lost the original document context for the chunks.\n",
            "LangChain has two different retrievers that can be used to address this challenge.\n",
            "The Multi-Vector retriever allows the user to use any document transformation (e.g., use an LLM to write a summary of the document) for indexing while retaining linkage to the source document.\n",
            "The ParentDocument retriever links document chunks from a text-splitter transformation for indexing while retaining linkage to the source document.\n",
            "NameIndex TypeUses an LLMWhen to UseDescriptionParentDocumentVector store + Document StoreNoIf your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together.This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks).Multi VectorVector store + Document StoreSometimes during indexingIf you are able to extract information from documents that you think is more relevant to index than the text itself.This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions.\n",
            "Further reading\n",
            "See our how-to guide on using the ParentDocument retriever.\n",
            "See our how-to guide on using the MultiVector retriever.\n",
            "See our RAG from Scratch video on the multi vector retriever.\n",
            "Edit this pageWas this page helpful?PreviousRetrievalNextRunnable interfaceOverviewKey conceptInterfaceCommon typesSearch apisRelational or graph databaseLexical searchVector storeAdvanced retrieval patternsEnsembleSource document retentionCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Retrieval | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideRetrievalOn this pageRetrieval\n",
            "Prerequisites\n",
            "Retrievers\n",
            "Vector stores\n",
            "Embeddings\n",
            "Text splitters\n",
            "\n",
            "SecuritySome of the concepts reviewed here utilize models to generate queries (e.g., for SQL or graph databases).\n",
            "There are inherent risks in doing this.\n",
            "Make sure that your database connection permissions are scoped as narrowly as possible for your application's needs.\n",
            "This will mitigate, though not eliminate, the risks of building a model-driven system capable of querying databases.\n",
            "For more on general security best practices, see our security guide.\n",
            "Overview‚Äã\n",
            "Retrieval systems are fundamental to many AI applications, efficiently identifying relevant information from large datasets.\n",
            "These systems accommodate various data formats:\n",
            "\n",
            "Unstructured text (e.g., documents) is often stored in vector stores or lexical search indexes.\n",
            "Structured data is typically housed in relational or graph databases with defined schemas.\n",
            "\n",
            "Despite the growing diversity in data formats, modern AI applications increasingly aim to make all types of data accessible through natural language interfaces.\n",
            "Models play a crucial role in this process by translating natural language queries into formats compatible with the underlying search index or database.\n",
            "This translation enables more intuitive and flexible interactions with complex data structures.\n",
            "Key concepts‚Äã\n",
            "\n",
            "(1) Query analysis: A process where models transform or construct search queries to optimize retrieval.\n",
            "(2) Information retrieval: Search queries are used to fetch information from various retrieval systems.\n",
            "Query analysis‚Äã\n",
            "While users typically prefer to interact with retrieval systems using natural language, these systems may require specific query syntax or benefit from certain keywords.\n",
            "Query analysis serves as a bridge between raw user input and optimized search queries. Some common applications of query analysis include:\n",
            "\n",
            "Query Re-writing: Queries can be re-written or expanded to improve semantic or lexical searches.\n",
            "Query Construction: Search indexes may require structured queries (e.g., SQL for databases).\n",
            "\n",
            "Query analysis employs models to transform or construct optimized search queries from raw user input.\n",
            "Query re-writing‚Äã\n",
            "Retrieval systems should ideally handle a wide spectrum of user inputs, from simple and poorly worded queries to complex, multi-faceted questions.\n",
            "To achieve this versatility, a popular approach is to use models to transform raw user queries into more effective search queries.\n",
            "This transformation can range from simple keyword extraction to sophisticated query expansion and reformulation.\n",
            "Here are some key benefits of using models for query analysis in unstructured data retrieval:\n",
            "\n",
            "Query Clarification: Models can rephrase ambiguous or poorly worded queries for clarity.\n",
            "Semantic Understanding: They can capture the intent behind a query, going beyond literal keyword matching.\n",
            "Query Expansion: Models can generate related terms or concepts to broaden the search scope.\n",
            "Complex Query Handling: They can break down multi-part questions into simpler sub-queries.\n",
            "\n",
            "Various techniques have been developed to leverage models for query re-writing, including:\n",
            "NameWhen to useDescriptionMulti-queryWhen you want to ensure high recall in retrieval by providing multiple phrasings of a question.Rewrite the user question with multiple phrasings, retrieve documents for each rewritten question, return the unique documents for all queries.DecompositionWhen a question can be broken down into smaller subproblems.Decompose a question into a set of subproblems / questions, which can either be solved sequentially (use the answer from first + retrieval to answer the second) or in parallel (consolidate each answer into final answer).Step-backWhen a higher-level conceptual understanding is required.First prompt the LLM to ask a generic step-back question about higher-level concepts or principles, and retrieve relevant facts about them. Use this grounding to help answer the user question. Paper.HyDEIf you have challenges retrieving relevant documents using the raw user inputs.Use an LLM to convert questions into hypothetical documents that answer the question. Use the embedded hypothetical documents to retrieve real documents with the premise that doc-doc similarity search can produce more relevant matches. Paper.\n",
            "As an example, query decomposition can simply be accomplished using prompting and a structured output that enforces a list of sub-questions.\n",
            "These can then be run sequentially or in parallel on a downstream retrieval system.\n",
            "from typing import Listfrom pydantic import BaseModel, Fieldfrom langchain_openai import ChatOpenAIfrom langchain_core.messages import SystemMessage, HumanMessage# Define a pydantic model to enforce the output structureclass Questions(BaseModel):    questions: List[str] = Field(        description=\"A list of sub-questions related to the input query.\"    )# Create an instance of the model and enforce the output structuremodel = ChatOpenAI(model=\"gpt-4o\", temperature=0) structured_model = model.with_structured_output(Questions)# Define the system promptsystem = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\nThe goal is to break down the input into a set of sub-problems / sub-questions that can be answered independently. \\n\"\"\"# Pass the question to the modelquestion = \"\"\"What are the main components of an LLM-powered autonomous agent system?\"\"\"questions = structured_model.invoke([SystemMessage(content=system)]+[HumanMessage(content=question)])API Reference:ChatOpenAI | SystemMessage | HumanMessage\n",
            "tipSee our RAG from Scratch videos for a few different specific approaches:\n",
            "Multi-query\n",
            "Decomposition\n",
            "Step-back\n",
            "HyDE\n",
            "\n",
            "Query construction‚Äã\n",
            "Query analysis also can focus on translating natural language queries into specialized query languages or filters.\n",
            "This translation is crucial for effectively interacting with various types of databases that house structured or semi-structured data.\n",
            "\n",
            "\n",
            "Structured Data examples: For relational and graph databases, Domain-Specific Languages (DSLs) are used to query data.\n",
            "\n",
            "Text-to-SQL: Converts natural language to SQL for relational databases.\n",
            "Text-to-Cypher: Converts natural language to Cypher for graph databases.\n",
            "\n",
            "\n",
            "\n",
            "Semi-structured Data examples: For vectorstores, queries can combine semantic search with metadata filtering.\n",
            "\n",
            "Natural Language to Metadata Filters: Converts user queries into appropriate metadata filters.\n",
            "\n",
            "\n",
            "\n",
            "These approaches leverage models to bridge the gap between user intent and the specific query requirements of different data storage systems. Here are some popular techniques:\n",
            "NameWhen to UseDescriptionSelf QueryIf users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text.This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filter to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself).Text to SQLIf users are asking questions that require information housed in a relational database, accessible via SQL.This uses an LLM to transform user input into a SQL query.Text-to-CypherIf users are asking questions that require information housed in a graph database, accessible via Cypher.This uses an LLM to transform user input into a Cypher query.\n",
            "As an example, here is how to use the SelfQueryRetriever to convert natural language queries into metadata filters.\n",
            "metadata_field_info = schema_for_metadata document_content_description = \"Brief summary of a movie\"llm = ChatOpenAI(temperature=0)retriever = SelfQueryRetriever.from_llm(    llm,    vectorstore,    document_content_description,    metadata_field_info,)\n",
            "Further reading\n",
            "See our tutorials on text-to-SQL, text-to-Cypher, and query analysis for metadata filters.\n",
            "See our blog post overview.\n",
            "See our RAG from Scratch video on query construction.\n",
            "\n",
            "Information retrieval‚Äã\n",
            "Common retrieval systems‚Äã\n",
            "Lexical search indexes‚Äã\n",
            "Many search engines are based upon matching words in a query to the words in each document.\n",
            "This approach is called lexical retrieval, using search algorithms that are typically based upon word frequencies.\n",
            "The intution is simple: a word appears frequently both in the user‚Äôs query and a particular document, then this document might be a good match.\n",
            "The particular data structure used to implement this is often an inverted index.\n",
            "This types of index contains a list of words and a mapping of each word to a list of locations at which it occurs in various documents.\n",
            "Using this data structure, it is possible to efficiently match the words in search queries to the documents in which they appear.\n",
            "BM25 and TF-IDF are two popular lexical search algorithms.\n",
            "Further reading\n",
            "See the BM25 retriever integration.\n",
            "See the Elasticsearch retriever integration.\n",
            "\n",
            "Vector indexes‚Äã\n",
            "Vector indexes are an alternative way to index and store unstructured data.\n",
            "See our conceptual guide on vectorstores for a detailed overview.\n",
            "In short, rather than using word frequencies, vectorstores use an embedding model to compress documents into high-dimensional vector representation.\n",
            "This allows for efficient similarity search over embedding vectors using simple mathematical operations like cosine similarity.\n",
            "Further reading\n",
            "See our how-to guide for more details on working with vectorstores.\n",
            "See our list of vectorstore integrations.\n",
            "See Cameron Wolfe's blog post on the basics of vector search.\n",
            "\n",
            "Relational databases‚Äã\n",
            "Relational databases are a fundamental type of structured data storage used in many applications.\n",
            "They organize data into tables with predefined schemas, where each table represents an entity or relationship.\n",
            "Data is stored in rows (records) and columns (attributes), allowing for efficient querying and manipulation through SQL (Structured Query Language).\n",
            "Relational databases excel at maintaining data integrity, supporting complex queries, and handling relationships between different data entities.\n",
            "Further reading\n",
            "See our tutorial for working with SQL databases.\n",
            "See our SQL database toolkit.\n",
            "\n",
            "Graph databases‚Äã\n",
            "Graph databases are a specialized type of database designed to store and manage highly interconnected data.\n",
            "Unlike traditional relational databases, graph databases use a flexible structure consisting of nodes (entities), edges (relationships), and properties.\n",
            "This structure allows for efficient representation and querying of complex, interconnected data.\n",
            "Graph databases store data in a graph structure, with nodes, edges, and properties.\n",
            "They are particularly useful for storing and querying complex relationships between data points, such as social networks, supply-chain management, fraud detection, and recommendation services\n",
            "Further reading\n",
            "See our tutorial for working with graph databases.\n",
            "See our list of graph database integrations.\n",
            "See Neo4j's starter kit for LangChain.\n",
            "\n",
            "Retriever‚Äã\n",
            "LangChain provides a unified interface for interacting with various retrieval systems through the retriever concept. The interface is straightforward:\n",
            "\n",
            "Input: A query (string)\n",
            "Output: A list of documents (standardized LangChain Document objects)\n",
            "\n",
            "You can create a retriever using any of the retrieval systems mentioned earlier. The query analysis techniques we discussed are particularly useful here, as they enable natural language interfaces for databases that typically require structured query languages.\n",
            "For example, you can build a retriever for a SQL database using text-to-SQL conversion. This allows a natural language query (string) to be transformed into a SQL query behind the scenes.\n",
            "Regardless of the underlying retrieval system, all retrievers in LangChain share a common interface. You can use them with the simple invoke method:\n",
            "docs = retriever.invoke(query)\n",
            "Further reading\n",
            "See our conceptual guide on retrievers.\n",
            "See our how-to guide on working with retrievers.\n",
            "Edit this pageWas this page helpful?PreviousRetrieval augmented generation (RAG)NextRetrieversOverviewKey conceptsQuery analysisQuery re-writingQuery constructionInformation retrievalCommon retrieval systemsRetrieverCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Prompt Templates | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guidePrompt TemplatesOn this pagePrompt Templates\n",
            "Prompt templates help to translate user input and parameters into instructions for a language model.\n",
            "This can be used to guide a model's response, helping it understand the context and generate relevant and coherent language-based output.\n",
            "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
            "Prompt Templates output a PromptValue. This PromptValue can be passed to an LLM or a ChatModel, and can also be cast to a string or a list of messages.\n",
            "The reason this PromptValue exists is to make it easy to switch between strings and messages.\n",
            "There are a few different types of prompt templates:\n",
            "String PromptTemplates‚Äã\n",
            "These prompt templates are used to format a single string, and generally are used for simpler inputs.\n",
            "For example, a common way to construct and use a PromptTemplate is as follows:\n",
            "from langchain_core.prompts import PromptTemplateprompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")prompt_template.invoke({\"topic\": \"cats\"})API Reference:PromptTemplate\n",
            "ChatPromptTemplates‚Äã\n",
            "These prompt templates are used to format a list of messages. These \"templates\" consist of a list of templates themselves.\n",
            "For example, a common way to construct and use a ChatPromptTemplate is as follows:\n",
            "from langchain_core.prompts import ChatPromptTemplateprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"user\", \"Tell me a joke about {topic}\")])prompt_template.invoke({\"topic\": \"cats\"})API Reference:ChatPromptTemplate\n",
            "In the above example, this ChatPromptTemplate will construct two messages when called.\n",
            "The first is a system message, that has no variables to format.\n",
            "The second is a HumanMessage, and will be formatted by the topic variable the user passes in.\n",
            "MessagesPlaceholder‚Äã\n",
            "\n",
            "This prompt template is responsible for adding a list of messages in a particular place.\n",
            "In the above ChatPromptTemplate, we saw how we could format two messages, each one a string.\n",
            "But what if we wanted the user to pass in a list of messages that we would slot into a particular spot?\n",
            "This is how you use MessagesPlaceholder.\n",
            "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholderfrom langchain_core.messages import HumanMessageprompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    MessagesPlaceholder(\"msgs\")])prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})API Reference:ChatPromptTemplate | MessagesPlaceholder | HumanMessage\n",
            "This will produce a list of two messages, the first one being a system message, and the second one being the HumanMessage we passed in.\n",
            "If we had passed in 5 messages, then it would have produced 6 messages in total (the system message plus the 5 passed in).\n",
            "This is useful for letting a list of messages be slotted into a particular spot.\n",
            "An alternative way to accomplish the same thing without using the MessagesPlaceholder class explicitly is:\n",
            "prompt_template = ChatPromptTemplate([    (\"system\", \"You are a helpful assistant\"),    (\"placeholder\", \"{msgs}\") # <-- This is the changed part])\n",
            "For specifics on how to use prompt templates, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousOutput parsersNextRetrieval augmented generation (RAG)String PromptTemplatesChatPromptTemplatesMessagesPlaceholderCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Output parsers | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideOutput parsersOutput parsers\n",
            "\n",
            "noteThe information here refers to parsers that take a text output from a model try to parse it into a more structured representation.\n",
            "More and more models are supporting function (or tool) calling, which handles this automatically.\n",
            "It is recommended to use function/tool calling rather than output parsing.\n",
            "See documentation for that here.\n",
            "Output parser is responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks.\n",
            "Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n",
            "LangChain has lots of different types of output parsers. This is a list of output parsers LangChain supports. The table below has various pieces of information:\n",
            "\n",
            "Name: The name of the output parser\n",
            "Supports Streaming: Whether the output parser supports streaming.\n",
            "Has Format Instructions: Whether the output parser has format instructions. This is generally available except when (a) the desired schema is not specified in the prompt but rather in other parameters (like OpenAI function calling), or (b) when the OutputParser wraps another OutputParser.\n",
            "Calls LLM: Whether this output parser itself calls an LLM. This is usually only done by output parsers that attempt to correct misformatted output.\n",
            "Input Type: Expected input type. Most output parsers work on both strings and messages, but some (like OpenAI Functions) need a message with specific kwargs.\n",
            "Output Type: The output type of the object returned by the parser.\n",
            "Description: Our commentary on this output parser and when to use it.\n",
            "\n",
            "NameSupports StreamingHas Format InstructionsCalls LLMInput TypeOutput TypeDescriptionStr‚úÖstr | MessageStringParses texts from message objects. Useful for handling variable formats of message content (e.g., extracting text from content blocks).JSON‚úÖ‚úÖstr | MessageJSON objectReturns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.XML‚úÖ‚úÖstr | MessagedictReturns a dictionary of tags. Use when XML output is needed. Use with models that are good at writing XML (like Anthropic's).CSV‚úÖ‚úÖstr | MessageList[str]Returns a list of comma separated values.OutputFixing‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the error message and the bad output to an LLM and ask it to fix the output.RetryWithError‚úÖstr | MessageWraps another output parser. If that output parser errors, then this will pass the original inputs, the bad output, and the error message to an LLM and ask it to fix it. Compared to OutputFixingParser, this one also sends the original instructions.Pydantic‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format.YAML‚úÖstr | Messagepydantic.BaseModelTakes a user defined Pydantic model and returns data in that format. Uses YAML to encode it.PandasDataFrame‚úÖstr | MessagedictUseful for doing operations with pandas DataFrames.Enum‚úÖstr | MessageEnumParses response into one of the provided enum values.Datetime‚úÖstr | Messagedatetime.datetimeParses response into a datetime string.Structured‚úÖstr | MessageDict[str, str]An output parser that returns structured information. It is less powerful than other output parsers since it only allows for fields to be strings. This can be useful when you are working with smaller LLMs.\n",
            "For specifics on how to use output parsers, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousMultimodalityNextPrompt TemplatesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Messages | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideMessagesOn this pageMessages\n",
            "Prerequisites\n",
            "Chat Models\n",
            "\n",
            "Overview‚Äã\n",
            "Messages are the unit of communication in chat models. They are used to represent the input and output of a chat model, as well as any additional context or metadata that may be associated with a conversation.\n",
            "Each message has a role (e.g., \"user\", \"assistant\") and content (e.g., text, multimodal data) with additional metadata that varies depending on the chat model provider.\n",
            "LangChain provides a unified message format that can be used across chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
            "What is inside a message?‚Äã\n",
            "A message typically consists of the following pieces of information:\n",
            "\n",
            "Role: The role of the message (e.g., \"user\", \"assistant\").\n",
            "Content: The content of the message (e.g., text, multimodal data).\n",
            "Additional metadata: id, name, token usage and other model-specific metadata.\n",
            "\n",
            "Role‚Äã\n",
            "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
            "RoleDescriptionsystemUsed to tell the chat model how to behave and provide additional context. Not supported by all chat model providers.userRepresents input from a user interacting with the model, usually in the form of text or other interactive input.assistantRepresents a response from the model, which can include text or a request to invoke tools.toolA message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.function (legacy)This is a legacy role, corresponding to OpenAI's legacy function-calling API. tool role should be used instead.\n",
            "Content‚Äã\n",
            "The content of a message text or a list of dictionaries representing multimodal data (e.g., images, audio, video). The exact format of the content can vary between different chat model providers.\n",
            "Currently, most chat models support text as the primary content type, with some models also supporting multimodal data. However, support for multimodal data is still limited across most chat model providers.\n",
            "For more information see:\n",
            "\n",
            "SystemMessage -- for content which should be passed to direct the conversation\n",
            "HumanMessage -- for content in the input from the user.\n",
            "AIMessage -- for content in the response from the model.\n",
            "Multimodality -- for more information on multimodal content.\n",
            "\n",
            "Other Message Data‚Äã\n",
            "Depending on the chat model provider, messages can include other data such as:\n",
            "\n",
            "ID: An optional unique identifier for the message.\n",
            "Name: An optional name property which allows differentiate between different entities/speakers with the same role. Not all models support this!\n",
            "Metadata: Additional information about the message, such as timestamps, token usage, etc.\n",
            "Tool Calls: A request made by the model to call one or more tools> See tool calling for more information.\n",
            "\n",
            "Conversation Structure‚Äã\n",
            "The sequence of messages into a chat model should follow a specific structure to ensure that the chat model can generate a valid response.\n",
            "For example, a typical conversation structure might look like this:\n",
            "\n",
            "User Message: \"Hello, how are you?\"\n",
            "Assistant Message: \"I'm doing well, thank you for asking.\"\n",
            "User Message: \"Can you tell me a joke?\"\n",
            "Assistant Message: \"Sure! Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
            "\n",
            "Please read the chat history guide for more information on managing chat history and ensuring that the conversation structure is correct.\n",
            "LangChain Messages‚Äã\n",
            "LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
            "LangChain messages are Python objects that subclass from a BaseMessage.\n",
            "The five main message types are:\n",
            "\n",
            "SystemMessage: corresponds to system role\n",
            "HumanMessage: corresponds to user role\n",
            "AIMessage: corresponds to assistant role\n",
            "AIMessageChunk: corresponds to assistant role, used for streaming responses\n",
            "ToolMessage: corresponds to tool role\n",
            "\n",
            "Other important messages include:\n",
            "\n",
            "RemoveMessage -- does not correspond to any role. This is an abstraction, mostly used in LangGraph to manage chat history.\n",
            "Legacy FunctionMessage: corresponds to the function role in OpenAI's legacy function-calling API.\n",
            "\n",
            "You can find more information about messages in the API Reference.\n",
            "SystemMessage‚Äã\n",
            "A SystemMessage is used to prime the behavior of the AI model and provide additional context, such as instructing the model to adopt a specific persona or setting the tone of the conversation (e.g., \"This is a conversation about cooking\").\n",
            "Different chat providers may support system message in one of the following ways:\n",
            "\n",
            "Through a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\"\n",
            "Through a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter.\n",
            "No support for system messages: Some models do not support system messages at all.\n",
            "\n",
            "Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider‚Äôs capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter.\n",
            "If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this behavior is not yet consistently enforced across all implementations, and if using a less popular implementation of a chat model (e.g., an implementation from the langchain-community package) it is recommended to check the specific documentation for that model.\n",
            "HumanMessage‚Äã\n",
            "The HumanMessage corresponds to the \"user\" role. A human message represents input from a user interacting with the model.\n",
            "Text Content‚Äã\n",
            "Most chat models expect the user input to be in the form of text.\n",
            "from langchain_core.messages import HumanMessagemodel.invoke([HumanMessage(content=\"Hello, how are you?\")])API Reference:HumanMessage\n",
            "tipWhen invoking a chat model with a string as input, LangChain will automatically convert the string into a HumanMessage object. This is mostly useful for quick testing.model.invoke(\"Hello, how are you?\")\n",
            "Multi-modal Content‚Äã\n",
            "Some chat models accept multimodal inputs, such as images, audio, video, or files like PDFs.\n",
            "Please see the multimodality guide for more information.\n",
            "AIMessage‚Äã\n",
            "AIMessage is used to represent a message with the role \"assistant\". This is the response from the model, which can include text or a request to invoke tools. It could also include other media types like images, audio, or video -- though this is still uncommon at the moment.\n",
            "from langchain_core.messages import HumanMessageai_message = model.invoke([HumanMessage(\"Tell me a joke\")])ai_message # <-- AIMessageAPI Reference:HumanMessage\n",
            "An AIMessage has the following attributes. The attributes which are standardized are the ones that LangChain attempts to standardize across different chat model providers. raw fields are specific to the model provider and may vary.\n",
            "AttributeStandardized/RawDescriptioncontentRawUsually a string, but can be a list of content blocks. See content for details.tool_callsStandardizedTool calls associated with the message. See tool calling for details.invalid_tool_callsStandardizedTool calls with parsing errors associated with the message. See tool calling for details.usage_metadataStandardizedUsage metadata for a message, such as token counts. See Usage Metadata API Reference.idStandardizedAn optional unique identifier for the message, ideally provided by the provider/model that created the message.response_metadataRawResponse metadata, e.g., response headers, logprobs, token counts.\n",
            "content‚Äã\n",
            "The content property of an AIMessage represents the response generated by the chat model.\n",
            "The content is either:\n",
            "\n",
            "text -- the norm for virtually all chat models.\n",
            "A list of dictionaries -- Each dictionary represents a content block and is associated with a type.\n",
            "\n",
            "Used by Anthropic for surfacing agent thought process when doing tool calling.\n",
            "Used by OpenAI for audio outputs. Please see multi-modal content for more information.\n",
            "\n",
            "\n",
            "\n",
            "importantThe content property is not standardized across different chat model providers, mostly because there are\n",
            "still few examples to generalize from.\n",
            "AIMessageChunk‚Äã\n",
            "It is common to stream responses for the chat model as they are being generated, so the user can see the response in real-time instead of waiting for the entire response to be generated before displaying it.\n",
            "It is returned from the stream, astream and astream_events methods of the chat model.\n",
            "For example,\n",
            "for chunk in model.stream([HumanMessage(\"what color is the sky?\")]):    print(chunk)\n",
            "AIMessageChunk follows nearly the same structure as AIMessage, but uses a different ToolCallChunk\n",
            "to be able to stream tool calling in a standardized manner.\n",
            "Aggregating‚Äã\n",
            "AIMessageChunks support the + operator to merge them into a single AIMessage. This is useful when you want to display the final response to the user.\n",
            "ai_message = chunk1 + chunk2 + chunk3 + ...\n",
            "ToolMessage‚Äã\n",
            "This represents a message with role \"tool\", which contains the result of calling a tool. In addition to role and content, this message has:\n",
            "\n",
            "a tool_call_id field which conveys the id of the call to the tool that was called to produce this result.\n",
            "an artifact field which can be used to pass along arbitrary artifacts of the tool execution which are useful to track but which should not be sent to the model.\n",
            "\n",
            "Please see tool calling for more information.\n",
            "RemoveMessage‚Äã\n",
            "This is a special message type that does not correspond to any roles. It is used\n",
            "for managing chat history in LangGraph.\n",
            "Please see the following for more information on how to use the RemoveMessage:\n",
            "\n",
            "Memory conceptual guide\n",
            "How to delete messages\n",
            "\n",
            "(Legacy) FunctionMessage‚Äã\n",
            "This is a legacy message type, corresponding to OpenAI's legacy function-calling API. ToolMessage should be used instead to correspond to the updated tool-calling API.\n",
            "OpenAI Format‚Äã\n",
            "Inputs‚Äã\n",
            "Chat models also accept OpenAI's format as inputs to chat models:\n",
            "chat_model.invoke([    {        \"role\": \"user\",        \"content\": \"Hello, how are you?\",    },    {        \"role\": \"assistant\",        \"content\": \"I'm doing well, thank you for asking.\",    },    {        \"role\": \"user\",        \"content\": \"Can you tell me a joke?\",    }])\n",
            "Outputs‚Äã\n",
            "At the moment, the output of the model will be in terms of LangChain messages, so you will need to convert the output to the OpenAI format if you\n",
            "need OpenAI format for the output as well.\n",
            "The convert_to_openai_messages utility function can be used to convert from LangChain messages to OpenAI format.Edit this pageWas this page helpful?PreviousLangChain Expression Language (LCEL)NextMultimodalityOverviewWhat is inside a message?RoleContentOther Message DataConversation StructureLangChain MessagesSystemMessageHumanMessageAIMessageAIMessageChunkToolMessageRemoveMessage(Legacy) FunctionMessageOpenAI FormatInputsOutputsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "LangChain Expression Language (LCEL) | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideLangChain Expression Language (LCEL)On this pageLangChain Expression Language (LCEL)\n",
            "Prerequisites\n",
            "Runnable Interface\n",
            "\n",
            "The LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables.\n",
            "This means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains.\n",
            "We often refer to a Runnable created using LCEL as a \"chain\". It's important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface.\n",
            "note\n",
            "The LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions.\n",
            "Please see the following list of how-to guides that cover common tasks with LCEL.\n",
            "A list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL.\n",
            "\n",
            "Benefits of LCEL‚Äã\n",
            "LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n",
            "\n",
            "Optimized parallel execution: Run Runnables in parallel using RunnableParallel or run multiple inputs through a given chain in parallel using the Runnable Batch API. Parallel execution can significantly reduce the latency as processing can be done in parallel instead of sequentially.\n",
            "Guaranteed Async support: Any chain built with LCEL can be run asynchronously using the Runnable Async API. This can be useful when running chains in a server environment where you want to handle large number of requests concurrently.\n",
            "Simplify streaming: LCEL chains can be streamed, allowing for incremental output as the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out).\n",
            "\n",
            "Other benefits include:\n",
            "\n",
            "Seamless LangSmith tracing\n",
            "As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step.\n",
            "With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.\n",
            "Standard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable.\n",
            "Deployable with LangServe: Chains built with LCEL can be deployed using for production use.\n",
            "\n",
            "Should I use LCEL?‚Äã\n",
            "LCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way.\n",
            "While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application requires complex state management, branching, cycles or multiple agents, we recommend that users take advantage of LangGraph.\n",
            "In LangGraph, users define graphs that specify the application's flow. This allows users to keep using LCEL within individual nodes when LCEL is needed, while making it easy to define complex orchestration logic that is more readable and maintainable.\n",
            "Here are some guidelines:\n",
            "\n",
            "If you are making a single LLM call, you don't need LCEL; instead call the underlying chat model directly.\n",
            "If you have a simple chain (e.g., prompt + llm + parser, simple retrieval set up etc.), LCEL is a reasonable fit, if you're taking advantage of the LCEL benefits.\n",
            "If you're building a complex chain (e.g., with branching, cycles, multiple agents, etc.) use LangGraph instead. Remember that you can always use LCEL within individual nodes in LangGraph.\n",
            "\n",
            "Composition Primitives‚Äã\n",
            "LCEL chains are built by composing existing Runnables together. The two main composition primitives are RunnableSequence and RunnableParallel.\n",
            "Many other composition primitives (e.g., RunnableAssign) can be thought of as variations of these two primitives.\n",
            "noteYou can find a list of all composition primitives in the LangChain Core API Reference.\n",
            "RunnableSequence‚Äã\n",
            "RunnableSequence is a composition primitive that allows you \"chain\" multiple runnables sequentially, with the output of one runnable serving as the input to the next.\n",
            "from langchain_core.runnables import RunnableSequencechain = RunnableSequence([runnable1, runnable2])API Reference:RunnableSequence\n",
            "Invoking the chain with some input:\n",
            "final_output = chain.invoke(some_input)\n",
            "corresponds to the following:\n",
            "output1 = runnable1.invoke(some_input)final_output = runnable2.invoke(output1)\n",
            "noterunnable1 and runnable2 are placeholders for any Runnable that you want to chain together.\n",
            "RunnableParallel‚Äã\n",
            "RunnableParallel is a composition primitive that allows you to run multiple runnables concurrently, with the same input provided to each.\n",
            "from langchain_core.runnables import RunnableParallelchain = RunnableParallel({    \"key1\": runnable1,    \"key2\": runnable2,})API Reference:RunnableParallel\n",
            "Invoking the chain with some input:\n",
            "final_output = chain.invoke(some_input)\n",
            "Will yield a final_output dictionary with the same keys as the input dictionary, but with the values replaced by the output of the corresponding runnable.\n",
            "{    \"key1\": runnable1.invoke(some_input),    \"key2\": runnable2.invoke(some_input),}\n",
            "Recall, that the runnables are executed in parallel, so while the result is the same as\n",
            "dictionary comprehension shown above, the execution time is much faster.\n",
            "noteRunnableParallelsupports both synchronous and asynchronous execution (as all Runnables do).\n",
            "For synchronous execution, RunnableParallel uses a ThreadPoolExecutor to run the runnables concurrently.\n",
            "For asynchronous execution, RunnableParallel uses asyncio.gather to run the runnables concurrently.\n",
            "\n",
            "Composition Syntax‚Äã\n",
            "The usage of RunnableSequence and RunnableParallel is so common that we created a shorthand syntax for using them. This helps\n",
            "to make the code more readable and concise.\n",
            "The | operator‚Äã\n",
            "We have overloaded the | operator to create a RunnableSequence from two Runnables.\n",
            "chain = runnable1 | runnable2\n",
            "is Equivalent to:\n",
            "chain = RunnableSequence([runnable1, runnable2])\n",
            "The .pipe method‚Äã\n",
            "If you have moral qualms with operator overloading, you can use the .pipe method instead. This is equivalent to the | operator.\n",
            "chain = runnable1.pipe(runnable2)\n",
            "Coercion‚Äã\n",
            "LCEL applies automatic type coercion to make it easier to compose chains.\n",
            "If you do not understand the type coercion, you can always use the RunnableSequence and RunnableParallel classes directly.\n",
            "This will make the code more verbose, but it will also make it more explicit.\n",
            "Dictionary to RunnableParallel‚Äã\n",
            "Inside an LCEL expression, a dictionary is automatically converted to a RunnableParallel.\n",
            "For example, the following code:\n",
            "mapping = {    \"key1\": runnable1,    \"key2\": runnable2,}chain = mapping | runnable3\n",
            "It gets automatically converted to the following:\n",
            "chain = RunnableSequence([RunnableParallel(mapping), runnable3])\n",
            "cautionYou have to be careful because the mapping dictionary is not a RunnableParallel object, it is just a dictionary. This means that the following code will raise an AttributeError:mapping.invoke(some_input)\n",
            "Function to RunnableLambda‚Äã\n",
            "Inside an LCEL expression, a function is automatically converted to a RunnableLambda.\n",
            "def some_func(x):    return xchain = some_func | runnable1\n",
            "It gets automatically converted to the following:\n",
            "chain = RunnableSequence([RunnableLambda(some_func), runnable1])\n",
            "cautionYou have to be careful because the lambda function is not a RunnableLambda object, it is just a function. This means that the following code will raise an AttributeError:lambda x: x + 1.invoke(some_input)\n",
            "Legacy chains‚Äã\n",
            "LCEL aims to provide consistency around behavior and customization over legacy subclassed chains such as LLMChain and\n",
            "ConversationalRetrievalChain. Many of these legacy chains hide important details like prompts, and as a wider variety\n",
            "of viable models emerge, customization has become more and more important.\n",
            "If you are currently using one of these legacy chains, please see this guide for guidance on how to migrate.\n",
            "For guides on how to do specific tasks with LCEL, check out the relevant how-to guides.Edit this pageWas this page helpful?PreviousKey-value storesNextMessagesBenefits of LCELShould I use LCEL?Composition PrimitivesRunnableSequenceRunnableParallelComposition SyntaxThe | operatorThe .pipe methodCoercionLegacy chainsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Evaluation | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEvaluationEvaluation\n",
            "\n",
            "Evaluation is the process of assessing the performance and effectiveness of your LLM-powered applications.\n",
            "It involves testing the model's responses against a set of predefined criteria or benchmarks to ensure it meets the desired quality standards and fulfills the intended purpose.\n",
            "This process is vital for building reliable applications.\n",
            "\n",
            "LangSmith helps with this process in a few ways:\n",
            "\n",
            "It makes it easier to create and curate datasets via its tracing and annotation features\n",
            "It provides an evaluation framework that helps you define metrics and run your app against your dataset\n",
            "It allows you to track results over time and automatically run your evaluators on a schedule or as part of CI/Code\n",
            "\n",
            "To learn more, check out this LangSmith guide.Edit this pageWas this page helpful?PreviousEmbedding modelsNextExample selectorsCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Embedding models | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideEmbedding modelsOn this pageEmbedding models\n",
            "\n",
            "Prerequisites\n",
            "Documents\n",
            "\n",
            "NoteThis conceptual overview focuses on text-based embedding models.Embedding models can also be multimodal though such models are not currently supported by LangChain.\n",
            "Imagine being able to capture the essence of any text - a tweet, document, or book - in a single, compact representation.\n",
            "This is the power of embedding models, which lie at the heart of many retrieval systems.\n",
            "Embedding models transform human language into a format that machines can understand and compare with speed and accuracy.\n",
            "These models take text as input and produce a fixed-length array of numbers, a numerical fingerprint of the text's semantic meaning.\n",
            "Embeddings allow search system to find relevant documents not just based on keyword matches, but on semantic understanding.\n",
            "Key concepts‚Äã\n",
            "\n",
            "(1) Embed text as a vector: Embeddings transform text into a numerical vector representation.\n",
            "(2) Measure similarity: Embedding vectors can be compared using simple mathematical operations.\n",
            "Embedding‚Äã\n",
            "Historical context‚Äã\n",
            "The landscape of embedding models has evolved significantly over the years.\n",
            "A pivotal moment came in 2018 when Google introduced BERT (Bidirectional Encoder Representations from Transformers).\n",
            "BERT applied transformer models to embed text as a simple vector representation, which lead to unprecedented performance across various NLP tasks.\n",
            "However, BERT wasn't optimized for generating sentence embeddings efficiently.\n",
            "This limitation spurred the creation of SBERT (Sentence-BERT), which adapted the BERT architecture to generate semantically rich sentence embeddings, easily comparable via similarity metrics like cosine similarity, dramatically reduced the computational overhead for tasks like finding similar sentences.\n",
            "Today, the embedding model ecosystem is diverse, with numerous providers offering their own implementations.\n",
            "To navigate this variety, researchers and practitioners often turn to benchmarks like the Massive Text Embedding Benchmark (MTEB) here for objective comparisons.\n",
            "Further reading\n",
            "See the seminal BERT paper.\n",
            "See Cameron Wolfe's excellent review of embedding models.\n",
            "See the Massive Text Embedding Benchmark (MTEB) leaderboard for a comprehensive overview of embedding models.\n",
            "\n",
            "Interface‚Äã\n",
            "LangChain provides a universal interface for working with them, providing standard methods for common operations.\n",
            "This common interface simplifies interaction with various embedding providers through two central methods:\n",
            "\n",
            "embed_documents: For embedding multiple texts (documents)\n",
            "embed_query: For embedding a single text (query)\n",
            "\n",
            "This distinction is important, as some providers employ different embedding strategies for documents (which are to be searched) versus queries (the search input itself).\n",
            "To illustrate, here's a practical example using LangChain's .embed_documents method to embed a list of strings:\n",
            "from langchain_openai import OpenAIEmbeddingsembeddings_model = OpenAIEmbeddings()embeddings = embeddings_model.embed_documents(    [        \"Hi there!\",        \"Oh, hello!\",        \"What's your name?\",        \"My friends call me World\",        \"Hello World!\"    ])len(embeddings), len(embeddings[0])(5, 1536)API Reference:OpenAIEmbeddings\n",
            "For convenience, you can also use the embed_query method to embed a single text:\n",
            "query_embedding = embeddings_model.embed_query(\"What is the meaning of life?\")\n",
            "Further reading\n",
            "See the full list of LangChain embedding model integrations.\n",
            "See these how-to guides for working with embedding models.\n",
            "\n",
            "Integrations‚Äã\n",
            "LangChain offers many embedding model integrations which you can find on the embedding models integrations page.\n",
            "Measure similarity‚Äã\n",
            "Each embedding is essentially a set of coordinates, often in a high-dimensional space.\n",
            "In this space, the position of each point (embedding) reflects the meaning of its corresponding text.\n",
            "Just as similar words might be close to each other in a thesaurus, similar concepts end up close to each other in this embedding space.\n",
            "This allows for intuitive comparisons between different pieces of text.\n",
            "By reducing text to these numerical representations, we can use simple mathematical operations to quickly measure how alike two pieces of text are, regardless of their original length or structure.\n",
            "Some common similarity metrics include:\n",
            "\n",
            "Cosine Similarity: Measures the cosine of the angle between two vectors.\n",
            "Euclidean Distance: Measures the straight-line distance between two points.\n",
            "Dot Product: Measures the projection of one vector onto another.\n",
            "\n",
            "The choice of similarity metric should be chosen based on the model.\n",
            "As an example, OpenAI suggests cosine similarity for their embeddings, which can be easily implemented:\n",
            "import numpy as npdef cosine_similarity(vec1, vec2):    dot_product = np.dot(vec1, vec2)    norm_vec1 = np.linalg.norm(vec1)    norm_vec2 = np.linalg.norm(vec2)    return dot_product / (norm_vec1 * norm_vec2)similarity = cosine_similarity(query_result, document_result)print(\"Cosine Similarity:\", similarity)\n",
            "Further reading\n",
            "See Simon Willison‚Äôs nice blog post and video on embeddings and similarity metrics.\n",
            "See this documentation from Google on similarity metrics to consider with embeddings.\n",
            "See Pinecone's blog post on similarity metrics.\n",
            "See OpenAI's FAQ on what similarity metric to use with OpenAI embeddings.\n",
            "Edit this pageWas this page helpful?PreviousDocument loadersNextEvaluationKey conceptsEmbeddingHistorical contextInterfaceIntegrationsMeasure similarityCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Chat models | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat modelsOn this pageChat models\n",
            "Overview‚Äã\n",
            "Large Language Models (LLMs) are advanced machine learning models that excel in a wide range of language-related tasks such as text generation, translation, summarization, question answering, and more, without needing task-specific fine tuning for every scenario.\n",
            "Modern LLMs are typically accessed through a chat model interface that takes a list of messages as input and returns a message as output.\n",
            "The newest generation of chat models offer additional capabilities:\n",
            "\n",
            "Tool calling: Many popular chat models offer a native tool calling API. This API allows developers to build rich applications that enable LLMs to interact with external services, APIs, and databases. Tool calling can also be used to extract structured information from unstructured data and perform various other tasks.\n",
            "Structured output: A technique to make a chat model respond in a structured format, such as JSON that matches a given schema.\n",
            "Multimodality: The ability to work with data other than text; for example, images, audio, and video.\n",
            "\n",
            "Features‚Äã\n",
            "LangChain provides a consistent interface for working with chat models from different providers while offering additional features for monitoring, debugging, and optimizing the performance of applications that use LLMs.\n",
            "\n",
            "Integrations with many chat model providers (e.g., Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, Groq). Please see chat model integrations for an up-to-date list of supported models.\n",
            "Use either LangChain's messages format or OpenAI format.\n",
            "Standard tool calling API: standard interface for binding tools to models, accessing tool call requests made by models, and sending tool results back to the model.\n",
            "Standard API for structuring outputs via the with_structured_output method.\n",
            "Provides support for async programming, efficient batching, a rich streaming API.\n",
            "Integration with LangSmith for monitoring and debugging production-grade applications based on LLMs.\n",
            "Additional features like standardized token usage, rate limiting, caching and more.\n",
            "\n",
            "Integrations‚Äã\n",
            "LangChain has many chat model integrations that allow you to use a wide variety of models from different providers.\n",
            "These integrations are one of two types:\n",
            "\n",
            "Official models: These are models that are officially supported by LangChain and/or model provider. You can find these models in the langchain-<provider> packages.\n",
            "Community models: There are models that are mostly contributed and supported by the community. You can find these models in the langchain-community package.\n",
            "\n",
            "LangChain chat models are named with a convention that prefixes \"Chat\" to their class names (e.g., ChatOllama, ChatAnthropic, ChatOpenAI, etc.).\n",
            "Please review the chat model integrations for a list of supported models.\n",
            "noteModels that do not include the prefix \"Chat\" in their name or include \"LLM\" as a suffix in their name typically refer to older models that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output.\n",
            "Interface‚Äã\n",
            "LangChain chat models implement the BaseChatModel interface. Because BaseChatModel also implements the Runnable Interface, chat models support a standard streaming interface, async programming, optimized batching, and more. Please see the Runnable Interface for more details.\n",
            "Many of the key methods of chat models operate on messages as input and return messages as output.\n",
            "Chat models offer a standard set of parameters that can be used to configure the model. These parameters are typically used to control the behavior of the model, such as the temperature of the output, the maximum number of tokens in the response, and the maximum time to wait for a response. Please see the standard parameters section for more details.\n",
            "noteIn documentation, we will often use the terms \"LLM\" and \"Chat Model\" interchangeably. This is because most modern LLMs are exposed to users via a chat model interface.However, LangChain also has implementations of older LLMs that do not follow the chat model interface and instead use an interface that takes a string as input and returns a string as output. These models are typically named without the \"Chat\" prefix (e.g., Ollama, Anthropic, OpenAI, etc.).\n",
            "These models implement the BaseLLM interface and may be named with the \"LLM\" suffix (e.g., OllamaLLM, AnthropicLLM, OpenAILLM, etc.). Generally, users should not use these models.\n",
            "Key methods‚Äã\n",
            "The key methods of a chat model are:\n",
            "\n",
            "invoke: The primary method for interacting with a chat model. It takes a list of messages as input and returns a list of messages as output.\n",
            "stream: A method that allows you to stream the output of a chat model as it is generated.\n",
            "batch: A method that allows you to batch multiple requests to a chat model together for more efficient processing.\n",
            "bind_tools: A method that allows you to bind a tool to a chat model for use in the model's execution context.\n",
            "with_structured_output: A wrapper around the invoke method for models that natively support structured output.\n",
            "\n",
            "Other important methods can be found in the BaseChatModel API Reference.\n",
            "Inputs and outputs‚Äã\n",
            "Modern LLMs are typically accessed through a chat model interface that takes messages as input and returns messages as output. Messages are typically associated with a role (e.g., \"system\", \"human\", \"assistant\") and one or more content blocks that contain text or potentially multimodal data (e.g., images, audio, video).\n",
            "LangChain supports two message formats to interact with chat models:\n",
            "\n",
            "LangChain Message Format: LangChain's own message format, which is used by default and is used internally by LangChain.\n",
            "OpenAI's Message Format: OpenAI's message format.\n",
            "\n",
            "Standard parameters‚Äã\n",
            "Many chat models have standardized parameters that can be used to configure the model:\n",
            "ParameterDescriptionmodelThe name or identifier of the specific AI model you want to use (e.g., \"gpt-3.5-turbo\" or \"gpt-4\").temperatureControls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.timeoutThe maximum time (in seconds) to wait for a response from the model before canceling the request. Ensures the request doesn‚Äôt hang indefinitely.max_tokensLimits the total number of tokens (words and punctuation) in the response. This controls how long the output can be.stopSpecifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.max_retriesThe maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.api_keyThe API key required for authenticating with the model provider. This is usually issued when you sign up for access to the model.base_urlThe URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.rate_limiterAn optional BaseRateLimiter to space out requests to avoid exceeding rate limits.  See rate-limiting below for more details.\n",
            "Some important things to note:\n",
            "\n",
            "Standard parameters only apply to model providers that expose parameters with the intended functionality. For example, some providers do not expose a configuration for maximum output tokens, so max_tokens can't be supported on these.\n",
            "Standard parameters are currently only enforced on integrations that have their own integration packages (e.g. langchain-openai, langchain-anthropic, etc.), they're not enforced on models in langchain-community.\n",
            "\n",
            "Chat models also accept other parameters that are specific to that integration. To find all the parameters supported by a Chat model head to the their respective API reference for that model.\n",
            "Tool calling‚Äã\n",
            "Chat models can call tools to perform tasks such as fetching data from a database, making API requests, or running custom code. Please\n",
            "see the tool calling guide for more information.\n",
            "Structured outputs‚Äã\n",
            "Chat models can be requested to respond in a particular format (e.g., JSON or matching a particular schema). This feature is extremely\n",
            "useful for information extraction tasks. Please read more about\n",
            "the technique in the structured outputs guide.\n",
            "Multimodality‚Äã\n",
            "Large Language Models (LLMs) are not limited to processing text. They can also be used to process other types of data, such as images, audio, and video. This is known as multimodality.\n",
            "Currently, only some LLMs support multimodal inputs, and almost none support multimodal outputs. Please consult the specific model documentation for details.\n",
            "Context window‚Äã\n",
            "A chat model's context window refers to the maximum size of the input sequence the model can process at one time. While the context windows of modern LLMs are quite large, they still present a limitation that developers must keep in mind when working with chat models.\n",
            "If the input exceeds the context window, the model may not be able to process the entire input and could raise an error. In conversational applications, this is especially important because the context window determines how much information the model can \"remember\" throughout a conversation. Developers often need to manage the input within the context window to maintain a coherent dialogue without exceeding the limit. For more details on handling memory in conversations, refer to the memory.\n",
            "The size of the input is measured in tokens which are the unit of processing that the model uses.\n",
            "Advanced topics‚Äã\n",
            "Rate-limiting‚Äã\n",
            "Many chat model providers impose a limit on the number of requests that can be made in a given time period.\n",
            "If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests.\n",
            "You have a few options to deal with rate limits:\n",
            "\n",
            "Try to avoid hitting rate limits by spacing out requests: Chat models accept a rate_limiter parameter that can be provided during initialization. This parameter is used to control the rate at which requests are made to the model provider. Spacing out the requests to a given model is a particularly useful strategy when benchmarking models to evaluate their performance. Please see the how to handle rate limits for more information on how to use this feature.\n",
            "Try to recover from rate limit errors: If you receive a rate limit error, you can wait a certain amount of time before retrying the request. The amount of time to wait can be increased with each subsequent rate limit error. Chat models have a max_retries parameter that can be used to control the number of retries. See the standard parameters section for more information.\n",
            "Fallback to another chat model: If you hit a rate limit with one chat model, you can switch to another chat model that is not rate-limited.\n",
            "\n",
            "Caching‚Äã\n",
            "Chat model APIs can be slow, so a natural question is whether to cache the results of previous conversations. Theoretically, caching can help improve performance by reducing the number of requests made to the model provider. In practice, caching chat model responses is a complex problem and should be approached with caution.\n",
            "The reason is that getting a cache hit is unlikely after the first or second interaction in a conversation if relying on caching the exact inputs into the model. For example, how likely do you think that multiple conversations start with the exact same message? What about the exact same three messages?\n",
            "An alternative approach is to use semantic caching, where you cache responses based on the meaning of the input rather than the exact input itself. This can be effective in some situations, but not in others.\n",
            "A semantic cache introduces a dependency on another model on the critical path of your application (e.g., the semantic cache may rely on an embedding model to convert text to a vector representation), and it's not guaranteed to capture the meaning of the input accurately.\n",
            "However, there might be situations where caching chat model responses is beneficial. For example, if you have a chat model that is used to answer frequently asked questions, caching responses can help reduce the load on the model provider, costs, and improve response times.\n",
            "Please see the how to cache chat model responses guide for more details.\n",
            "Related resources‚Äã\n",
            "\n",
            "How-to guides on using chat models: how-to guides.\n",
            "List of supported chat models: chat model integrations.\n",
            "\n",
            "Conceptual guides‚Äã\n",
            "\n",
            "Messages\n",
            "Tool calling\n",
            "Multimodality\n",
            "Structured outputs\n",
            "Tokens\n",
            "Edit this pageWas this page helpful?PreviousChat historyNextDocument loadersOverviewFeaturesIntegrationsInterfaceKey methodsInputs and outputsStandard parametersTool callingStructured outputsMultimodalityContext windowAdvanced topicsRate-limitingCachingRelated resourcesConceptual guidesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Chat history | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideChat historyOn this pageChat history\n",
            "Prerequisites\n",
            "Messages\n",
            "Chat models\n",
            "Tool calling\n",
            "\n",
            "Chat history is a record of the conversation between the user and the chat model. It is used to maintain context and state throughout the conversation. The chat history is sequence of messages, each of which is associated with a specific role, such as \"user\", \"assistant\", \"system\", or \"tool\".\n",
            "Conversation patterns‚Äã\n",
            "\n",
            "Most conversations start with a system message that sets the context for the conversation. This is followed by a user message containing the user's input, and then an assistant message containing the model's response.\n",
            "The assistant may respond directly to the user or if configured with tools request that a tool be invoked to perform a specific task.\n",
            "A full conversation often involves a combination of two patterns of alternating messages:\n",
            "\n",
            "The user and the assistant representing a back-and-forth conversation.\n",
            "The assistant and tool messages representing an \"agentic\" workflow where the assistant is invoking tools to perform specific tasks.\n",
            "\n",
            "Managing chat history‚Äã\n",
            "Since chat models have a maximum limit on input size, it's important to manage chat history and trim it as needed to avoid exceeding the context window.\n",
            "While processing chat history, it's essential to preserve a correct conversation structure.\n",
            "Key guidelines for managing chat history:\n",
            "\n",
            "The conversation should follow one of these structures:\n",
            "\n",
            "The first message is either a \"user\" message or a \"system\" message, followed by a \"user\" and then an \"assistant\" message.\n",
            "The last message should be either a \"user\" message or a \"tool\" message containing the result of a tool call.\n",
            "\n",
            "\n",
            "When using tool calling, a \"tool\" message should only follow an \"assistant\" message that requested the tool invocation.\n",
            "\n",
            "tipUnderstanding correct conversation structure is essential for being able to properly implement\n",
            "memory in chat models.\n",
            "Related resources‚Äã\n",
            "\n",
            "How to trim messages\n",
            "Memory guide for information on implementing short-term and long-term memory in chat models using LangGraph.\n",
            "Edit this pageWas this page helpful?PreviousCallbacksNextChat modelsConversation patternsManaging chat historyRelated resourcesCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Callbacks | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideCallbacksOn this pageCallbacks\n",
            "Prerequisites\n",
            "Runnable interface\n",
            "\n",
            "LangChain provides a callback system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.\n",
            "You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.\n",
            "Callback events‚Äã\n",
            "EventEvent TriggerAssociated MethodChat model startWhen a chat model startson_chat_model_startLLM startWhen a llm startson_llm_startLLM new tokenWhen an llm OR chat model emits a new tokenon_llm_new_tokenLLM endsWhen an llm OR chat model endson_llm_endLLM errorsWhen an llm OR chat model errorson_llm_errorChain startWhen a chain starts runningon_chain_startChain endWhen a chain endson_chain_endChain errorWhen a chain errorson_chain_errorTool startWhen a tool starts runningon_tool_startTool endWhen a tool endson_tool_endTool errorWhen a tool errorson_tool_errorAgent actionWhen an agent takes an actionon_agent_actionAgent finishWhen an agent endson_agent_finishRetriever startWhen a retriever startson_retriever_startRetriever endWhen a retriever endson_retriever_endRetriever errorWhen a retriever errorson_retriever_errorTextWhen arbitrary text is runon_textRetryWhen a retry event is runon_retry\n",
            "Callback handlers‚Äã\n",
            "Callback handlers can either be sync or async:\n",
            "\n",
            "Sync callback handlers implement the BaseCallbackHandler interface.\n",
            "Async callback handlers implement the AsyncCallbackHandler interface.\n",
            "\n",
            "During run-time LangChain configures an appropriate callback manager (e.g., CallbackManager or AsyncCallbackManager which will be responsible for calling the appropriate method on each \"registered\" callback handler when the event is triggered.\n",
            "Passing callbacks‚Äã\n",
            "The callbacks property is available on most objects throughout the API (Models, Tools, Agents, etc.) in two different places:\n",
            "\n",
            "Request time callbacks: Passed at the time of the request in addition to the input data.\n",
            "Available on all standard Runnable objects. These callbacks are INHERITED by all children\n",
            "of the object they are defined on. For example, chain.invoke({\"number\": 25}, {\"callbacks\": [handler]}).\n",
            "Constructor callbacks: chain = TheNameOfSomeChain(callbacks=[handler]). These callbacks\n",
            "are passed as arguments to the constructor of the object. The callbacks are scoped\n",
            "only to the object they are defined on, and are not inherited by any children of the object.\n",
            "\n",
            "warningConstructor callbacks are scoped only to the object they are defined on. They are not inherited by children\n",
            "of the object.\n",
            "If you're creating a custom chain or runnable, you need to remember to propagate request time\n",
            "callbacks to any child objects.\n",
            "Async in Python<=3.10Any RunnableLambda, a RunnableGenerator, or Tool that invokes other runnables\n",
            "and is running async in python<=3.10, will have to propagate callbacks to child\n",
            "objects manually. This is because LangChain cannot automatically propagate\n",
            "callbacks to child objects in this case.This is a common reason why you may fail to see events being emitted from custom\n",
            "runnables or tools.\n",
            "For specifics on how to use callbacks, see the relevant how-to guides here.Edit this pageWas this page helpful?PreviousAsync programming with langchainNextChat historyCallback eventsCallback handlersPassing callbacksCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            " --- \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Agents | ü¶úÔ∏èüîó LangChain\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Skip to main contentJoin us at  Interrupt: The Agent AI Conference by LangChain on May 13 & 14 in San Francisco!IntegrationsAPI ReferenceMoreContributingPeopleError referenceLangSmithLangGraphLangChain HubLangChain JS/TSv0.3v0.3v0.2v0.1üí¨SearchIntroductionTutorialsBuild a Question Answering application over a Graph DatabaseTutorialsBuild a simple LLM application with chat models and prompt templatesBuild a ChatbotBuild a Retrieval Augmented Generation (RAG) App: Part 2Build an Extraction ChainBuild an AgentTaggingBuild a Retrieval Augmented Generation (RAG) App: Part 1Build a semantic search engineBuild a Question/Answering system over SQL dataSummarize TextHow-to guidesHow-to guidesHow to use tools in a chainHow to use a vectorstore as a retrieverHow to add memory to chatbotsHow to use example selectorsHow to add a semantic layer over graph databaseHow to invoke runnables in parallelHow to stream chat model responsesHow to add default invocation args to a RunnableHow to add retrieval to chatbotsHow to use few shot examples in chat modelsHow to do tool/function callingHow to install LangChain packagesHow to add examples to the prompt for query analysisHow to use few shot examplesHow to run custom functionsHow to use output parsers to parse an LLM response into structured formatHow to handle cases where no queries are generatedHow to route between sub-chainsHow to return structured data from a modelHow to summarize text through parallelizationHow to summarize text through iterative refinementHow to summarize text in a single LLM callHow to use toolkitsHow to add ad-hoc tool calling capability to LLMs and Chat ModelsBuild an Agent with AgentExecutor (Legacy)How to construct knowledge graphsHow to partially format prompt templatesHow to handle multiple queries when doing query analysisHow to use built-in tools and toolkitsHow to pass through arguments from one step to the nextHow to compose prompts togetherHow to handle multiple retrievers when doing query analysisHow to add values to a chain's stateHow to construct filters for query analysisHow to configure runtime chain internalsHow deal with high cardinality categoricals when doing query analysisCustom Document LoaderHow to use the MultiQueryRetrieverHow to add scores to retriever resultsCachingHow to use callbacks in async environmentsHow to attach callbacks to a runnableHow to propagate callbacks  constructorHow to dispatch custom callback eventsHow to pass callbacks in at runtimeHow to split by characterHow to cache chat model responsesHow to handle rate limitsHow to init any model in one lineHow to track token usage in ChatModelsHow to add tools to chatbotsHow to split codeHow to do retrieval with contextual compressionHow to convert Runnables to ToolsHow to create custom callback handlersHow to create a custom chat model classCustom EmbeddingsHow to create a custom LLM classCustom RetrieverHow to create toolsHow to debug your LLM appsHow to load CSVsHow to load documents from a directoryHow to load HTMLHow to load JSONHow to load MarkdownHow to load Microsoft Office filesHow to load PDFsHow to load web pagesHow to create a dynamic (self-constructing) chainText embedding modelsHow to combine results from multiple retrieversHow to select examples from a LangSmith datasetHow to select examples by lengthHow to select examples by maximal marginal relevance (MMR)How to select examples by n-gram overlapHow to select examples by similarityHow to use reference examples when doing extractionHow to handle long text when doing extractionHow to use prompting alone (no tool calling) to do extractionHow to add fallbacks to a runnableHow to filter messagesHybrid SearchHow to use the LangChain indexing APIHow to inspect runnablesLangChain Expression Language CheatsheetHow to cache LLM responsesHow to track token usage for LLMsRun models locallyHow to get log probabilitiesHow to reorder retrieved results to mitigate the \"lost in the middle\" effectHow to split Markdown by HeadersHow to merge consecutive messages of the same typeHow to add message historyHow to migrate from legacy LangChain agents to LangGraphHow to retrieve using multiple vectors per documentHow to pass multimodal data to modelsHow to use multimodal promptsHow to create a custom Output ParserHow to use the output-fixing parserHow to parse JSON outputHow to retry when a parsing error occursHow to parse text from message objectsHow to parse XML outputHow to parse YAML outputHow to use the Parent Document RetrieverHow to use LangChain with different Pydantic versionsHow to add chat historyHow to get a RAG application to add citationsHow to do per-user retrievalHow to get your RAG application to return sourcesHow to stream results from your RAG applicationHow to split JSON dataHow to recursively split text by charactersResponse metadataHow to pass runtime secrets to runnablesHow to do \"self-querying\" retrievalHow to split text based on semantic similarityHow to chain runnablesHow to save and load LangChain objectsHow to split text by tokensHow to split HTMLHow to do question answering over CSVsHow to deal with large databases when doing SQL question-answeringHow to better prompt when doing SQL question-answeringHow to do query validation as part of SQL question-answeringHow to stream runnablesHow to stream responses from an LLMHow to use a time-weighted vector store retrieverHow to return artifacts from a toolHow to use chat models to call toolsHow to disable parallel tool callingHow to force models to call a toolHow to access the RunnableConfig from a toolHow to pass tool outputs to chat modelsHow to pass run time values to toolsHow to stream events from a toolHow to stream tool callsHow to convert tools to OpenAI FunctionsHow to handle tool errorsHow to use few-shot prompting with tool callingHow to add a human-in-the-loop for toolsHow to bind model-specific toolsHow to trim messagesHow to create and query vector storesConceptual guideAgentsArchitectureAsync programming with langchainCallbacksChat historyChat modelsDocument loadersEmbedding modelsEvaluationExample selectorsFew-shot promptingConceptual guideKey-value storesLangChain Expression Language (LCEL)MessagesMultimodalityOutput parsersPrompt TemplatesRetrieval augmented generation (RAG)RetrievalRetrieversRunnable interfaceStreamingStructured outputsTestingString-in, string-out llmsText splittersTokensTool callingToolsTracingVector storesWhy LangChain?Ecosystemü¶úüõ†Ô∏è LangSmithü¶úüï∏Ô∏è LangGraphVersionsv0.3v0.2Pydantic compatibilityMigrating from v0.0 chainsHow to migrate from v0.0 chainsMigrating from ConstitutionalChainMigrating from ConversationalChainMigrating from ConversationalRetrievalChainMigrating from LLMChainMigrating from LLMMathChainMigrating from LLMRouterChainMigrating from MapReduceDocumentsChainMigrating from MapRerankDocumentsChainMigrating from MultiPromptChainMigrating from RefineDocumentsChainMigrating from RetrievalQAMigrating from StuffDocumentsChainUpgrading to LangGraph memoryHow to migrate to LangGraph memoryHow to use BaseChatMessageHistory with LangGraphMigrating off ConversationBufferMemory or ConversationStringBufferMemoryMigrating off ConversationBufferWindowMemory or ConversationTokenBufferMemoryMigrating off ConversationSummaryMemory or ConversationSummaryBufferMemoryA Long-Term Memory AgentRelease policySecurity PolicyConceptual guideAgentsOn this pageAgents\n",
            "By themselves, language models can't take actions - they just output text. Agents are systems that take a high-level task and use an LLM as a reasoning engine to decide what actions to take and execute those actions.\n",
            "LangGraph is an extension of LangChain specifically aimed at creating highly controllable and customizable agents. We recommend that you use LangGraph for building agents.\n",
            "Please see the following resources for more information:\n",
            "\n",
            "LangGraph docs on common agent architectures\n",
            "Pre-built agents in LangGraph\n",
            "\n",
            "Legacy agent concept: AgentExecutor‚Äã\n",
            "LangChain previously introduced the AgentExecutor as a runtime for agents.\n",
            "While it served as an excellent starting point, its limitations became apparent when dealing with more sophisticated and customized agents.\n",
            "As a result, we're gradually phasing out AgentExecutor in favor of more flexible solutions in LangGraph.\n",
            "Transitioning from AgentExecutor to langgraph‚Äã\n",
            "If you're currently using AgentExecutor, don't worry! We've prepared resources to help you:\n",
            "\n",
            "\n",
            "For those who still need to use AgentExecutor, we offer a comprehensive guide on how to use AgentExecutor.\n",
            "\n",
            "\n",
            "However, we strongly recommend transitioning to LangGraph for improved flexibility and control. To facilitate this transition, we've created a detailed migration guide to help you move from AgentExecutor to LangGraph seamlessly.\n",
            "\n",
            "Edit this pageWas this page helpful?PreviousConceptual guideNextArchitectureLegacy agent concept: AgentExecutorTransitioning from AgentExecutor to langgraphCommunityTwitterGitHubOrganizationPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2025 LangChain, Inc.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(concatenated_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znc27KlFwnZ4"
      },
      "source": [
        "# LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbXI2ducwvLV"
      },
      "source": [
        "## Code solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4sZ5EtQxDtQ"
      },
      "source": [
        "먼저, OpenAI 함수 호출(Function Calling) 방식으로 사용해보겠습니다.\n",
        "\n",
        "OpenAI GPT 모델을 사용하여 code_gen_chain을 생성하고, 여기서 테스트해볼 예정입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJbCvBuhx0yA"
      },
      "source": [
        "**LangChain에서 Pydantic 사용하기**\n",
        "\n",
        "이 노트북은 Pydantic v2의 BaseModel을 사용하며, 이를 위해 langchain-core >= 0.3 버전이 필요합니다.\n",
        "langchain-core < 0.3 버전을 사용할 경우, Pydantic v1과 v2의 BaseModel이 혼합되어 오류가 발생할 수 있습니다.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaRDbhmxwWqn"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "### OpenAI\n",
        "\n",
        "# Grader prompt\n",
        "code_gen_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are a coding assistant with expertise in LCEL, LangChain expression language. \\n\n",
        "    Here is a full set of LCEL documentation:  \\n ------- \\n  {context} \\n ------- \\n Answer the user\n",
        "    question based on the above provided documentation. Ensure any code you provide can be executed \\n\n",
        "    with all required imports and variables defined. Structure your answer with a description of the code solution. \\n\n",
        "    Then list the imports. And finally list the functioning code block. Here is the user question:\"\"\",\n",
        "        ),\n",
        "        (\"placeholder\", \"{messages}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "# Data model\n",
        "class code(BaseModel):\n",
        "    \"\"\"Schema for code solutions to questions about LCEL.\"\"\"\n",
        "\n",
        "    prefix: str = Field(description=\"Description of the problem and approach\")\n",
        "    imports: str = Field(description=\"Code block import statements\")\n",
        "    code: str = Field(description=\"Code block not including import statements\")\n",
        "\n",
        "\n",
        "expt_llm = \"gpt-4o-mini\"\n",
        "llm = ChatOpenAI(temperature=0, model=expt_llm)\n",
        "code_gen_chain = code_gen_prompt | llm.with_structured_output(code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh7VmG1uyMVw"
      },
      "outputs": [],
      "source": [
        "# 당신은 LCEL, LangChain expression language에 전문성을 가진 코딩 어시스턴트입니다.\n",
        "# 다음은 LCEL 전체 문서입니다:\n",
        "# {context}\n",
        "# 위 문서를 기반으로 사용자 질문에 답하세요.\n",
        "# 제공하는 모든 코드가 실행 가능하도록 필요한 import와 변수들이 모두 정의되어 있어야 합니다.\n",
        "# 답변은 코드 솔루션에 대한 설명, import 목록, 작동 가능한 코드 블록의 순서로 구성하세요.\n",
        "# 다음은 사용자 질문입니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PDqDgVQyMTX",
        "outputId": "8e9f5b1d-e486-45df-ca0c-fba81693e37d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "code(prefix='RAG (Retrieval-Augmented Generation) 체인은 정보 검색과 생성 모델을 결합하여 더 정확하고 관련성 높은 응답을 생성하는 방법입니다. LCEL (LangChain Expression Language)을 사용하여 RAG 체인을 구축하는 방법은 다음과 같습니다. 이 예제에서는 벡터 저장소에서 문서를 검색하고, 검색된 문서를 기반으로 LLM (대형 언어 모델)을 사용하여 응답을 생성하는 체인을 구성합니다.', imports='from langchain_core.vectorstores import InMemoryVectorStore\\nfrom langchain_core.embeddings import OpenAIEmbeddings\\nfrom langchain_core.chains import RetrievalQA\\nfrom langchain_core.llms import OpenAI\\nfrom langchain_core.prompts import PromptTemplate\\nfrom langchain_core.documents import Document', code='# 1. 임베딩 모델 초기화\\nembedding_model = OpenAIEmbeddings()\\n\\n# 2. 벡터 저장소 초기화\\nvector_store = InMemoryVectorStore(embedding=embedding_model)\\n\\n# 3. 문서 추가\\ndocuments = [\\n    Document(page_content=\"이것은 첫 번째 문서입니다.\", metadata={\"source\": \"source1\"}),\\n    Document(page_content=\"이것은 두 번째 문서입니다.\", metadata={\"source\": \"source2\"})\\n]\\nvector_store.add_documents(documents)\\n\\n# 4. LLM 초기화\\nllm = OpenAI(model=\"gpt-3.5-turbo\")\\n\\n# 5. 프롬프트 템플릿 정의\\nprompt_template = PromptTemplate.from_template(\"다음 문서에 대한 질문에 답하십시오: {context}\")\\n\\n# 6. RAG 체인 생성\\nrag_chain = RetrievalQA(llm=llm, retriever=vector_store.as_retriever(), prompt=prompt_template)\\n\\n# 7. 질문에 대한 응답 생성\\nquery = \"첫 번째 문서에 대해 설명해 주세요.\"\\nresponse = rag_chain.invoke(query)\\nprint(response)')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "question = \"LCEL을 이용해서 RAG 체인을 어떻게 구축하나요?\"\n",
        "solution = code_gen_chain.invoke(\n",
        "    {\"context\": concatenated_content, \"messages\": [(\"user\", question)]}\n",
        ")\n",
        "solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYKl3eQDyMQq",
        "outputId": "abc4c6d3-cdb6-4e79-f58f-d4f4dd1552a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG (Retrieval-Augmented Generation) 체인은 정보 검색과 생성 모델을 결합하여 더 정확하고 관련성 높은 응답을 생성하는 방법입니다. LCEL (LangChain Expression Language)을 사용하여 RAG 체인을 구축하는 방법은 다음과 같습니다. 이 예제에서는 벡터 저장소에서 문서를 검색하고, 검색된 문서를 기반으로 LLM (대형 언어 모델)을 사용하여 응답을 생성하는 체인을 구성합니다.\n"
          ]
        }
      ],
      "source": [
        "print(solution.prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0xxst5IwWoM",
        "outputId": "5a7aaea2-e6fa-4b69-9c8c-181881286b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "from langchain_core.vectorstores import InMemoryVectorStore\n",
            "from langchain_core.embeddings import OpenAIEmbeddings\n",
            "from langchain_core.chains import RetrievalQA\n",
            "from langchain_core.llms import OpenAI\n",
            "from langchain_core.prompts import PromptTemplate\n",
            "from langchain_core.documents import Document\n"
          ]
        }
      ],
      "source": [
        "print(solution.imports)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5nJ_6HKvMZs",
        "outputId": "2fd4976f-f66a-4d28-e7e1-afd8f0b6fd4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1. 임베딩 모델 초기화\n",
            "embedding_model = OpenAIEmbeddings()\n",
            "\n",
            "# 2. 벡터 저장소 초기화\n",
            "vector_store = InMemoryVectorStore(embedding=embedding_model)\n",
            "\n",
            "# 3. 문서 추가\n",
            "documents = [\n",
            "    Document(page_content=\"이것은 첫 번째 문서입니다.\", metadata={\"source\": \"source1\"}),\n",
            "    Document(page_content=\"이것은 두 번째 문서입니다.\", metadata={\"source\": \"source2\"})\n",
            "]\n",
            "vector_store.add_documents(documents)\n",
            "\n",
            "# 4. LLM 초기화\n",
            "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
            "\n",
            "# 5. 프롬프트 템플릿 정의\n",
            "prompt_template = PromptTemplate.from_template(\"다음 문서에 대한 질문에 답하십시오: {context}\")\n",
            "\n",
            "# 6. RAG 체인 생성\n",
            "rag_chain = RetrievalQA(llm=llm, retriever=vector_store.as_retriever(), prompt=prompt_template)\n",
            "\n",
            "# 7. 질문에 대한 응답 생성\n",
            "query = \"첫 번째 문서에 대해 설명해 주세요.\"\n",
            "response = rag_chain.invoke(query)\n",
            "print(response)\n"
          ]
        }
      ],
      "source": [
        "print(solution.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5GSkksH03nE"
      },
      "source": [
        "# State"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX8wv4Ab08jw"
      },
      "source": [
        "우리의 상태(state)는 코드 생성과 관련된 키들(errors, question, code generation)을 포함하는 딕셔너리(dict)입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZq97uW50Yyl"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    \"\"\"\n",
        "    Represents the state of our graph.\n",
        "\n",
        "    Attributes:\n",
        "        error : Binary flag for control flow to indicate whether test error was tripped\n",
        "        messages : With user question, error messages, reasoning\n",
        "        generation : Code solution\n",
        "        iterations : Number of tries\n",
        "    \"\"\"\n",
        "\n",
        "    error: str\n",
        "    messages: List\n",
        "    generation: str\n",
        "    iterations: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AhlNWJV16b4"
      },
      "source": [
        "# Graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YV91shjJ1-Zx"
      },
      "source": [
        "우리의 그래프는 위 그림에 나타난 논리적 흐름을 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHl-NXSe16VW"
      },
      "outputs": [],
      "source": [
        "### Parameter\n",
        "\n",
        "# Max tries\n",
        "max_iterations = 3\n",
        "# Reflect\n",
        "# flag = 'reflect'\n",
        "flag = \"do not reflect\"\n",
        "\n",
        "### Nodes\n",
        "\n",
        "\n",
        "def generate(state: GraphState):\n",
        "    \"\"\"\n",
        "    Generate a code solution\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    error = state[\"error\"]\n",
        "\n",
        "    # We have been routed back to generation with an error\n",
        "    if error == \"yes\":\n",
        "        messages += [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Now, try again. Invoke the code tool to structure the output with a prefix, imports, and code block:\",\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    # Solution\n",
        "    code_solution = code_gen_chain.invoke(\n",
        "        {\"context\": concatenated_content, \"messages\": messages}\n",
        "    )\n",
        "    messages += [\n",
        "        (\n",
        "            \"assistant\",\n",
        "            f\"{code_solution.prefix} \\n Imports: {code_solution.imports} \\n Code: {code_solution.code}\",\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Increment\n",
        "    iterations = iterations + 1\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "def code_check(state: GraphState):\n",
        "    \"\"\"\n",
        "    Check code\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, error\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---CHECKING CODE---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    # Get solution components\n",
        "    imports = code_solution.imports\n",
        "    code = code_solution.code\n",
        "\n",
        "    # Check imports\n",
        "    try:\n",
        "        exec(imports)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE IMPORT CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the import test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # Check execution\n",
        "    try:\n",
        "        full_code = f\"\"\"{imports}\n",
        "\n",
        "        {code}\n",
        "        \"\"\"\n",
        "        # 실행 컨텍스트를 명시적으로 분리\n",
        "        exec_globals = {}\n",
        "        exec(full_code, exec_globals)\n",
        "    except Exception as e:\n",
        "        print(\"---CODE BLOCK CHECK: FAILED---\")\n",
        "        error_message = [(\"user\", f\"Your solution failed the code execution test: {e}\")]\n",
        "        messages += error_message\n",
        "        return {\n",
        "            \"generation\": code_solution,\n",
        "            \"messages\": messages,\n",
        "            \"iterations\": iterations,\n",
        "            \"error\": \"yes\",\n",
        "        }\n",
        "\n",
        "    # No errors\n",
        "    print(\"---NO CODE TEST FAILURES---\")\n",
        "    return {\n",
        "        \"generation\": code_solution,\n",
        "        \"messages\": messages,\n",
        "        \"iterations\": iterations,\n",
        "        \"error\": \"no\",\n",
        "    }\n",
        "\n",
        "\n",
        "def reflect(state: GraphState):\n",
        "    \"\"\"\n",
        "    Reflect on errors\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        state (dict): New key added to state, generation\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"---GENERATING CODE SOLUTION---\")\n",
        "\n",
        "    # State\n",
        "    messages = state[\"messages\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "    code_solution = state[\"generation\"]\n",
        "\n",
        "    # Prompt reflection\n",
        "\n",
        "    # Add reflection\n",
        "    reflections = code_gen_chain.invoke(\n",
        "        {\"context\": concatenated_content, \"messages\": messages}\n",
        "    )\n",
        "    messages += [(\"assistant\", f\"Here are reflections on the error: {reflections}\")]\n",
        "    return {\"generation\": code_solution, \"messages\": messages, \"iterations\": iterations}\n",
        "\n",
        "\n",
        "### Edges\n",
        "\n",
        "\n",
        "def decide_to_finish(state: GraphState):\n",
        "    \"\"\"\n",
        "    Determines whether to finish.\n",
        "\n",
        "    Args:\n",
        "        state (dict): The current graph state\n",
        "\n",
        "    Returns:\n",
        "        str: Next node to call\n",
        "    \"\"\"\n",
        "    error = state[\"error\"]\n",
        "    iterations = state[\"iterations\"]\n",
        "\n",
        "    if error == \"no\" or iterations == max_iterations:\n",
        "        print(\"---DECISION: FINISH---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"---DECISION: RE-TRY SOLUTION---\")\n",
        "        if flag == \"reflect\":\n",
        "            return \"reflect\"\n",
        "        else:\n",
        "            return \"generate\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Scx1K4Sh16Qu"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "# Define the nodes\n",
        "workflow.add_node(\"generate\", generate)  # generation solution\n",
        "workflow.add_node(\"check_code\", code_check)  # check code\n",
        "workflow.add_node(\"reflect\", reflect)  # reflect\n",
        "\n",
        "# Build graph\n",
        "workflow.add_edge(START, \"generate\")\n",
        "workflow.add_edge(\"generate\", \"check_code\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"check_code\",\n",
        "    decide_to_finish,\n",
        "    {\n",
        "        \"end\": END,\n",
        "        \"reflect\": \"reflect\",\n",
        "        \"generate\": \"generate\",\n",
        "    },\n",
        ")\n",
        "workflow.add_edge(\"reflect\", \"generate\")\n",
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "Xu8xJdTmUWdx",
        "outputId": "40256858-30c8-4d1b-d475-70d011c287bf"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "display(Image(app.get_graph(xray=True).draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47cXhDiK16N_",
        "outputId": "66f8f3a5-e230-4c7e-f5fb-063cd1b3794e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "text='Tell me a joke about cats'\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n"
          ]
        }
      ],
      "source": [
        "question = \"How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?\"\n",
        "\n",
        "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KZNvzY8q16Lt",
        "outputId": "1034df30-5d6a-408c-cc22-10c3b2c292ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "code(prefix=\"To directly pass a string to a runnable and use it to construct the input for your prompt, you can utilize the `RunnableLambda` class in LangChain. This allows you to define a function that takes a string input and constructs the necessary prompt format. You can then invoke this runnable with the string input. Here's how you can do it:\", imports='from langchain_core.runnables import RunnableLambda\\nfrom langchain_core.prompts import PromptTemplate', code='# Define a function that constructs the prompt\\ndef construct_prompt(input_string: str) -> str:\\n    prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n    return prompt_template.invoke({\\'topic\\': input_string})\\n\\n# Create a RunnableLambda from the function\\nrunnable = RunnableLambda(construct_prompt)\\n\\n# Now you can invoke the runnable with a string input\\nresult = runnable.invoke(\"cats\")\\nprint(result)  # This will print the constructed prompt based on the input.')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"generation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "gPQTEVh12lBx",
        "outputId": "1e708be7-cbf2-4b9f-c40c-de27c7ac5eba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'no'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"error\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00Sa-OqC2mtp",
        "outputId": "c66fa4a7-45dc-4f77-a98f-ef6c979d2b8c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user',\n",
              "  'How can I directly pass a string to a runnable and use it to construct the input needed for my prompt?'),\n",
              " ('assistant',\n",
              "  'To directly pass a string to a runnable and use it to construct the input for your prompt, you can utilize the `RunnableLambda` class in LangChain. This allows you to define a function that takes a string input and constructs the necessary prompt format. You can then invoke this runnable with the string input. Here\\'s how you can do it: \\n Imports: from langchain_core.runnables import RunnableLambda\\nfrom langchain_core.prompts import PromptTemplate \\n Code: # Define a function that constructs the prompt\\ndef construct_prompt(input_string: str) -> str:\\n    prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\\n    return prompt_template.invoke({\\'topic\\': input_string})\\n\\n# Create a RunnableLambda from the function\\nrunnable = RunnableLambda(construct_prompt)\\n\\n# Now you can invoke the runnable with a string input\\nresult = runnable.invoke(\"cats\")\\nprint(result)  # This will print the constructed prompt based on the input.')]"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p70Gl9UAaYx1",
        "outputId": "ba3b1f45-6ef9-4463-e104-683e6620ecd2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"iterations\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRan-mR5c6ZR"
      },
      "source": [
        "# 생성해준 코드 Imports & Code 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2LybU3I-Z4mg"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apK1vKvw2oQi",
        "outputId": "94910947-6288-4cf3-8955-244e7ac50968"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text='Tell me a joke about cats'\n"
          ]
        }
      ],
      "source": [
        "# Define a prompt template\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Create a runnable that constructs the prompt\n",
        "runnable = RunnableLambda(lambda topic: prompt_template.invoke({'topic': topic}))\n",
        "\n",
        "# Invoke the runnable with a string input\n",
        "result = runnable.invoke('cats')\n",
        "\n",
        "# Print the constructed prompt\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT5-58qGc_Hg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF0yQk5wdGS6",
        "outputId": "ac47f005-d353-4a4a-a3ac-cd22f6c67bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "content='Hello! I’m just a computer program, so I don’t have feelings, but I’m here and ready to help you. How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 13, 'total_tokens': 47, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BOQJEnQUUQcIqCpw2Xa8SnSwLS6PI', 'finish_reason': 'stop', 'logprobs': None} id='run-b6c14223-3cd6-4cdf-8ed5-12447f2846d9-0' usage_metadata={'input_tokens': 13, 'output_tokens': 34, 'total_tokens': 47, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n"
          ]
        }
      ],
      "source": [
        "question = \"문자열을 runnable에 직접 전달해서 프롬프트에 필요한 입력을 구성하려면 어떻게 해야 하나요?\"\n",
        "\n",
        "solution = app.invoke({\"messages\": [(\"user\", question)], \"iterations\": 0, \"error\": \"\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paqYdNeddGS7",
        "outputId": "74e64e6b-7737-48a9-9b6c-38a4ed2c5966"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "code(prefix=\"To pass a string directly to a runnable and construct the necessary input for a prompt, you can use the `invoke` method of the runnable. This method allows you to provide a string input, which will be automatically converted into the appropriate message format required by the chat model. Here's how you can do it:\", imports='from langchain_core.messages import HumanMessage\\nfrom langchain_openai import ChatOpenAI', code='# Initialize the chat model\\nmodel = ChatOpenAI(model=\"gpt-4o\")\\n\\n# Directly invoke the model with a string input\\nresponse = model.invoke(\"Hello, how are you?\")\\n\\n# Print the response from the model\\nprint(response)')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"generation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "w694kDZ1dGS8",
        "outputId": "d04839c4-f92a-4e02-dc8b-e69b7a348aa3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'no'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"error\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUV_DiffdGS9",
        "outputId": "cfac9c60-6316-4556-d9a5-727d8dcc076e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user', '문자열을 runnable에 직접 전달해서 프롬프트에 필요한 입력을 구성하려면 어떻게 해야 하나요?'),\n",
              " ('assistant',\n",
              "  'To pass a string directly to a runnable and construct the necessary input for a prompt, you can use the `invoke` method of the runnable. This method allows you to provide a string input, which will be automatically converted into the appropriate message format required by the chat model. Here\\'s how you can do it: \\n Imports: from langchain_core.messages import HumanMessage\\nfrom langchain_openai import ChatOpenAI \\n Code: # Initialize the chat model\\nmodel = ChatOpenAI(model=\"gpt-4o\")\\n\\n# Directly invoke the model with a string input\\nresponse = model.invoke(\"Hello, how are you?\")\\n\\n# Print the response from the model\\nprint(response)')]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpMgzA9BdGS9",
        "outputId": "bdb10d20-84c1-4fe0-fa80-c2390878a2a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "solution[\"iterations\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGZ4RI3Sdb0S"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1HPcgmkdpAG",
        "outputId": "0eab0af9-d940-4ade-d01d-4f4647d4a486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "content=\"Hello! I'm just a program, so I don't have feelings, but I'm here and ready to help you. How can I assist you today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 13, 'total_tokens': 43, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f5bdcc3276', 'id': 'chatcmpl-BOQJFMDcS2ZoOD9Tm5FwZ0B9V8Si7', 'finish_reason': 'stop', 'logprobs': None} id='run-4ad9d934-e16b-4f27-bcaa-446738fde697-0' usage_metadata={'input_tokens': 13, 'output_tokens': 30, 'total_tokens': 43, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# Define a simple runnable that takes a string input and returns a formatted message\n",
        "runnable = RunnableLambda(lambda input_str: HumanMessage(content=input_str))\n",
        "\n",
        "# Now, you can invoke the runnable with a string input\n",
        "input_string = \"Hello, how are you?\"\n",
        "formatted_message = runnable.invoke(input_string)\n",
        "\n",
        "# If you want to pass this to a chat model\n",
        "model = ChatOpenAI(model=\"gpt-4o\")\n",
        "response = model.invoke([formatted_message])\n",
        "\n",
        "# Print the response from the model\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6BqxFFDdo5c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gipyBK7lEZV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j0Fn6I9lEW-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WC7a_3LlEUR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8xvt3132s2f"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fANECva43QHa"
      },
      "source": [
        "다음은 LCEL 질문들로 구성된 공개 데이터셋 (https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d) 입니다.\n",
        "\n",
        "이 데이터셋은 lcel-teacher-eval이라는 이름으로 저장해두었습니다.\n",
        "\n",
        "CSV 파일은 여기( https://github.com/langchain-ai/lcel-teacher/blob/main/eval/eval.csv )에서도 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECUjHSGv2qqH"
      },
      "outputs": [],
      "source": [
        "import langsmith\n",
        "\n",
        "client = langsmith.Client()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8zeOfxM2qmJ"
      },
      "outputs": [],
      "source": [
        "# Clone the dataset to your tenant to use it\n",
        "try:\n",
        "    public_dataset = (\n",
        "        \"https://smith.langchain.com/public/326674a6-62bd-462d-88ae-eea49d503f9d/d\"\n",
        "    )\n",
        "    client.clone_public_dataset(public_dataset)\n",
        "except:\n",
        "    print(\"Please setup LangSmith\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhtBGBWs46z8"
      },
      "source": [
        "커스텀 평가 (Custom evals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdhMzxvv2qkC"
      },
      "outputs": [],
      "source": [
        "from langsmith.schemas import Example, Run\n",
        "\n",
        "\n",
        "def check_import(run: Run, example: Example) -> dict:\n",
        "    imports = run.outputs.get(\"imports\")\n",
        "    try:\n",
        "        exec(imports)\n",
        "        return {\"key\": \"import_check\", \"score\": 1}\n",
        "    except Exception:\n",
        "        return {\"key\": \"import_check\", \"score\": 0}\n",
        "\n",
        "\n",
        "def check_execution(run: Run, example: Example) -> dict:\n",
        "    imports = run.outputs.get(\"imports\")\n",
        "    code = run.outputs.get(\"code\")\n",
        "    try:\n",
        "        #exec(imports + \"\\n\" + code)\n",
        "        full_code = f\"\"\"{imports}\n",
        "\n",
        "        {code}\n",
        "        \"\"\"\n",
        "        # 실행 컨텍스트를 명시적으로 분리\n",
        "        exec_globals = {}\n",
        "        exec(full_code, exec_globals)\n",
        "        return {\"key\": \"code_execution_check\", \"score\": 1}\n",
        "    except Exception:\n",
        "        return {\"key\": \"code_execution_check\", \"score\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU50f8TW5CWg"
      },
      "source": [
        "LangGraph와 컨텍스트 스터핑(Context Stuffing)을 비교하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2MfPelN2qhq"
      },
      "outputs": [],
      "source": [
        "def predict_base_case(example: dict):\n",
        "    \"\"\"Context stuffing\"\"\"\n",
        "    solution = code_gen_chain.invoke(\n",
        "        {\"context\": concatenated_content, \"messages\": [(\"user\", example[\"question\"])]}\n",
        "    )\n",
        "    return {\"imports\": solution.imports, \"code\": solution.code}\n",
        "\n",
        "\n",
        "def predict_langgraph(example: dict):\n",
        "    \"\"\"LangGraph\"\"\"\n",
        "    graph = app.invoke(\n",
        "        {\"messages\": [(\"user\", example[\"question\"])], \"iterations\": 0, \"error\": \"\"}\n",
        "    )\n",
        "    solution = graph[\"generation\"]\n",
        "    return {\"imports\": solution.imports, \"code\": solution.code}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbsH3F0_0YwJ"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import evaluate\n",
        "\n",
        "# Evaluator\n",
        "code_evalulator = [check_import, check_execution]\n",
        "\n",
        "# Dataset\n",
        "dataset_name = \"lcel-teacher-eval\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468,
          "referenced_widgets": [
            "c61d7ab61ed146b4a4d0bb20b1ee3c5f",
            "0f0af31c52c24c61adf6e9eba7dcea7b",
            "916470962c45492cb88426a6ffaec3ce",
            "9c77b86ffd6042efb5300ed465e08da1",
            "3389f10c682c4fe79c894ad64ec26dcb",
            "39ec0b1871af4b2fa1ca51240526401c",
            "fc578afb57b24477a7caa1373f539e61",
            "84c308bb100a4a069f53e50867ddbe8d",
            "7fb9aed2269442d4a265281d79ff3249",
            "f63226d0321747789b58fb359f83596f",
            "13dbec678ad64653b1a6a42fb7ef74ed"
          ]
        },
        "id": "oHCAdr9O0Ytq",
        "outputId": "665f84aa-ad84-4765-f3ab-88811bde959a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'test-without-langgraph-gpt-4o-mini-82b9afcd' at:\n",
            "https://smith.langchain.com/o/2da4358c-aaa8-5f93-b4af-fa2d78b96bd8/datasets/3d1682fd-0c76-4193-99e3-1f9bca0b6bd9/compare?selectedSessions=23a5a320-1249-4524-a079-87dd09b46680\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c61d7ab61ed146b4a4d0bb20b1ee3c5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "itemgetter: What is LCEL?\n",
            "lambda: What is LCEL?\n",
            "get: What is LCEL?\n",
            "('content', 'The capital of France is Paris.')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BOQMNg7RnlKbNlec1cqmMAUh747oJ', 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-fb038643-e0d0-434d-b363-73c8ffd87581-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
            "{'a': 1, 'b': 2, 'c': 3}\n",
            "{'chain_one_output': 'Output from chain one with input: Test Input', 'chain_two_output': 'Output from chain two with input: Test Input'}\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_WTFZxMqdj48qdGTvW3tjgW4P', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 53, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f7a584cf1f', 'id': 'chatcmpl-BOQMrY5MUvrl6rVPmZu4V0L38nJcR', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-a2efbcec-6196-488b-9812-aa2739cf98f3-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_WTFZxMqdj48qdGTvW3tjgW4P', 'type': 'tool_call'}] usage_metadata={'input_tokens': 53, 'output_tokens': 18, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_RRpnWDAlz2IrJfQQKSz9MOLY', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 53, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f7a584cf1f', 'id': 'chatcmpl-BOQMsp4kxPNQU7GYN1pyq93OCYVLH', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-268e40ca-d41d-487c-9584-e0817bd363c2-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_RRpnWDAlz2IrJfQQKSz9MOLY', 'type': 'tool_call'}] usage_metadata={'input_tokens': 53, 'output_tokens': 18, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "content='' additional_kwargs={'tool_calls': [{'id': 'call_8plOjQ03plZpOGjRwWA3ja6G', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'multiply'}, 'type': 'function'}], 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 53, 'total_tokens': 71, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f7a584cf1f', 'id': 'chatcmpl-BOQMtfCf8LdzwPYPnrrEHCJpZ2CmC', 'finish_reason': 'tool_calls', 'logprobs': None} id='run-3534c9e4-2763-405f-a594-e2dae90ba02e-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_8plOjQ03plZpOGjRwWA3ja6G', 'type': 'tool_call'}] usage_metadata={'input_tokens': 53, 'output_tokens': 18, 'total_tokens': 71, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
            "text='Tell me more about: Artificial Intelligence'\n"
          ]
        }
      ],
      "source": [
        "# Run base case\n",
        "try:\n",
        "    experiment_results_ = evaluate(\n",
        "        predict_base_case,\n",
        "        data=dataset_name,\n",
        "        evaluators=code_evalulator,\n",
        "        experiment_prefix=f\"test-without-langgraph-{expt_llm}\",\n",
        "        max_concurrency=2,\n",
        "        metadata={\n",
        "            \"llm\": expt_llm,\n",
        "        },\n",
        "    )\n",
        "except:\n",
        "    print(\"Please setup LangSmith\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "af8043894e63494497787450a4207735",
            "5c64a9d35cd7410d8f759e7a7771f743",
            "23e545deb02344e58593428612bf8ed8",
            "3fd6f6c3deef4d25b52495bab772cb80",
            "087f07d10ff9422aa2172bf539729bee",
            "92726a2135614d4f9c7a83ad72ab6af9",
            "2980fcd448064a1e87721afecf7b2017",
            "481e39cde1fe49d98fb7ee3b195c5f7f",
            "1bddea93ddeb4748bc42226a31e7e8f5",
            "64914601bb2146d5a068864ddb0279b4",
            "a6ff61a796554a009f7ecc0722d1a058"
          ]
        },
        "id": "xDTpFRu00Yri",
        "outputId": "1463bc13-d869-4b89-8de4-68057c3a898a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for experiment: 'test-with-langgraph-gpt-4o-mini-do not reflect-98be5059' at:\n",
            "https://smith.langchain.com/o/2da4358c-aaa8-5f93-b4af-fa2d78b96bd8/datasets/3d1682fd-0c76-4193-99e3-1f9bca0b6bd9/compare?selectedSessions=7a335a68-2015-40b9-a3bc-40a8df047eff\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af8043894e63494497787450a4207735",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---GENERATING CODE SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "Using itemgetter: What is the capital of France?\n",
            "Using lambda: What is the capital of France?\n",
            "Using get method: What is the capital of France?\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "Using itemgetter: What is the capital of France?\n",
            "Using lambda: What is the capital of France?\n",
            "Using get method: What is the capital of France?\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "('content', 'The capital of France is Paris.')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BOQNskQKc2vZa0i59yu0HDpDHUeJH', 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-fd0834fe-bba3-4502-9039-5433158fc591-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "('content', 'The capital of France is Paris.')\n",
            "('additional_kwargs', {'refusal': None})\n",
            "('response_metadata', {'token_usage': {'completion_tokens': 8, 'prompt_tokens': 24, 'total_tokens': 32, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4-0613', 'system_fingerprint': None, 'id': 'chatcmpl-BOQNtNOdCsDzvMtuUbtFRqt7eMFxy', 'finish_reason': 'stop', 'logprobs': None})\n",
            "('type', 'ai')\n",
            "('name', None)\n",
            "('id', 'run-81830a78-29d0-4557-b53b-c1defa4e3b9a-0')\n",
            "('example', False)\n",
            "('tool_calls', [])\n",
            "('invalid_tool_calls', [])\n",
            "('usage_metadata', {'input_tokens': 24, 'output_tokens': 8, 'total_tokens': 32, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "{'a': 1, 'b': 2, 'c': 3}\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "{'a': 1, 'b': 2, 'c': 3}\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "<class 'langchain_core.utils.pydantic.PromptInput'>\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "<class 'langchain_core.utils.pydantic.PromptInput'>\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "Tool called with arguments: {'a': 2, 'b': 3}\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "answer='The powerhouse of the cell is the **mitochondrion** (plural: mitochondria). These organelles are responsible for producing energy in the form of adenosine triphosphate (ATP) through a process known as cellular respiration. Mitochondria are unique in that they have their own DNA and are believed to have originated from ancient symbiotic bacteria. They play a crucial role in energy metabolism and are essential for the survival and function of eukaryotic cells.' followup_question='How do mitochondria produce energy?'\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "---GENERATING CODE SOLUTION---\n",
            "answer='The powerhouse of the cell is the **mitochondrion** (plural: mitochondria). These organelles are responsible for producing energy in the form of adenosine triphosphate (ATP) through a process known as cellular respiration. Mitochondria are unique in that they have their own DNA and are believed to have originated from ancient symbiotic bacteria. They play a crucial role in energy metabolism and are vital for the survival and function of eukaryotic cells.' followup_question='How do mitochondria produce energy?'\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "text='Tell me a joke about cats'\n",
            "text='Tell me a joke about cats'\n",
            "text='Tell me a joke about cats'\n",
            "---NO CODE TEST FAILURES---\n",
            "---DECISION: FINISH---\n",
            "text='Tell me a joke about cats'\n",
            "---GENERATING CODE SOLUTION---\n",
            "text='Tell me a joke about cats'\n",
            "text='Tell me a joke about cats'\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE IMPORT CHECK: FAILED---\n",
            "---DECISION: FINISH---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: RE-TRY SOLUTION---\n",
            "---GENERATING CODE SOLUTION---\n",
            "---CHECKING CODE---\n",
            "---CODE BLOCK CHECK: FAILED---\n",
            "---DECISION: FINISH---\n"
          ]
        }
      ],
      "source": [
        "# Run with langgraph\n",
        "try:\n",
        "    experiment_results = evaluate(\n",
        "        predict_langgraph,\n",
        "        data=dataset_name,\n",
        "        evaluators=code_evalulator,\n",
        "        experiment_prefix=f\"test-with-langgraph-{expt_llm}-{flag}\",\n",
        "        max_concurrency=2,\n",
        "        metadata={\n",
        "            \"llm\": expt_llm,\n",
        "            \"feedback\": flag,\n",
        "        },\n",
        "    )\n",
        "except:\n",
        "    print(\"Please setup LangSmith\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yP0onxJ5vDr"
      },
      "source": [
        "결과:\n",
        "\n",
        "\n",
        "*   LangGraph는 기본 방식보다 더 나은 성능을 보였습니다:**재시도 루프(re-try loop)를 추가하면 성능이 향상**되었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vRP6d2T5KsN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableSequence\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Xj3OHqz35Kph",
        "outputId": "87c399ef-3bab-4ba3-e536-49963915d045"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-c0e28ca00ed9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Create a chain using LCEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mchain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunnableSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Get the input schema of the chain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, first, middle, last, *steps)\u001b[0m\n\u001b[1;32m   2777\u001b[0m                 \u001b[0msteps_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2778\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2779\u001b[0;31m                 \u001b[0msteps_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoerce_to_runnable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2780\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_flat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2781\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"RunnableSequence must have at least 2 steps, got {len(steps_flat)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5913\u001b[0m         \u001b[0;34mf\"Instead got an unsupported type: {type(thing)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5914\u001b[0m     )\n\u001b[0;32m-> 5915\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'list'>"
          ]
        }
      ],
      "source": [
        "# Define a prompt template\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Initialize the chat model\n",
        "model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "# Create a chain using LCEL\n",
        "chain = RunnableSequence([prompt_template, model])\n",
        "\n",
        "# Get the input schema of the chain\n",
        "input_schema = chain.get_input_schema()\n",
        "\n",
        "# Print the input schema\n",
        "print(input_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-WjiVFI5Km4",
        "outputId": "e8b29d70-4fdc-455e-f7b1-2835bab776b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'langchain_core.utils.pydantic.PromptInput'>\n"
          ]
        }
      ],
      "source": [
        "# Define a prompt template\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "\n",
        "# Initialize the chat model\n",
        "model = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "# Create a chain using LCEL\n",
        "chain = RunnableSequence(prompt_template | model)\n",
        "\n",
        "# Get the input schema of the chain\n",
        "input_schema = chain.get_input_schema()\n",
        "\n",
        "# Print the input schema\n",
        "print(input_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdX-RObLI8Wy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aiagent",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "087f07d10ff9422aa2172bf539729bee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f0af31c52c24c61adf6e9eba7dcea7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ec0b1871af4b2fa1ca51240526401c",
            "placeholder": "​",
            "style": "IPY_MODEL_fc578afb57b24477a7caa1373f539e61",
            "value": ""
          }
        },
        "13dbec678ad64653b1a6a42fb7ef74ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bddea93ddeb4748bc42226a31e7e8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23e545deb02344e58593428612bf8ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_481e39cde1fe49d98fb7ee3b195c5f7f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1bddea93ddeb4748bc42226a31e7e8f5",
            "value": 1
          }
        },
        "2980fcd448064a1e87721afecf7b2017": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3389f10c682c4fe79c894ad64ec26dcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39ec0b1871af4b2fa1ca51240526401c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fd6f6c3deef4d25b52495bab772cb80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64914601bb2146d5a068864ddb0279b4",
            "placeholder": "​",
            "style": "IPY_MODEL_a6ff61a796554a009f7ecc0722d1a058",
            "value": " 20/? [02:47&lt;00:00,  5.07s/it]"
          }
        },
        "481e39cde1fe49d98fb7ee3b195c5f7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5c64a9d35cd7410d8f759e7a7771f743": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92726a2135614d4f9c7a83ad72ab6af9",
            "placeholder": "​",
            "style": "IPY_MODEL_2980fcd448064a1e87721afecf7b2017",
            "value": ""
          }
        },
        "64914601bb2146d5a068864ddb0279b4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fb9aed2269442d4a265281d79ff3249": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84c308bb100a4a069f53e50867ddbe8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "916470962c45492cb88426a6ffaec3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84c308bb100a4a069f53e50867ddbe8d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7fb9aed2269442d4a265281d79ff3249",
            "value": 1
          }
        },
        "92726a2135614d4f9c7a83ad72ab6af9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c77b86ffd6042efb5300ed465e08da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f63226d0321747789b58fb359f83596f",
            "placeholder": "​",
            "style": "IPY_MODEL_13dbec678ad64653b1a6a42fb7ef74ed",
            "value": " 20/? [00:58&lt;00:00,  1.86s/it]"
          }
        },
        "a6ff61a796554a009f7ecc0722d1a058": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8043894e63494497787450a4207735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c64a9d35cd7410d8f759e7a7771f743",
              "IPY_MODEL_23e545deb02344e58593428612bf8ed8",
              "IPY_MODEL_3fd6f6c3deef4d25b52495bab772cb80"
            ],
            "layout": "IPY_MODEL_087f07d10ff9422aa2172bf539729bee"
          }
        },
        "c61d7ab61ed146b4a4d0bb20b1ee3c5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f0af31c52c24c61adf6e9eba7dcea7b",
              "IPY_MODEL_916470962c45492cb88426a6ffaec3ce",
              "IPY_MODEL_9c77b86ffd6042efb5300ed465e08da1"
            ],
            "layout": "IPY_MODEL_3389f10c682c4fe79c894ad64ec26dcb"
          }
        },
        "f63226d0321747789b58fb359f83596f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc578afb57b24477a7caa1373f539e61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
